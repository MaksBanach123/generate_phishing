# -*- coding: utf-8 -*-
"""fine_tuning_phi_1_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OCp63n3FEg12_uKorC3YURP1kUTZ9ERU
"""

!pip install bitsandbytes

!pip install -U datasets

import torch
from pathlib import Path
from huggingface_hub import login
from datasets import load_dataset
from transformers import(
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
  )
from peft import get_peft_model, LoraConfig, TaskType
from huggingface_hub import login
from google.colab import drive

drive.mount('/content/drive')

print(torch.cuda.is_available())  # Should return True
print(torch.cuda.get_device_name(0))  # Should show your GPU name

MODEL_NAME = "microsoft/phi-1_5"
DATA_PATH   = Path("/content/drive/MyDrive/NLP/phishing_dataset_link_in_prompt.jsonl")
OUTPUT_DIR  = Path("/content/drive/MyDrive/NLP/model_phi_1_5")
MAX_LENGTH  = 1024
BATCH_SIZE  = 2
GR_ACC_STEPS = 4
EPOCHS = 3
HF_TOKEN = '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'

login(token=HF_TOKEN)

# Tokenizer i model (z 4bit)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True) # ładowanie odpowiedniego tokenizera dla modelu Mistral 7B
tokenizer.pad_token = tokenizer.eos_token #Większość modeli LLM (jak Mistral, LLaMA) nie ma domyślnie pad_token, bo były trenowane na danych bez paddingu.
                                          #Ale HuggingFace potrzebuje tego tokenu, gdy używasz:
                                          #- padding="max_length"
                                          #- Trainer lub DataCollatorWithPadding
                                          #eos_token jako pad_token to standardowy trick w LoRA/QLoRA.
tokenizer.padding_side = "right"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                       # 4-bit quantization
    bnb_4bit_compute_dtype=torch.float16,    # compute in FP16
    bnb_4bit_quant_type="nf4",                # typ kwantyzacji (np. 'nf4' albo 'fp4')
    bnb_4bit_use_double_quant=True            # użycie podwójnej kwantyzacji
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16
)
# funkcja do pobierania gotowego modelu z HF, łądowanie jego wag i struktury.
# Mistral domyslnie uzywa FP16, 16-bitowa reprezentacja zmiennoprzecinkowa - my ładujemy w 4 bitach dla lepszej wydajności

# Tokenizacja
def tokenize(example):
    text = f"### Instruction:\n{example['prompt']}\n\n### Response:\n{example['completion']}"
    enc = tokenizer(
        text,
        truncation=True,
        max_length=MAX_LENGTH
    )
    return enc

# Dataset

RAW_COLUMNS = ["prompt", "completion"]

dataset = load_dataset(
    "json",
    data_files=str(DATA_PATH),
    split="train",
    keep_in_memory=True
)

splits = dataset.train_test_split(test_size=0.1, seed=42)
dataset_train = splits["train"]
dataset_val = splits["test"]

tokenized_train = (
    dataset_train
    .map(tokenize, remove_columns=RAW_COLUMNS, num_proc=4)
    .shuffle(seed=42)
    .with_format("torch")
)

tokenized_val = (
    dataset_val
    .map(tokenize, remove_columns=RAW_COLUMNS, num_proc=4)
    .shuffle(seed=42)
    .with_format("torch")
)

print(tokenized_train.column_names)   # ['input_ids', 'attention_mask', 'labels']
print(tokenized_train[0].keys())
print(tokenized_val[0].keys())

# 2️⃣ collator, który:
#     • dopada wszystkie sekwencje na tę samą długość
#     • tworzy labels = input_ids, a pady zamienia na -100

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,          # causal LM
    pad_to_multiple_of=8  # (opcjonalnie) lepsze wyrównanie dla GPU
)

"""###IMPLEMENTACJA LORA"""

# Konfiguracja LoRA
peft_config = LoraConfig(
    r=8, #parametr r lora, im wyższy tym bardziej wymagający proces douczania ale też wiecej parametrow douczanych
    lora_alpha=16, # współczynnik skalujący wpływ Lora'y na model
    target_modules=["q_proj", "v_proj", "v_proj", "o_proj"], # wskazanie na warstwy atencji które chcemy douczać, zamrożenie reszty, q-> query layer , v-> value layer, dlatego one bo mają pdoobno najwiekszy wpływ na atencję
    lora_dropout=0.1, # dodanie dropout'u, losowe zerowanie wag
    bias="none", # nie dodajemy biasu
    task_type=TaskType.CAUSAL_LM # mówimy modelowi że douczamy go do generacji
)# dzieki Lorze douczamy model dużo szybciej, dzieli ona macierz parametrów na dwie podmacierze i zamraza dotychczasową macierz wag


model = get_peft_model(model, peft_config)

model.enable_input_require_grads()

# Parametry treningu
training_args = TrainingArguments(
    output_dir=str(OUTPUT_DIR),
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GR_ACC_STEPS,
    num_train_epochs=EPOCHS,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
    optim = "paged_adamw_32bit",
    gradient_checkpointing=False,
    report_to="wandb",
    learning_rate=3e-4,
    lr_scheduler_type="cosine",
    warmup_steps=200,
    eval_strategy="steps",
    eval_steps=500
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    data_collator=data_collator
)

trainer.train()