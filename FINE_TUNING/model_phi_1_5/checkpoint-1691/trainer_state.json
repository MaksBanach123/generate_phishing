{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.999556672085119,
  "eval_steps": 500,
  "global_step": 1691,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005911038865080538,
      "grad_norm": 0.2276100516319275,
      "learning_rate": 1.3499999999999998e-05,
      "loss": 3.9956,
      "step": 10
    },
    {
      "epoch": 0.011822077730161076,
      "grad_norm": 0.28873324394226074,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 4.0319,
      "step": 20
    },
    {
      "epoch": 0.017733116595241613,
      "grad_norm": 0.33441638946533203,
      "learning_rate": 4.3499999999999993e-05,
      "loss": 4.2832,
      "step": 30
    },
    {
      "epoch": 0.023644155460322152,
      "grad_norm": 0.4686412811279297,
      "learning_rate": 5.85e-05,
      "loss": 3.9913,
      "step": 40
    },
    {
      "epoch": 0.02955519432540269,
      "grad_norm": 0.4693009853363037,
      "learning_rate": 7.35e-05,
      "loss": 3.7657,
      "step": 50
    },
    {
      "epoch": 0.035466233190483226,
      "grad_norm": 0.41090336441993713,
      "learning_rate": 8.849999999999998e-05,
      "loss": 3.6837,
      "step": 60
    },
    {
      "epoch": 0.04137727205556377,
      "grad_norm": 0.5891649127006531,
      "learning_rate": 0.00010349999999999998,
      "loss": 3.7584,
      "step": 70
    },
    {
      "epoch": 0.047288310920644304,
      "grad_norm": 0.65283203125,
      "learning_rate": 0.0001185,
      "loss": 3.4932,
      "step": 80
    },
    {
      "epoch": 0.05319934978572484,
      "grad_norm": 0.9791437983512878,
      "learning_rate": 0.0001335,
      "loss": 3.3853,
      "step": 90
    },
    {
      "epoch": 0.05911038865080538,
      "grad_norm": 0.8459473252296448,
      "learning_rate": 0.00014849999999999998,
      "loss": 2.8556,
      "step": 100
    },
    {
      "epoch": 0.06502142751588591,
      "grad_norm": 1.0254615545272827,
      "learning_rate": 0.0001635,
      "loss": 2.8705,
      "step": 110
    },
    {
      "epoch": 0.07093246638096645,
      "grad_norm": 0.929900050163269,
      "learning_rate": 0.00017849999999999997,
      "loss": 2.4165,
      "step": 120
    },
    {
      "epoch": 0.076843505246047,
      "grad_norm": 0.9569188952445984,
      "learning_rate": 0.0001935,
      "loss": 2.2081,
      "step": 130
    },
    {
      "epoch": 0.08275454411112754,
      "grad_norm": 1.4186241626739502,
      "learning_rate": 0.00020849999999999997,
      "loss": 1.972,
      "step": 140
    },
    {
      "epoch": 0.08866558297620807,
      "grad_norm": 1.543451189994812,
      "learning_rate": 0.00022349999999999998,
      "loss": 2.0021,
      "step": 150
    },
    {
      "epoch": 0.09457662184128861,
      "grad_norm": 1.427973747253418,
      "learning_rate": 0.0002385,
      "loss": 2.3088,
      "step": 160
    },
    {
      "epoch": 0.10048766070636915,
      "grad_norm": 1.124901533126831,
      "learning_rate": 0.0002535,
      "loss": 2.1868,
      "step": 170
    },
    {
      "epoch": 0.10639869957144968,
      "grad_norm": 0.8841444849967957,
      "learning_rate": 0.00026849999999999997,
      "loss": 1.5767,
      "step": 180
    },
    {
      "epoch": 0.11230973843653022,
      "grad_norm": 1.7946240901947021,
      "learning_rate": 0.00028349999999999995,
      "loss": 1.8317,
      "step": 190
    },
    {
      "epoch": 0.11822077730161076,
      "grad_norm": 0.7886484265327454,
      "learning_rate": 0.0002985,
      "loss": 1.5576,
      "step": 200
    },
    {
      "epoch": 0.12413181616669129,
      "grad_norm": 1.8757169246673584,
      "learning_rate": 0.00029999747505462783,
      "loss": 1.5806,
      "step": 210
    },
    {
      "epoch": 0.13004285503177182,
      "grad_norm": 0.6265178322792053,
      "learning_rate": 0.00029998874695753624,
      "loss": 1.885,
      "step": 220
    },
    {
      "epoch": 0.13595389389685236,
      "grad_norm": 0.8252459168434143,
      "learning_rate": 0.00029997378489923836,
      "loss": 1.7116,
      "step": 230
    },
    {
      "epoch": 0.1418649327619329,
      "grad_norm": 0.6410897970199585,
      "learning_rate": 0.0002999525895016001,
      "loss": 1.3141,
      "step": 240
    },
    {
      "epoch": 0.14777597162701345,
      "grad_norm": 0.5499432682991028,
      "learning_rate": 0.00029992516164556297,
      "loss": 1.81,
      "step": 250
    },
    {
      "epoch": 0.153687010492094,
      "grad_norm": 2.034027338027954,
      "learning_rate": 0.00029989150247110687,
      "loss": 1.6135,
      "step": 260
    },
    {
      "epoch": 0.15959804935717453,
      "grad_norm": 0.771909236907959,
      "learning_rate": 0.0002998516133772035,
      "loss": 1.5172,
      "step": 270
    },
    {
      "epoch": 0.16550908822225507,
      "grad_norm": 0.49242275953292847,
      "learning_rate": 0.0002998054960217575,
      "loss": 1.164,
      "step": 280
    },
    {
      "epoch": 0.1714201270873356,
      "grad_norm": 0.7855795621871948,
      "learning_rate": 0.0002997531523215382,
      "loss": 1.245,
      "step": 290
    },
    {
      "epoch": 0.17733116595241613,
      "grad_norm": 0.6119864583015442,
      "learning_rate": 0.00029969458445209933,
      "loss": 1.24,
      "step": 300
    },
    {
      "epoch": 0.18324220481749667,
      "grad_norm": 1.9419697523117065,
      "learning_rate": 0.0002996297948476891,
      "loss": 1.8625,
      "step": 310
    },
    {
      "epoch": 0.18915324368257722,
      "grad_norm": 1.0362036228179932,
      "learning_rate": 0.00029955878620114873,
      "loss": 1.3991,
      "step": 320
    },
    {
      "epoch": 0.19506428254765776,
      "grad_norm": 2.193244695663452,
      "learning_rate": 0.00029948156146380057,
      "loss": 1.2342,
      "step": 330
    },
    {
      "epoch": 0.2009753214127383,
      "grad_norm": 1.1690107583999634,
      "learning_rate": 0.0002993981238453255,
      "loss": 1.2931,
      "step": 340
    },
    {
      "epoch": 0.20688636027781881,
      "grad_norm": 0.3863016366958618,
      "learning_rate": 0.0002993084768136296,
      "loss": 1.1641,
      "step": 350
    },
    {
      "epoch": 0.21279739914289936,
      "grad_norm": 2.0656979084014893,
      "learning_rate": 0.0002992126240946998,
      "loss": 1.8293,
      "step": 360
    },
    {
      "epoch": 0.2187084380079799,
      "grad_norm": 1.1432462930679321,
      "learning_rate": 0.0002991105696724492,
      "loss": 1.6783,
      "step": 370
    },
    {
      "epoch": 0.22461947687306044,
      "grad_norm": 0.9850197434425354,
      "learning_rate": 0.00029900231778855137,
      "loss": 1.4363,
      "step": 380
    },
    {
      "epoch": 0.23053051573814098,
      "grad_norm": 0.4927749037742615,
      "learning_rate": 0.00029888787294226417,
      "loss": 1.1233,
      "step": 390
    },
    {
      "epoch": 0.23644155460322153,
      "grad_norm": 1.6659817695617676,
      "learning_rate": 0.0002987672398902427,
      "loss": 1.7724,
      "step": 400
    },
    {
      "epoch": 0.24235259346830204,
      "grad_norm": 0.7641460299491882,
      "learning_rate": 0.00029864042364634145,
      "loss": 1.4903,
      "step": 410
    },
    {
      "epoch": 0.24826363233338258,
      "grad_norm": 2.1336848735809326,
      "learning_rate": 0.00029850742948140627,
      "loss": 1.3408,
      "step": 420
    },
    {
      "epoch": 0.25417467119846315,
      "grad_norm": 1.7345354557037354,
      "learning_rate": 0.00029836826292305483,
      "loss": 1.5539,
      "step": 430
    },
    {
      "epoch": 0.26008571006354364,
      "grad_norm": 1.8847336769104004,
      "learning_rate": 0.0002982229297554473,
      "loss": 1.297,
      "step": 440
    },
    {
      "epoch": 0.2659967489286242,
      "grad_norm": 0.5554383397102356,
      "learning_rate": 0.0002980714360190456,
      "loss": 1.441,
      "step": 450
    },
    {
      "epoch": 0.2719077877937047,
      "grad_norm": 0.8399075269699097,
      "learning_rate": 0.0002979137880103627,
      "loss": 1.3417,
      "step": 460
    },
    {
      "epoch": 0.27781882665878527,
      "grad_norm": 1.0724455118179321,
      "learning_rate": 0.00029774999228170064,
      "loss": 1.328,
      "step": 470
    },
    {
      "epoch": 0.2837298655238658,
      "grad_norm": 2.457491636276245,
      "learning_rate": 0.00029758005564087817,
      "loss": 1.3979,
      "step": 480
    },
    {
      "epoch": 0.28964090438894635,
      "grad_norm": 0.5214126706123352,
      "learning_rate": 0.0002974039851509481,
      "loss": 1.1475,
      "step": 490
    },
    {
      "epoch": 0.2955519432540269,
      "grad_norm": 1.4495922327041626,
      "learning_rate": 0.0002972217881299033,
      "loss": 1.3412,
      "step": 500
    },
    {
      "epoch": 0.2955519432540269,
      "eval_loss": 1.3146368265151978,
      "eval_runtime": 216.4078,
      "eval_samples_per_second": 6.95,
      "eval_steps_per_second": 0.869,
      "step": 500
    },
    {
      "epoch": 0.30146298211910744,
      "grad_norm": 0.7506139874458313,
      "learning_rate": 0.00029703347215037315,
      "loss": 1.3206,
      "step": 510
    },
    {
      "epoch": 0.307374020984188,
      "grad_norm": 1.047113060951233,
      "learning_rate": 0.00029683904503930813,
      "loss": 1.1369,
      "step": 520
    },
    {
      "epoch": 0.3132850598492685,
      "grad_norm": 0.8574621677398682,
      "learning_rate": 0.0002966385148776549,
      "loss": 1.0607,
      "step": 530
    },
    {
      "epoch": 0.31919609871434906,
      "grad_norm": 1.2770711183547974,
      "learning_rate": 0.0002964318900000204,
      "loss": 1.4287,
      "step": 540
    },
    {
      "epoch": 0.3251071375794296,
      "grad_norm": 0.9782669544219971,
      "learning_rate": 0.00029621917899432546,
      "loss": 1.2887,
      "step": 550
    },
    {
      "epoch": 0.33101817644451015,
      "grad_norm": 0.8555631637573242,
      "learning_rate": 0.0002960003907014476,
      "loss": 1.0195,
      "step": 560
    },
    {
      "epoch": 0.33692921530959064,
      "grad_norm": 1.2670342922210693,
      "learning_rate": 0.0002957755342148539,
      "loss": 1.7348,
      "step": 570
    },
    {
      "epoch": 0.3428402541746712,
      "grad_norm": 0.8302950859069824,
      "learning_rate": 0.00029554461888022296,
      "loss": 1.5702,
      "step": 580
    },
    {
      "epoch": 0.3487512930397517,
      "grad_norm": 1.1769555807113647,
      "learning_rate": 0.00029530765429505636,
      "loss": 1.5087,
      "step": 590
    },
    {
      "epoch": 0.35466233190483226,
      "grad_norm": 1.7179492712020874,
      "learning_rate": 0.00029506465030827977,
      "loss": 1.1668,
      "step": 600
    },
    {
      "epoch": 0.3605733707699128,
      "grad_norm": 1.6589977741241455,
      "learning_rate": 0.0002948156170198338,
      "loss": 1.211,
      "step": 610
    },
    {
      "epoch": 0.36648440963499335,
      "grad_norm": 0.8197120428085327,
      "learning_rate": 0.000294560564780254,
      "loss": 1.3555,
      "step": 620
    },
    {
      "epoch": 0.3723954485000739,
      "grad_norm": 2.922220468521118,
      "learning_rate": 0.00029429950419024075,
      "loss": 1.7,
      "step": 630
    },
    {
      "epoch": 0.37830648736515443,
      "grad_norm": 0.39568033814430237,
      "learning_rate": 0.0002940324461002187,
      "loss": 1.1265,
      "step": 640
    },
    {
      "epoch": 0.384217526230235,
      "grad_norm": 0.767401933670044,
      "learning_rate": 0.0002937594016098856,
      "loss": 1.0005,
      "step": 650
    },
    {
      "epoch": 0.3901285650953155,
      "grad_norm": 0.4762462079524994,
      "learning_rate": 0.00029348038206775134,
      "loss": 1.0934,
      "step": 660
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 1.488071084022522,
      "learning_rate": 0.00029319539907066584,
      "loss": 1.5414,
      "step": 670
    },
    {
      "epoch": 0.4019506428254766,
      "grad_norm": 1.5370007753372192,
      "learning_rate": 0.0002929044644633374,
      "loss": 1.1256,
      "step": 680
    },
    {
      "epoch": 0.40786168169055714,
      "grad_norm": 2.742067813873291,
      "learning_rate": 0.0002926075903378402,
      "loss": 1.7847,
      "step": 690
    },
    {
      "epoch": 0.41377272055563763,
      "grad_norm": 0.6739020943641663,
      "learning_rate": 0.0002923047890331117,
      "loss": 1.2375,
      "step": 700
    },
    {
      "epoch": 0.41968375942071817,
      "grad_norm": 3.385519027709961,
      "learning_rate": 0.0002919960731344401,
      "loss": 1.0736,
      "step": 710
    },
    {
      "epoch": 0.4255947982857987,
      "grad_norm": 0.6010761857032776,
      "learning_rate": 0.00029168145547294085,
      "loss": 1.3674,
      "step": 720
    },
    {
      "epoch": 0.43150583715087926,
      "grad_norm": 0.5061453580856323,
      "learning_rate": 0.0002913609491250236,
      "loss": 0.7746,
      "step": 730
    },
    {
      "epoch": 0.4374168760159598,
      "grad_norm": 0.8537613153457642,
      "learning_rate": 0.00029103456741184854,
      "loss": 1.5548,
      "step": 740
    },
    {
      "epoch": 0.44332791488104034,
      "grad_norm": 0.6990466117858887,
      "learning_rate": 0.0002907023238987731,
      "loss": 1.1144,
      "step": 750
    },
    {
      "epoch": 0.4492389537461209,
      "grad_norm": 0.7418966293334961,
      "learning_rate": 0.0002903642323947876,
      "loss": 0.997,
      "step": 760
    },
    {
      "epoch": 0.4551499926112014,
      "grad_norm": 1.2348270416259766,
      "learning_rate": 0.0002900203069519417,
      "loss": 1.462,
      "step": 770
    },
    {
      "epoch": 0.46106103147628197,
      "grad_norm": 1.3047974109649658,
      "learning_rate": 0.00028967056186476024,
      "loss": 1.5589,
      "step": 780
    },
    {
      "epoch": 0.4669720703413625,
      "grad_norm": 1.1756370067596436,
      "learning_rate": 0.0002893150116696492,
      "loss": 1.2753,
      "step": 790
    },
    {
      "epoch": 0.47288310920644305,
      "grad_norm": 0.934016227722168,
      "learning_rate": 0.00028895367114429123,
      "loss": 0.9582,
      "step": 800
    },
    {
      "epoch": 0.4787941480715236,
      "grad_norm": 0.7867525219917297,
      "learning_rate": 0.0002885865553070319,
      "loss": 1.3415,
      "step": 810
    },
    {
      "epoch": 0.4847051869366041,
      "grad_norm": 0.7471956014633179,
      "learning_rate": 0.0002882136794162551,
      "loss": 1.0717,
      "step": 820
    },
    {
      "epoch": 0.4906162258016846,
      "grad_norm": 1.183523178100586,
      "learning_rate": 0.000287835058969749,
      "loss": 1.1274,
      "step": 830
    },
    {
      "epoch": 0.49652726466676517,
      "grad_norm": 2.128274440765381,
      "learning_rate": 0.00028745070970406214,
      "loss": 1.3969,
      "step": 840
    },
    {
      "epoch": 0.5024383035318457,
      "grad_norm": 0.6979519128799438,
      "learning_rate": 0.00028706064759384894,
      "loss": 1.1142,
      "step": 850
    },
    {
      "epoch": 0.5083493423969263,
      "grad_norm": 0.5899310111999512,
      "learning_rate": 0.000286664888851206,
      "loss": 0.7925,
      "step": 860
    },
    {
      "epoch": 0.5142603812620068,
      "grad_norm": 1.3585809469223022,
      "learning_rate": 0.0002862634499249985,
      "loss": 0.8093,
      "step": 870
    },
    {
      "epoch": 0.5201714201270873,
      "grad_norm": 0.9386045932769775,
      "learning_rate": 0.0002858563475001759,
      "loss": 1.0482,
      "step": 880
    },
    {
      "epoch": 0.5260824589921679,
      "grad_norm": 1.514111876487732,
      "learning_rate": 0.0002854435984970792,
      "loss": 1.3709,
      "step": 890
    },
    {
      "epoch": 0.5319934978572484,
      "grad_norm": 0.592781662940979,
      "learning_rate": 0.000285025220070737,
      "loss": 1.083,
      "step": 900
    },
    {
      "epoch": 0.537904536722329,
      "grad_norm": 0.8946014046669006,
      "learning_rate": 0.00028460122961015326,
      "loss": 0.9572,
      "step": 910
    },
    {
      "epoch": 0.5438155755874094,
      "grad_norm": 0.5024324059486389,
      "learning_rate": 0.00028417164473758374,
      "loss": 1.02,
      "step": 920
    },
    {
      "epoch": 0.54972661445249,
      "grad_norm": 0.688870370388031,
      "learning_rate": 0.0002837364833078042,
      "loss": 1.0437,
      "step": 930
    },
    {
      "epoch": 0.5556376533175705,
      "grad_norm": 0.8181172609329224,
      "learning_rate": 0.0002832957634073679,
      "loss": 1.4931,
      "step": 940
    },
    {
      "epoch": 0.5615486921826511,
      "grad_norm": 2.1315081119537354,
      "learning_rate": 0.00028284950335385415,
      "loss": 1.4226,
      "step": 950
    },
    {
      "epoch": 0.5674597310477316,
      "grad_norm": 1.5933620929718018,
      "learning_rate": 0.0002823977216951068,
      "loss": 0.72,
      "step": 960
    },
    {
      "epoch": 0.5733707699128122,
      "grad_norm": 3.1443240642547607,
      "learning_rate": 0.00028194043720846355,
      "loss": 1.3788,
      "step": 970
    },
    {
      "epoch": 0.5792818087778927,
      "grad_norm": 0.34296682476997375,
      "learning_rate": 0.00028147766889997504,
      "loss": 0.909,
      "step": 980
    },
    {
      "epoch": 0.5851928476429733,
      "grad_norm": 0.8706747889518738,
      "learning_rate": 0.0002810094360036155,
      "loss": 0.8898,
      "step": 990
    },
    {
      "epoch": 0.5911038865080538,
      "grad_norm": 1.1580030918121338,
      "learning_rate": 0.0002805357579804831,
      "loss": 1.5932,
      "step": 1000
    },
    {
      "epoch": 0.5911038865080538,
      "eval_loss": 1.0840506553649902,
      "eval_runtime": 217.1875,
      "eval_samples_per_second": 6.925,
      "eval_steps_per_second": 0.866,
      "step": 1000
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 1.5580549240112305,
      "learning_rate": 0.0002800566545179908,
      "loss": 1.2045,
      "step": 1010
    },
    {
      "epoch": 0.6029259642382149,
      "grad_norm": 1.199741005897522,
      "learning_rate": 0.0002795721455290485,
      "loss": 1.5051,
      "step": 1020
    },
    {
      "epoch": 0.6088370031032954,
      "grad_norm": 0.39826700091362,
      "learning_rate": 0.0002790822511512352,
      "loss": 0.9564,
      "step": 1030
    },
    {
      "epoch": 0.614748041968376,
      "grad_norm": 0.8212458491325378,
      "learning_rate": 0.00027858699174596215,
      "loss": 0.8803,
      "step": 1040
    },
    {
      "epoch": 0.6206590808334564,
      "grad_norm": 2.1567301750183105,
      "learning_rate": 0.00027808638789762616,
      "loss": 1.2964,
      "step": 1050
    },
    {
      "epoch": 0.626570119698537,
      "grad_norm": 0.815433919429779,
      "learning_rate": 0.0002775804604127549,
      "loss": 1.2126,
      "step": 1060
    },
    {
      "epoch": 0.6324811585636175,
      "grad_norm": 1.2142722606658936,
      "learning_rate": 0.00027706923031914105,
      "loss": 1.1671,
      "step": 1070
    },
    {
      "epoch": 0.6383921974286981,
      "grad_norm": 1.4374685287475586,
      "learning_rate": 0.00027655271886496935,
      "loss": 1.0652,
      "step": 1080
    },
    {
      "epoch": 0.6443032362937786,
      "grad_norm": 2.028388023376465,
      "learning_rate": 0.0002760309475179326,
      "loss": 1.3826,
      "step": 1090
    },
    {
      "epoch": 0.6502142751588592,
      "grad_norm": 1.4759591817855835,
      "learning_rate": 0.00027550393796434,
      "loss": 1.1697,
      "step": 1100
    },
    {
      "epoch": 0.6561253140239397,
      "grad_norm": 1.076900839805603,
      "learning_rate": 0.0002749717121082156,
      "loss": 0.9173,
      "step": 1110
    },
    {
      "epoch": 0.6620363528890203,
      "grad_norm": 1.2914588451385498,
      "learning_rate": 0.0002744342920703878,
      "loss": 0.9848,
      "step": 1120
    },
    {
      "epoch": 0.6679473917541008,
      "grad_norm": 0.7284201979637146,
      "learning_rate": 0.0002738917001875701,
      "loss": 1.1538,
      "step": 1130
    },
    {
      "epoch": 0.6738584306191813,
      "grad_norm": 1.2725015878677368,
      "learning_rate": 0.0002733439590114326,
      "loss": 1.0447,
      "step": 1140
    },
    {
      "epoch": 0.6797694694842619,
      "grad_norm": 0.8510275483131409,
      "learning_rate": 0.00027279109130766486,
      "loss": 1.1817,
      "step": 1150
    },
    {
      "epoch": 0.6856805083493424,
      "grad_norm": 1.7715678215026855,
      "learning_rate": 0.0002722331200550296,
      "loss": 1.122,
      "step": 1160
    },
    {
      "epoch": 0.691591547214423,
      "grad_norm": 1.3184545040130615,
      "learning_rate": 0.00027167006844440743,
      "loss": 1.3256,
      "step": 1170
    },
    {
      "epoch": 0.6975025860795034,
      "grad_norm": 1.1101163625717163,
      "learning_rate": 0.0002711019598778334,
      "loss": 0.8867,
      "step": 1180
    },
    {
      "epoch": 0.703413624944584,
      "grad_norm": 1.0922558307647705,
      "learning_rate": 0.0002705288179675239,
      "loss": 1.2342,
      "step": 1190
    },
    {
      "epoch": 0.7093246638096645,
      "grad_norm": 1.424505591392517,
      "learning_rate": 0.0002699506665348957,
      "loss": 1.0746,
      "step": 1200
    },
    {
      "epoch": 0.7152357026747451,
      "grad_norm": 0.6099621653556824,
      "learning_rate": 0.0002693675296095755,
      "loss": 1.0473,
      "step": 1210
    },
    {
      "epoch": 0.7211467415398256,
      "grad_norm": 1.2074164152145386,
      "learning_rate": 0.0002687794314284013,
      "loss": 1.1465,
      "step": 1220
    },
    {
      "epoch": 0.7270577804049062,
      "grad_norm": 0.6486126780509949,
      "learning_rate": 0.00026818639643441514,
      "loss": 0.9935,
      "step": 1230
    },
    {
      "epoch": 0.7329688192699867,
      "grad_norm": 0.9025202393531799,
      "learning_rate": 0.0002675884492758472,
      "loss": 0.7493,
      "step": 1240
    },
    {
      "epoch": 0.7388798581350673,
      "grad_norm": 1.1157371997833252,
      "learning_rate": 0.000266985614805091,
      "loss": 1.1788,
      "step": 1250
    },
    {
      "epoch": 0.7447908970001478,
      "grad_norm": 1.0885167121887207,
      "learning_rate": 0.00026637791807767117,
      "loss": 1.0209,
      "step": 1260
    },
    {
      "epoch": 0.7507019358652283,
      "grad_norm": 2.4428985118865967,
      "learning_rate": 0.00026576538435120115,
      "loss": 0.8735,
      "step": 1270
    },
    {
      "epoch": 0.7566129747303089,
      "grad_norm": 0.8540585041046143,
      "learning_rate": 0.00026514803908433423,
      "loss": 0.7358,
      "step": 1280
    },
    {
      "epoch": 0.7625240135953893,
      "grad_norm": 0.8274838328361511,
      "learning_rate": 0.00026452590793570486,
      "loss": 1.1572,
      "step": 1290
    },
    {
      "epoch": 0.76843505246047,
      "grad_norm": 0.8122208714485168,
      "learning_rate": 0.00026389901676286264,
      "loss": 1.1574,
      "step": 1300
    },
    {
      "epoch": 0.7743460913255504,
      "grad_norm": 0.8998039364814758,
      "learning_rate": 0.00026326739162119714,
      "loss": 1.3476,
      "step": 1310
    },
    {
      "epoch": 0.780257130190631,
      "grad_norm": 1.0652897357940674,
      "learning_rate": 0.0002626310587628554,
      "loss": 0.7719,
      "step": 1320
    },
    {
      "epoch": 0.7861681690557115,
      "grad_norm": 1.297702431678772,
      "learning_rate": 0.0002619900446356506,
      "loss": 0.9897,
      "step": 1330
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 2.369802951812744,
      "learning_rate": 0.00026134437588196276,
      "loss": 0.8323,
      "step": 1340
    },
    {
      "epoch": 0.7979902467858726,
      "grad_norm": 1.0665628910064697,
      "learning_rate": 0.0002606940793376315,
      "loss": 0.9712,
      "step": 1350
    },
    {
      "epoch": 0.8039012856509532,
      "grad_norm": 0.6863260269165039,
      "learning_rate": 0.00026003918203084075,
      "loss": 0.8202,
      "step": 1360
    },
    {
      "epoch": 0.8098123245160337,
      "grad_norm": 1.0297291278839111,
      "learning_rate": 0.0002593797111809952,
      "loss": 0.8177,
      "step": 1370
    },
    {
      "epoch": 0.8157233633811143,
      "grad_norm": 0.8568499088287354,
      "learning_rate": 0.0002587156941975891,
      "loss": 0.9803,
      "step": 1380
    },
    {
      "epoch": 0.8216344022461948,
      "grad_norm": 1.5378499031066895,
      "learning_rate": 0.00025804715867906687,
      "loss": 1.3927,
      "step": 1390
    },
    {
      "epoch": 0.8275454411112753,
      "grad_norm": 1.4208747148513794,
      "learning_rate": 0.0002573741324116764,
      "loss": 0.9625,
      "step": 1400
    },
    {
      "epoch": 0.8334564799763559,
      "grad_norm": 0.7616665363311768,
      "learning_rate": 0.0002566966433683138,
      "loss": 0.8857,
      "step": 1410
    },
    {
      "epoch": 0.8393675188414363,
      "grad_norm": 0.5649605393409729,
      "learning_rate": 0.00025601471970736103,
      "loss": 1.0454,
      "step": 1420
    },
    {
      "epoch": 0.8452785577065169,
      "grad_norm": 1.8511734008789062,
      "learning_rate": 0.0002553283897715152,
      "loss": 0.9013,
      "step": 1430
    },
    {
      "epoch": 0.8511895965715974,
      "grad_norm": 1.551117181777954,
      "learning_rate": 0.0002546376820866111,
      "loss": 1.0111,
      "step": 1440
    },
    {
      "epoch": 0.857100635436678,
      "grad_norm": 2.121812582015991,
      "learning_rate": 0.00025394262536043504,
      "loss": 1.0325,
      "step": 1450
    },
    {
      "epoch": 0.8630116743017585,
      "grad_norm": 2.537290573120117,
      "learning_rate": 0.0002532432484815321,
      "loss": 0.904,
      "step": 1460
    },
    {
      "epoch": 0.8689227131668391,
      "grad_norm": 1.2138627767562866,
      "learning_rate": 0.0002525395805180052,
      "loss": 0.8901,
      "step": 1470
    },
    {
      "epoch": 0.8748337520319196,
      "grad_norm": 0.9661867022514343,
      "learning_rate": 0.00025183165071630695,
      "loss": 0.8248,
      "step": 1480
    },
    {
      "epoch": 0.8807447908970002,
      "grad_norm": 0.527542769908905,
      "learning_rate": 0.00025111948850002413,
      "loss": 0.9494,
      "step": 1490
    },
    {
      "epoch": 0.8866558297620807,
      "grad_norm": 1.1769733428955078,
      "learning_rate": 0.00025040312346865496,
      "loss": 0.9115,
      "step": 1500
    },
    {
      "epoch": 0.8866558297620807,
      "eval_loss": 0.966824471950531,
      "eval_runtime": 217.3109,
      "eval_samples_per_second": 6.921,
      "eval_steps_per_second": 0.865,
      "step": 1500
    },
    {
      "epoch": 0.8925668686271612,
      "grad_norm": 0.8589854836463928,
      "learning_rate": 0.00024968258539637844,
      "loss": 0.8922,
      "step": 1510
    },
    {
      "epoch": 0.8984779074922418,
      "grad_norm": 0.6945375204086304,
      "learning_rate": 0.0002489579042308172,
      "loss": 0.7376,
      "step": 1520
    },
    {
      "epoch": 0.9043889463573223,
      "grad_norm": 0.36732661724090576,
      "learning_rate": 0.00024822911009179276,
      "loss": 0.8396,
      "step": 1530
    },
    {
      "epoch": 0.9102999852224029,
      "grad_norm": 1.337815761566162,
      "learning_rate": 0.00024749623327007333,
      "loss": 0.5952,
      "step": 1540
    },
    {
      "epoch": 0.9162110240874833,
      "grad_norm": 0.7494143843650818,
      "learning_rate": 0.00024675930422611526,
      "loss": 0.9009,
      "step": 1550
    },
    {
      "epoch": 0.9221220629525639,
      "grad_norm": 2.027194023132324,
      "learning_rate": 0.00024601835358879694,
      "loss": 0.8604,
      "step": 1560
    },
    {
      "epoch": 0.9280331018176444,
      "grad_norm": 0.3332920968532562,
      "learning_rate": 0.00024527341215414546,
      "loss": 0.8194,
      "step": 1570
    },
    {
      "epoch": 0.933944140682725,
      "grad_norm": 3.1836729049682617,
      "learning_rate": 0.00024452451088405693,
      "loss": 1.1178,
      "step": 1580
    },
    {
      "epoch": 0.9398551795478055,
      "grad_norm": 1.0708240270614624,
      "learning_rate": 0.00024377168090500956,
      "loss": 1.0784,
      "step": 1590
    },
    {
      "epoch": 0.9457662184128861,
      "grad_norm": 2.001142740249634,
      "learning_rate": 0.00024301495350677,
      "loss": 1.1588,
      "step": 1600
    },
    {
      "epoch": 0.9516772572779666,
      "grad_norm": 0.5157231092453003,
      "learning_rate": 0.00024225436014109272,
      "loss": 0.9851,
      "step": 1610
    },
    {
      "epoch": 0.9575882961430472,
      "grad_norm": 1.1543854475021362,
      "learning_rate": 0.0002414899324204129,
      "loss": 1.0326,
      "step": 1620
    },
    {
      "epoch": 0.9634993350081277,
      "grad_norm": 1.1339360475540161,
      "learning_rate": 0.00024072170211653237,
      "loss": 1.0898,
      "step": 1630
    },
    {
      "epoch": 0.9694103738732082,
      "grad_norm": 1.0551549196243286,
      "learning_rate": 0.00023994970115929928,
      "loss": 1.0719,
      "step": 1640
    },
    {
      "epoch": 0.9753214127382888,
      "grad_norm": 0.7559623718261719,
      "learning_rate": 0.00023917396163528092,
      "loss": 0.5571,
      "step": 1650
    },
    {
      "epoch": 0.9812324516033692,
      "grad_norm": 1.2452393770217896,
      "learning_rate": 0.00023839451578643004,
      "loss": 1.0676,
      "step": 1660
    },
    {
      "epoch": 0.9871434904684498,
      "grad_norm": 0.6703391075134277,
      "learning_rate": 0.00023761139600874496,
      "loss": 0.6167,
      "step": 1670
    },
    {
      "epoch": 0.9930545293335303,
      "grad_norm": 2.0774176120758057,
      "learning_rate": 0.00023682463485092287,
      "loss": 1.1952,
      "step": 1680
    },
    {
      "epoch": 0.9989655681986109,
      "grad_norm": 0.8266769647598267,
      "learning_rate": 0.00023603426501300718,
      "loss": 0.7562,
      "step": 1690
    }
  ],
  "logging_steps": 10,
  "max_steps": 5073,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.136778086121472e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
