{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9995566720851192,
  "eval_steps": 500,
  "global_step": 5073,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005911038865080538,
      "grad_norm": 0.2276100516319275,
      "learning_rate": 1.3499999999999998e-05,
      "loss": 3.9956,
      "step": 10
    },
    {
      "epoch": 0.011822077730161076,
      "grad_norm": 0.28873324394226074,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 4.0319,
      "step": 20
    },
    {
      "epoch": 0.017733116595241613,
      "grad_norm": 0.33441638946533203,
      "learning_rate": 4.3499999999999993e-05,
      "loss": 4.2832,
      "step": 30
    },
    {
      "epoch": 0.023644155460322152,
      "grad_norm": 0.4686412811279297,
      "learning_rate": 5.85e-05,
      "loss": 3.9913,
      "step": 40
    },
    {
      "epoch": 0.02955519432540269,
      "grad_norm": 0.4693009853363037,
      "learning_rate": 7.35e-05,
      "loss": 3.7657,
      "step": 50
    },
    {
      "epoch": 0.035466233190483226,
      "grad_norm": 0.41090336441993713,
      "learning_rate": 8.849999999999998e-05,
      "loss": 3.6837,
      "step": 60
    },
    {
      "epoch": 0.04137727205556377,
      "grad_norm": 0.5891649127006531,
      "learning_rate": 0.00010349999999999998,
      "loss": 3.7584,
      "step": 70
    },
    {
      "epoch": 0.047288310920644304,
      "grad_norm": 0.65283203125,
      "learning_rate": 0.0001185,
      "loss": 3.4932,
      "step": 80
    },
    {
      "epoch": 0.05319934978572484,
      "grad_norm": 0.9791437983512878,
      "learning_rate": 0.0001335,
      "loss": 3.3853,
      "step": 90
    },
    {
      "epoch": 0.05911038865080538,
      "grad_norm": 0.8459473252296448,
      "learning_rate": 0.00014849999999999998,
      "loss": 2.8556,
      "step": 100
    },
    {
      "epoch": 0.06502142751588591,
      "grad_norm": 1.0254615545272827,
      "learning_rate": 0.0001635,
      "loss": 2.8705,
      "step": 110
    },
    {
      "epoch": 0.07093246638096645,
      "grad_norm": 0.929900050163269,
      "learning_rate": 0.00017849999999999997,
      "loss": 2.4165,
      "step": 120
    },
    {
      "epoch": 0.076843505246047,
      "grad_norm": 0.9569188952445984,
      "learning_rate": 0.0001935,
      "loss": 2.2081,
      "step": 130
    },
    {
      "epoch": 0.08275454411112754,
      "grad_norm": 1.4186241626739502,
      "learning_rate": 0.00020849999999999997,
      "loss": 1.972,
      "step": 140
    },
    {
      "epoch": 0.08866558297620807,
      "grad_norm": 1.543451189994812,
      "learning_rate": 0.00022349999999999998,
      "loss": 2.0021,
      "step": 150
    },
    {
      "epoch": 0.09457662184128861,
      "grad_norm": 1.427973747253418,
      "learning_rate": 0.0002385,
      "loss": 2.3088,
      "step": 160
    },
    {
      "epoch": 0.10048766070636915,
      "grad_norm": 1.124901533126831,
      "learning_rate": 0.0002535,
      "loss": 2.1868,
      "step": 170
    },
    {
      "epoch": 0.10639869957144968,
      "grad_norm": 0.8841444849967957,
      "learning_rate": 0.00026849999999999997,
      "loss": 1.5767,
      "step": 180
    },
    {
      "epoch": 0.11230973843653022,
      "grad_norm": 1.7946240901947021,
      "learning_rate": 0.00028349999999999995,
      "loss": 1.8317,
      "step": 190
    },
    {
      "epoch": 0.11822077730161076,
      "grad_norm": 0.7886484265327454,
      "learning_rate": 0.0002985,
      "loss": 1.5576,
      "step": 200
    },
    {
      "epoch": 0.12413181616669129,
      "grad_norm": 1.8757169246673584,
      "learning_rate": 0.00029999747505462783,
      "loss": 1.5806,
      "step": 210
    },
    {
      "epoch": 0.13004285503177182,
      "grad_norm": 0.6265178322792053,
      "learning_rate": 0.00029998874695753624,
      "loss": 1.885,
      "step": 220
    },
    {
      "epoch": 0.13595389389685236,
      "grad_norm": 0.8252459168434143,
      "learning_rate": 0.00029997378489923836,
      "loss": 1.7116,
      "step": 230
    },
    {
      "epoch": 0.1418649327619329,
      "grad_norm": 0.6410897970199585,
      "learning_rate": 0.0002999525895016001,
      "loss": 1.3141,
      "step": 240
    },
    {
      "epoch": 0.14777597162701345,
      "grad_norm": 0.5499432682991028,
      "learning_rate": 0.00029992516164556297,
      "loss": 1.81,
      "step": 250
    },
    {
      "epoch": 0.153687010492094,
      "grad_norm": 2.034027338027954,
      "learning_rate": 0.00029989150247110687,
      "loss": 1.6135,
      "step": 260
    },
    {
      "epoch": 0.15959804935717453,
      "grad_norm": 0.771909236907959,
      "learning_rate": 0.0002998516133772035,
      "loss": 1.5172,
      "step": 270
    },
    {
      "epoch": 0.16550908822225507,
      "grad_norm": 0.49242275953292847,
      "learning_rate": 0.0002998054960217575,
      "loss": 1.164,
      "step": 280
    },
    {
      "epoch": 0.1714201270873356,
      "grad_norm": 0.7855795621871948,
      "learning_rate": 0.0002997531523215382,
      "loss": 1.245,
      "step": 290
    },
    {
      "epoch": 0.17733116595241613,
      "grad_norm": 0.6119864583015442,
      "learning_rate": 0.00029969458445209933,
      "loss": 1.24,
      "step": 300
    },
    {
      "epoch": 0.18324220481749667,
      "grad_norm": 1.9419697523117065,
      "learning_rate": 0.0002996297948476891,
      "loss": 1.8625,
      "step": 310
    },
    {
      "epoch": 0.18915324368257722,
      "grad_norm": 1.0362036228179932,
      "learning_rate": 0.00029955878620114873,
      "loss": 1.3991,
      "step": 320
    },
    {
      "epoch": 0.19506428254765776,
      "grad_norm": 2.193244695663452,
      "learning_rate": 0.00029948156146380057,
      "loss": 1.2342,
      "step": 330
    },
    {
      "epoch": 0.2009753214127383,
      "grad_norm": 1.1690107583999634,
      "learning_rate": 0.0002993981238453255,
      "loss": 1.2931,
      "step": 340
    },
    {
      "epoch": 0.20688636027781881,
      "grad_norm": 0.3863016366958618,
      "learning_rate": 0.0002993084768136296,
      "loss": 1.1641,
      "step": 350
    },
    {
      "epoch": 0.21279739914289936,
      "grad_norm": 2.0656979084014893,
      "learning_rate": 0.0002992126240946998,
      "loss": 1.8293,
      "step": 360
    },
    {
      "epoch": 0.2187084380079799,
      "grad_norm": 1.1432462930679321,
      "learning_rate": 0.0002991105696724492,
      "loss": 1.6783,
      "step": 370
    },
    {
      "epoch": 0.22461947687306044,
      "grad_norm": 0.9850197434425354,
      "learning_rate": 0.00029900231778855137,
      "loss": 1.4363,
      "step": 380
    },
    {
      "epoch": 0.23053051573814098,
      "grad_norm": 0.4927749037742615,
      "learning_rate": 0.00029888787294226417,
      "loss": 1.1233,
      "step": 390
    },
    {
      "epoch": 0.23644155460322153,
      "grad_norm": 1.6659817695617676,
      "learning_rate": 0.0002987672398902427,
      "loss": 1.7724,
      "step": 400
    },
    {
      "epoch": 0.24235259346830204,
      "grad_norm": 0.7641460299491882,
      "learning_rate": 0.00029864042364634145,
      "loss": 1.4903,
      "step": 410
    },
    {
      "epoch": 0.24826363233338258,
      "grad_norm": 2.1336848735809326,
      "learning_rate": 0.00029850742948140627,
      "loss": 1.3408,
      "step": 420
    },
    {
      "epoch": 0.25417467119846315,
      "grad_norm": 1.7345354557037354,
      "learning_rate": 0.00029836826292305483,
      "loss": 1.5539,
      "step": 430
    },
    {
      "epoch": 0.26008571006354364,
      "grad_norm": 1.8847336769104004,
      "learning_rate": 0.0002982229297554473,
      "loss": 1.297,
      "step": 440
    },
    {
      "epoch": 0.2659967489286242,
      "grad_norm": 0.5554383397102356,
      "learning_rate": 0.0002980714360190456,
      "loss": 1.441,
      "step": 450
    },
    {
      "epoch": 0.2719077877937047,
      "grad_norm": 0.8399075269699097,
      "learning_rate": 0.0002979137880103627,
      "loss": 1.3417,
      "step": 460
    },
    {
      "epoch": 0.27781882665878527,
      "grad_norm": 1.0724455118179321,
      "learning_rate": 0.00029774999228170064,
      "loss": 1.328,
      "step": 470
    },
    {
      "epoch": 0.2837298655238658,
      "grad_norm": 2.457491636276245,
      "learning_rate": 0.00029758005564087817,
      "loss": 1.3979,
      "step": 480
    },
    {
      "epoch": 0.28964090438894635,
      "grad_norm": 0.5214126706123352,
      "learning_rate": 0.0002974039851509481,
      "loss": 1.1475,
      "step": 490
    },
    {
      "epoch": 0.2955519432540269,
      "grad_norm": 1.4495922327041626,
      "learning_rate": 0.0002972217881299033,
      "loss": 1.3412,
      "step": 500
    },
    {
      "epoch": 0.2955519432540269,
      "eval_loss": 1.3146368265151978,
      "eval_runtime": 216.4078,
      "eval_samples_per_second": 6.95,
      "eval_steps_per_second": 0.869,
      "step": 500
    },
    {
      "epoch": 0.30146298211910744,
      "grad_norm": 0.7506139874458313,
      "learning_rate": 0.00029703347215037315,
      "loss": 1.3206,
      "step": 510
    },
    {
      "epoch": 0.307374020984188,
      "grad_norm": 1.047113060951233,
      "learning_rate": 0.00029683904503930813,
      "loss": 1.1369,
      "step": 520
    },
    {
      "epoch": 0.3132850598492685,
      "grad_norm": 0.8574621677398682,
      "learning_rate": 0.0002966385148776549,
      "loss": 1.0607,
      "step": 530
    },
    {
      "epoch": 0.31919609871434906,
      "grad_norm": 1.2770711183547974,
      "learning_rate": 0.0002964318900000204,
      "loss": 1.4287,
      "step": 540
    },
    {
      "epoch": 0.3251071375794296,
      "grad_norm": 0.9782669544219971,
      "learning_rate": 0.00029621917899432546,
      "loss": 1.2887,
      "step": 550
    },
    {
      "epoch": 0.33101817644451015,
      "grad_norm": 0.8555631637573242,
      "learning_rate": 0.0002960003907014476,
      "loss": 1.0195,
      "step": 560
    },
    {
      "epoch": 0.33692921530959064,
      "grad_norm": 1.2670342922210693,
      "learning_rate": 0.0002957755342148539,
      "loss": 1.7348,
      "step": 570
    },
    {
      "epoch": 0.3428402541746712,
      "grad_norm": 0.8302950859069824,
      "learning_rate": 0.00029554461888022296,
      "loss": 1.5702,
      "step": 580
    },
    {
      "epoch": 0.3487512930397517,
      "grad_norm": 1.1769555807113647,
      "learning_rate": 0.00029530765429505636,
      "loss": 1.5087,
      "step": 590
    },
    {
      "epoch": 0.35466233190483226,
      "grad_norm": 1.7179492712020874,
      "learning_rate": 0.00029506465030827977,
      "loss": 1.1668,
      "step": 600
    },
    {
      "epoch": 0.3605733707699128,
      "grad_norm": 1.6589977741241455,
      "learning_rate": 0.0002948156170198338,
      "loss": 1.211,
      "step": 610
    },
    {
      "epoch": 0.36648440963499335,
      "grad_norm": 0.8197120428085327,
      "learning_rate": 0.000294560564780254,
      "loss": 1.3555,
      "step": 620
    },
    {
      "epoch": 0.3723954485000739,
      "grad_norm": 2.922220468521118,
      "learning_rate": 0.00029429950419024075,
      "loss": 1.7,
      "step": 630
    },
    {
      "epoch": 0.37830648736515443,
      "grad_norm": 0.39568033814430237,
      "learning_rate": 0.0002940324461002187,
      "loss": 1.1265,
      "step": 640
    },
    {
      "epoch": 0.384217526230235,
      "grad_norm": 0.767401933670044,
      "learning_rate": 0.0002937594016098856,
      "loss": 1.0005,
      "step": 650
    },
    {
      "epoch": 0.3901285650953155,
      "grad_norm": 0.4762462079524994,
      "learning_rate": 0.00029348038206775134,
      "loss": 1.0934,
      "step": 660
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 1.488071084022522,
      "learning_rate": 0.00029319539907066584,
      "loss": 1.5414,
      "step": 670
    },
    {
      "epoch": 0.4019506428254766,
      "grad_norm": 1.5370007753372192,
      "learning_rate": 0.0002929044644633374,
      "loss": 1.1256,
      "step": 680
    },
    {
      "epoch": 0.40786168169055714,
      "grad_norm": 2.742067813873291,
      "learning_rate": 0.0002926075903378402,
      "loss": 1.7847,
      "step": 690
    },
    {
      "epoch": 0.41377272055563763,
      "grad_norm": 0.6739020943641663,
      "learning_rate": 0.0002923047890331117,
      "loss": 1.2375,
      "step": 700
    },
    {
      "epoch": 0.41968375942071817,
      "grad_norm": 3.385519027709961,
      "learning_rate": 0.0002919960731344401,
      "loss": 1.0736,
      "step": 710
    },
    {
      "epoch": 0.4255947982857987,
      "grad_norm": 0.6010761857032776,
      "learning_rate": 0.00029168145547294085,
      "loss": 1.3674,
      "step": 720
    },
    {
      "epoch": 0.43150583715087926,
      "grad_norm": 0.5061453580856323,
      "learning_rate": 0.0002913609491250236,
      "loss": 0.7746,
      "step": 730
    },
    {
      "epoch": 0.4374168760159598,
      "grad_norm": 0.8537613153457642,
      "learning_rate": 0.00029103456741184854,
      "loss": 1.5548,
      "step": 740
    },
    {
      "epoch": 0.44332791488104034,
      "grad_norm": 0.6990466117858887,
      "learning_rate": 0.0002907023238987731,
      "loss": 1.1144,
      "step": 750
    },
    {
      "epoch": 0.4492389537461209,
      "grad_norm": 0.7418966293334961,
      "learning_rate": 0.0002903642323947876,
      "loss": 0.997,
      "step": 760
    },
    {
      "epoch": 0.4551499926112014,
      "grad_norm": 1.2348270416259766,
      "learning_rate": 0.0002900203069519417,
      "loss": 1.462,
      "step": 770
    },
    {
      "epoch": 0.46106103147628197,
      "grad_norm": 1.3047974109649658,
      "learning_rate": 0.00028967056186476024,
      "loss": 1.5589,
      "step": 780
    },
    {
      "epoch": 0.4669720703413625,
      "grad_norm": 1.1756370067596436,
      "learning_rate": 0.0002893150116696492,
      "loss": 1.2753,
      "step": 790
    },
    {
      "epoch": 0.47288310920644305,
      "grad_norm": 0.934016227722168,
      "learning_rate": 0.00028895367114429123,
      "loss": 0.9582,
      "step": 800
    },
    {
      "epoch": 0.4787941480715236,
      "grad_norm": 0.7867525219917297,
      "learning_rate": 0.0002885865553070319,
      "loss": 1.3415,
      "step": 810
    },
    {
      "epoch": 0.4847051869366041,
      "grad_norm": 0.7471956014633179,
      "learning_rate": 0.0002882136794162551,
      "loss": 1.0717,
      "step": 820
    },
    {
      "epoch": 0.4906162258016846,
      "grad_norm": 1.183523178100586,
      "learning_rate": 0.000287835058969749,
      "loss": 1.1274,
      "step": 830
    },
    {
      "epoch": 0.49652726466676517,
      "grad_norm": 2.128274440765381,
      "learning_rate": 0.00028745070970406214,
      "loss": 1.3969,
      "step": 840
    },
    {
      "epoch": 0.5024383035318457,
      "grad_norm": 0.6979519128799438,
      "learning_rate": 0.00028706064759384894,
      "loss": 1.1142,
      "step": 850
    },
    {
      "epoch": 0.5083493423969263,
      "grad_norm": 0.5899310111999512,
      "learning_rate": 0.000286664888851206,
      "loss": 0.7925,
      "step": 860
    },
    {
      "epoch": 0.5142603812620068,
      "grad_norm": 1.3585809469223022,
      "learning_rate": 0.0002862634499249985,
      "loss": 0.8093,
      "step": 870
    },
    {
      "epoch": 0.5201714201270873,
      "grad_norm": 0.9386045932769775,
      "learning_rate": 0.0002858563475001759,
      "loss": 1.0482,
      "step": 880
    },
    {
      "epoch": 0.5260824589921679,
      "grad_norm": 1.514111876487732,
      "learning_rate": 0.0002854435984970792,
      "loss": 1.3709,
      "step": 890
    },
    {
      "epoch": 0.5319934978572484,
      "grad_norm": 0.592781662940979,
      "learning_rate": 0.000285025220070737,
      "loss": 1.083,
      "step": 900
    },
    {
      "epoch": 0.537904536722329,
      "grad_norm": 0.8946014046669006,
      "learning_rate": 0.00028460122961015326,
      "loss": 0.9572,
      "step": 910
    },
    {
      "epoch": 0.5438155755874094,
      "grad_norm": 0.5024324059486389,
      "learning_rate": 0.00028417164473758374,
      "loss": 1.02,
      "step": 920
    },
    {
      "epoch": 0.54972661445249,
      "grad_norm": 0.688870370388031,
      "learning_rate": 0.0002837364833078042,
      "loss": 1.0437,
      "step": 930
    },
    {
      "epoch": 0.5556376533175705,
      "grad_norm": 0.8181172609329224,
      "learning_rate": 0.0002832957634073679,
      "loss": 1.4931,
      "step": 940
    },
    {
      "epoch": 0.5615486921826511,
      "grad_norm": 2.1315081119537354,
      "learning_rate": 0.00028284950335385415,
      "loss": 1.4226,
      "step": 950
    },
    {
      "epoch": 0.5674597310477316,
      "grad_norm": 1.5933620929718018,
      "learning_rate": 0.0002823977216951068,
      "loss": 0.72,
      "step": 960
    },
    {
      "epoch": 0.5733707699128122,
      "grad_norm": 3.1443240642547607,
      "learning_rate": 0.00028194043720846355,
      "loss": 1.3788,
      "step": 970
    },
    {
      "epoch": 0.5792818087778927,
      "grad_norm": 0.34296682476997375,
      "learning_rate": 0.00028147766889997504,
      "loss": 0.909,
      "step": 980
    },
    {
      "epoch": 0.5851928476429733,
      "grad_norm": 0.8706747889518738,
      "learning_rate": 0.0002810094360036155,
      "loss": 0.8898,
      "step": 990
    },
    {
      "epoch": 0.5911038865080538,
      "grad_norm": 1.1580030918121338,
      "learning_rate": 0.0002805357579804831,
      "loss": 1.5932,
      "step": 1000
    },
    {
      "epoch": 0.5911038865080538,
      "eval_loss": 1.0840506553649902,
      "eval_runtime": 217.1875,
      "eval_samples_per_second": 6.925,
      "eval_steps_per_second": 0.866,
      "step": 1000
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 1.5580549240112305,
      "learning_rate": 0.0002800566545179908,
      "loss": 1.2045,
      "step": 1010
    },
    {
      "epoch": 0.6029259642382149,
      "grad_norm": 1.199741005897522,
      "learning_rate": 0.0002795721455290485,
      "loss": 1.5051,
      "step": 1020
    },
    {
      "epoch": 0.6088370031032954,
      "grad_norm": 0.39826700091362,
      "learning_rate": 0.0002790822511512352,
      "loss": 0.9564,
      "step": 1030
    },
    {
      "epoch": 0.614748041968376,
      "grad_norm": 0.8212458491325378,
      "learning_rate": 0.00027858699174596215,
      "loss": 0.8803,
      "step": 1040
    },
    {
      "epoch": 0.6206590808334564,
      "grad_norm": 2.1567301750183105,
      "learning_rate": 0.00027808638789762616,
      "loss": 1.2964,
      "step": 1050
    },
    {
      "epoch": 0.626570119698537,
      "grad_norm": 0.815433919429779,
      "learning_rate": 0.0002775804604127549,
      "loss": 1.2126,
      "step": 1060
    },
    {
      "epoch": 0.6324811585636175,
      "grad_norm": 1.2142722606658936,
      "learning_rate": 0.00027706923031914105,
      "loss": 1.1671,
      "step": 1070
    },
    {
      "epoch": 0.6383921974286981,
      "grad_norm": 1.4374685287475586,
      "learning_rate": 0.00027655271886496935,
      "loss": 1.0652,
      "step": 1080
    },
    {
      "epoch": 0.6443032362937786,
      "grad_norm": 2.028388023376465,
      "learning_rate": 0.0002760309475179326,
      "loss": 1.3826,
      "step": 1090
    },
    {
      "epoch": 0.6502142751588592,
      "grad_norm": 1.4759591817855835,
      "learning_rate": 0.00027550393796434,
      "loss": 1.1697,
      "step": 1100
    },
    {
      "epoch": 0.6561253140239397,
      "grad_norm": 1.076900839805603,
      "learning_rate": 0.0002749717121082156,
      "loss": 0.9173,
      "step": 1110
    },
    {
      "epoch": 0.6620363528890203,
      "grad_norm": 1.2914588451385498,
      "learning_rate": 0.0002744342920703878,
      "loss": 0.9848,
      "step": 1120
    },
    {
      "epoch": 0.6679473917541008,
      "grad_norm": 0.7284201979637146,
      "learning_rate": 0.0002738917001875701,
      "loss": 1.1538,
      "step": 1130
    },
    {
      "epoch": 0.6738584306191813,
      "grad_norm": 1.2725015878677368,
      "learning_rate": 0.0002733439590114326,
      "loss": 1.0447,
      "step": 1140
    },
    {
      "epoch": 0.6797694694842619,
      "grad_norm": 0.8510275483131409,
      "learning_rate": 0.00027279109130766486,
      "loss": 1.1817,
      "step": 1150
    },
    {
      "epoch": 0.6856805083493424,
      "grad_norm": 1.7715678215026855,
      "learning_rate": 0.0002722331200550296,
      "loss": 1.122,
      "step": 1160
    },
    {
      "epoch": 0.691591547214423,
      "grad_norm": 1.3184545040130615,
      "learning_rate": 0.00027167006844440743,
      "loss": 1.3256,
      "step": 1170
    },
    {
      "epoch": 0.6975025860795034,
      "grad_norm": 1.1101163625717163,
      "learning_rate": 0.0002711019598778334,
      "loss": 0.8867,
      "step": 1180
    },
    {
      "epoch": 0.703413624944584,
      "grad_norm": 1.0922558307647705,
      "learning_rate": 0.0002705288179675239,
      "loss": 1.2342,
      "step": 1190
    },
    {
      "epoch": 0.7093246638096645,
      "grad_norm": 1.424505591392517,
      "learning_rate": 0.0002699506665348957,
      "loss": 1.0746,
      "step": 1200
    },
    {
      "epoch": 0.7152357026747451,
      "grad_norm": 0.6099621653556824,
      "learning_rate": 0.0002693675296095755,
      "loss": 1.0473,
      "step": 1210
    },
    {
      "epoch": 0.7211467415398256,
      "grad_norm": 1.2074164152145386,
      "learning_rate": 0.0002687794314284013,
      "loss": 1.1465,
      "step": 1220
    },
    {
      "epoch": 0.7270577804049062,
      "grad_norm": 0.6486126780509949,
      "learning_rate": 0.00026818639643441514,
      "loss": 0.9935,
      "step": 1230
    },
    {
      "epoch": 0.7329688192699867,
      "grad_norm": 0.9025202393531799,
      "learning_rate": 0.0002675884492758472,
      "loss": 0.7493,
      "step": 1240
    },
    {
      "epoch": 0.7388798581350673,
      "grad_norm": 1.1157371997833252,
      "learning_rate": 0.000266985614805091,
      "loss": 1.1788,
      "step": 1250
    },
    {
      "epoch": 0.7447908970001478,
      "grad_norm": 1.0885167121887207,
      "learning_rate": 0.00026637791807767117,
      "loss": 1.0209,
      "step": 1260
    },
    {
      "epoch": 0.7507019358652283,
      "grad_norm": 2.4428985118865967,
      "learning_rate": 0.00026576538435120115,
      "loss": 0.8735,
      "step": 1270
    },
    {
      "epoch": 0.7566129747303089,
      "grad_norm": 0.8540585041046143,
      "learning_rate": 0.00026514803908433423,
      "loss": 0.7358,
      "step": 1280
    },
    {
      "epoch": 0.7625240135953893,
      "grad_norm": 0.8274838328361511,
      "learning_rate": 0.00026452590793570486,
      "loss": 1.1572,
      "step": 1290
    },
    {
      "epoch": 0.76843505246047,
      "grad_norm": 0.8122208714485168,
      "learning_rate": 0.00026389901676286264,
      "loss": 1.1574,
      "step": 1300
    },
    {
      "epoch": 0.7743460913255504,
      "grad_norm": 0.8998039364814758,
      "learning_rate": 0.00026326739162119714,
      "loss": 1.3476,
      "step": 1310
    },
    {
      "epoch": 0.780257130190631,
      "grad_norm": 1.0652897357940674,
      "learning_rate": 0.0002626310587628554,
      "loss": 0.7719,
      "step": 1320
    },
    {
      "epoch": 0.7861681690557115,
      "grad_norm": 1.297702431678772,
      "learning_rate": 0.0002619900446356506,
      "loss": 0.9897,
      "step": 1330
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 2.369802951812744,
      "learning_rate": 0.00026134437588196276,
      "loss": 0.8323,
      "step": 1340
    },
    {
      "epoch": 0.7979902467858726,
      "grad_norm": 1.0665628910064697,
      "learning_rate": 0.0002606940793376315,
      "loss": 0.9712,
      "step": 1350
    },
    {
      "epoch": 0.8039012856509532,
      "grad_norm": 0.6863260269165039,
      "learning_rate": 0.00026003918203084075,
      "loss": 0.8202,
      "step": 1360
    },
    {
      "epoch": 0.8098123245160337,
      "grad_norm": 1.0297291278839111,
      "learning_rate": 0.0002593797111809952,
      "loss": 0.8177,
      "step": 1370
    },
    {
      "epoch": 0.8157233633811143,
      "grad_norm": 0.8568499088287354,
      "learning_rate": 0.0002587156941975891,
      "loss": 0.9803,
      "step": 1380
    },
    {
      "epoch": 0.8216344022461948,
      "grad_norm": 1.5378499031066895,
      "learning_rate": 0.00025804715867906687,
      "loss": 1.3927,
      "step": 1390
    },
    {
      "epoch": 0.8275454411112753,
      "grad_norm": 1.4208747148513794,
      "learning_rate": 0.0002573741324116764,
      "loss": 0.9625,
      "step": 1400
    },
    {
      "epoch": 0.8334564799763559,
      "grad_norm": 0.7616665363311768,
      "learning_rate": 0.0002566966433683138,
      "loss": 0.8857,
      "step": 1410
    },
    {
      "epoch": 0.8393675188414363,
      "grad_norm": 0.5649605393409729,
      "learning_rate": 0.00025601471970736103,
      "loss": 1.0454,
      "step": 1420
    },
    {
      "epoch": 0.8452785577065169,
      "grad_norm": 1.8511734008789062,
      "learning_rate": 0.0002553283897715152,
      "loss": 0.9013,
      "step": 1430
    },
    {
      "epoch": 0.8511895965715974,
      "grad_norm": 1.551117181777954,
      "learning_rate": 0.0002546376820866111,
      "loss": 1.0111,
      "step": 1440
    },
    {
      "epoch": 0.857100635436678,
      "grad_norm": 2.121812582015991,
      "learning_rate": 0.00025394262536043504,
      "loss": 1.0325,
      "step": 1450
    },
    {
      "epoch": 0.8630116743017585,
      "grad_norm": 2.537290573120117,
      "learning_rate": 0.0002532432484815321,
      "loss": 0.904,
      "step": 1460
    },
    {
      "epoch": 0.8689227131668391,
      "grad_norm": 1.2138627767562866,
      "learning_rate": 0.0002525395805180052,
      "loss": 0.8901,
      "step": 1470
    },
    {
      "epoch": 0.8748337520319196,
      "grad_norm": 0.9661867022514343,
      "learning_rate": 0.00025183165071630695,
      "loss": 0.8248,
      "step": 1480
    },
    {
      "epoch": 0.8807447908970002,
      "grad_norm": 0.527542769908905,
      "learning_rate": 0.00025111948850002413,
      "loss": 0.9494,
      "step": 1490
    },
    {
      "epoch": 0.8866558297620807,
      "grad_norm": 1.1769733428955078,
      "learning_rate": 0.00025040312346865496,
      "loss": 0.9115,
      "step": 1500
    },
    {
      "epoch": 0.8866558297620807,
      "eval_loss": 0.966824471950531,
      "eval_runtime": 217.3109,
      "eval_samples_per_second": 6.921,
      "eval_steps_per_second": 0.865,
      "step": 1500
    },
    {
      "epoch": 0.8925668686271612,
      "grad_norm": 0.8589854836463928,
      "learning_rate": 0.00024968258539637844,
      "loss": 0.8922,
      "step": 1510
    },
    {
      "epoch": 0.8984779074922418,
      "grad_norm": 0.6945375204086304,
      "learning_rate": 0.0002489579042308172,
      "loss": 0.7376,
      "step": 1520
    },
    {
      "epoch": 0.9043889463573223,
      "grad_norm": 0.36732661724090576,
      "learning_rate": 0.00024822911009179276,
      "loss": 0.8396,
      "step": 1530
    },
    {
      "epoch": 0.9102999852224029,
      "grad_norm": 1.337815761566162,
      "learning_rate": 0.00024749623327007333,
      "loss": 0.5952,
      "step": 1540
    },
    {
      "epoch": 0.9162110240874833,
      "grad_norm": 0.7494143843650818,
      "learning_rate": 0.00024675930422611526,
      "loss": 0.9009,
      "step": 1550
    },
    {
      "epoch": 0.9221220629525639,
      "grad_norm": 2.027194023132324,
      "learning_rate": 0.00024601835358879694,
      "loss": 0.8604,
      "step": 1560
    },
    {
      "epoch": 0.9280331018176444,
      "grad_norm": 0.3332920968532562,
      "learning_rate": 0.00024527341215414546,
      "loss": 0.8194,
      "step": 1570
    },
    {
      "epoch": 0.933944140682725,
      "grad_norm": 3.1836729049682617,
      "learning_rate": 0.00024452451088405693,
      "loss": 1.1178,
      "step": 1580
    },
    {
      "epoch": 0.9398551795478055,
      "grad_norm": 1.0708240270614624,
      "learning_rate": 0.00024377168090500956,
      "loss": 1.0784,
      "step": 1590
    },
    {
      "epoch": 0.9457662184128861,
      "grad_norm": 2.001142740249634,
      "learning_rate": 0.00024301495350677,
      "loss": 1.1588,
      "step": 1600
    },
    {
      "epoch": 0.9516772572779666,
      "grad_norm": 0.5157231092453003,
      "learning_rate": 0.00024225436014109272,
      "loss": 0.9851,
      "step": 1610
    },
    {
      "epoch": 0.9575882961430472,
      "grad_norm": 1.1543854475021362,
      "learning_rate": 0.0002414899324204129,
      "loss": 1.0326,
      "step": 1620
    },
    {
      "epoch": 0.9634993350081277,
      "grad_norm": 1.1339360475540161,
      "learning_rate": 0.00024072170211653237,
      "loss": 1.0898,
      "step": 1630
    },
    {
      "epoch": 0.9694103738732082,
      "grad_norm": 1.0551549196243286,
      "learning_rate": 0.00023994970115929928,
      "loss": 1.0719,
      "step": 1640
    },
    {
      "epoch": 0.9753214127382888,
      "grad_norm": 0.7559623718261719,
      "learning_rate": 0.00023917396163528092,
      "loss": 0.5571,
      "step": 1650
    },
    {
      "epoch": 0.9812324516033692,
      "grad_norm": 1.2452393770217896,
      "learning_rate": 0.00023839451578643004,
      "loss": 1.0676,
      "step": 1660
    },
    {
      "epoch": 0.9871434904684498,
      "grad_norm": 0.6703391075134277,
      "learning_rate": 0.00023761139600874496,
      "loss": 0.6167,
      "step": 1670
    },
    {
      "epoch": 0.9930545293335303,
      "grad_norm": 2.0774176120758057,
      "learning_rate": 0.00023682463485092287,
      "loss": 1.1952,
      "step": 1680
    },
    {
      "epoch": 0.9989655681986109,
      "grad_norm": 0.8266769647598267,
      "learning_rate": 0.00023603426501300718,
      "loss": 0.7562,
      "step": 1690
    },
    {
      "epoch": 1.0053199349785724,
      "grad_norm": 1.2659260034561157,
      "learning_rate": 0.0002352403193450284,
      "loss": 0.8698,
      "step": 1700
    },
    {
      "epoch": 1.0112309738436531,
      "grad_norm": 0.7206515073776245,
      "learning_rate": 0.00023444283084563867,
      "loss": 0.6852,
      "step": 1710
    },
    {
      "epoch": 1.0171420127087336,
      "grad_norm": 2.1051628589630127,
      "learning_rate": 0.00023364183266074047,
      "loss": 1.0288,
      "step": 1720
    },
    {
      "epoch": 1.023053051573814,
      "grad_norm": 1.2459101676940918,
      "learning_rate": 0.00023283735808210865,
      "loss": 0.7756,
      "step": 1730
    },
    {
      "epoch": 1.0289640904388946,
      "grad_norm": 0.8717842102050781,
      "learning_rate": 0.00023202944054600714,
      "loss": 0.8669,
      "step": 1740
    },
    {
      "epoch": 1.034875129303975,
      "grad_norm": 1.0875258445739746,
      "learning_rate": 0.0002312181136317989,
      "loss": 1.2068,
      "step": 1750
    },
    {
      "epoch": 1.0407861681690558,
      "grad_norm": 1.4109116792678833,
      "learning_rate": 0.00023040341106055033,
      "loss": 0.9505,
      "step": 1760
    },
    {
      "epoch": 1.0466972070341363,
      "grad_norm": 1.0999497175216675,
      "learning_rate": 0.00022958536669363003,
      "loss": 0.9594,
      "step": 1770
    },
    {
      "epoch": 1.0526082458992168,
      "grad_norm": 0.5027688145637512,
      "learning_rate": 0.0002287640145313009,
      "loss": 0.8496,
      "step": 1780
    },
    {
      "epoch": 1.0585192847642972,
      "grad_norm": 1.0464584827423096,
      "learning_rate": 0.00022793938871130753,
      "loss": 0.8856,
      "step": 1790
    },
    {
      "epoch": 1.064430323629378,
      "grad_norm": 1.0491294860839844,
      "learning_rate": 0.00022711152350745684,
      "loss": 0.9196,
      "step": 1800
    },
    {
      "epoch": 1.0703413624944584,
      "grad_norm": 1.4380377531051636,
      "learning_rate": 0.000226280453328194,
      "loss": 1.2361,
      "step": 1810
    },
    {
      "epoch": 1.076252401359539,
      "grad_norm": 1.8503388166427612,
      "learning_rate": 0.00022544621271517202,
      "loss": 1.3809,
      "step": 1820
    },
    {
      "epoch": 1.0821634402246194,
      "grad_norm": 0.9865618348121643,
      "learning_rate": 0.00022460883634181628,
      "loss": 0.9286,
      "step": 1830
    },
    {
      "epoch": 1.0880744790897001,
      "grad_norm": 1.408588171005249,
      "learning_rate": 0.00022376835901188319,
      "loss": 0.8711,
      "step": 1840
    },
    {
      "epoch": 1.0939855179547806,
      "grad_norm": 1.0152348279953003,
      "learning_rate": 0.00022292481565801397,
      "loss": 0.7668,
      "step": 1850
    },
    {
      "epoch": 1.099896556819861,
      "grad_norm": 2.4467756748199463,
      "learning_rate": 0.00022207824134028238,
      "loss": 1.0573,
      "step": 1860
    },
    {
      "epoch": 1.1058075956849416,
      "grad_norm": 1.2333484888076782,
      "learning_rate": 0.00022122867124473784,
      "loss": 1.0001,
      "step": 1870
    },
    {
      "epoch": 1.111718634550022,
      "grad_norm": 0.7063289284706116,
      "learning_rate": 0.00022037614068194276,
      "loss": 1.1188,
      "step": 1880
    },
    {
      "epoch": 1.1176296734151028,
      "grad_norm": 2.782322645187378,
      "learning_rate": 0.00021952068508550516,
      "loss": 0.8756,
      "step": 1890
    },
    {
      "epoch": 1.1235407122801833,
      "grad_norm": 0.707579493522644,
      "learning_rate": 0.00021866234001060566,
      "loss": 0.9134,
      "step": 1900
    },
    {
      "epoch": 1.1294517511452637,
      "grad_norm": 0.6689584255218506,
      "learning_rate": 0.00021780114113252003,
      "loss": 0.6925,
      "step": 1910
    },
    {
      "epoch": 1.1353627900103442,
      "grad_norm": 1.1791493892669678,
      "learning_rate": 0.00021693712424513607,
      "loss": 0.7996,
      "step": 1920
    },
    {
      "epoch": 1.141273828875425,
      "grad_norm": 0.6566810607910156,
      "learning_rate": 0.00021607032525946637,
      "loss": 0.8078,
      "step": 1930
    },
    {
      "epoch": 1.1471848677405054,
      "grad_norm": 2.270536184310913,
      "learning_rate": 0.0002152007802021552,
      "loss": 1.1544,
      "step": 1940
    },
    {
      "epoch": 1.153095906605586,
      "grad_norm": 0.9766626358032227,
      "learning_rate": 0.0002143285252139816,
      "loss": 1.2543,
      "step": 1950
    },
    {
      "epoch": 1.1590069454706664,
      "grad_norm": 1.4445466995239258,
      "learning_rate": 0.00021345359654835705,
      "loss": 0.8612,
      "step": 1960
    },
    {
      "epoch": 1.1649179843357471,
      "grad_norm": 0.4692048728466034,
      "learning_rate": 0.00021257603056981867,
      "loss": 0.9292,
      "step": 1970
    },
    {
      "epoch": 1.1708290232008276,
      "grad_norm": 1.3267546892166138,
      "learning_rate": 0.00021169586375251786,
      "loss": 0.9749,
      "step": 1980
    },
    {
      "epoch": 1.176740062065908,
      "grad_norm": 2.033278226852417,
      "learning_rate": 0.00021081313267870436,
      "loss": 0.7921,
      "step": 1990
    },
    {
      "epoch": 1.1826511009309886,
      "grad_norm": 1.5818259716033936,
      "learning_rate": 0.00020992787403720565,
      "loss": 0.8481,
      "step": 2000
    },
    {
      "epoch": 1.1826511009309886,
      "eval_loss": 0.8974140882492065,
      "eval_runtime": 217.5682,
      "eval_samples_per_second": 6.913,
      "eval_steps_per_second": 0.864,
      "step": 2000
    },
    {
      "epoch": 1.188562139796069,
      "grad_norm": 1.1696337461471558,
      "learning_rate": 0.00020904012462190223,
      "loss": 0.6901,
      "step": 2010
    },
    {
      "epoch": 1.1944731786611498,
      "grad_norm": 1.1192651987075806,
      "learning_rate": 0.00020814992133019833,
      "loss": 0.7199,
      "step": 2020
    },
    {
      "epoch": 1.2003842175262303,
      "grad_norm": 1.3145755529403687,
      "learning_rate": 0.00020725730116148813,
      "loss": 0.9976,
      "step": 2030
    },
    {
      "epoch": 1.2062952563913107,
      "grad_norm": 1.2743269205093384,
      "learning_rate": 0.00020636230121561826,
      "loss": 1.0017,
      "step": 2040
    },
    {
      "epoch": 1.2122062952563912,
      "grad_norm": 1.6741915941238403,
      "learning_rate": 0.00020546495869134567,
      "loss": 0.6079,
      "step": 2050
    },
    {
      "epoch": 1.218117334121472,
      "grad_norm": 1.3378978967666626,
      "learning_rate": 0.0002045653108847916,
      "loss": 1.4589,
      "step": 2060
    },
    {
      "epoch": 1.2240283729865524,
      "grad_norm": 0.26088035106658936,
      "learning_rate": 0.00020366339518789129,
      "loss": 0.7271,
      "step": 2070
    },
    {
      "epoch": 1.229939411851633,
      "grad_norm": 1.3203980922698975,
      "learning_rate": 0.00020275924908684016,
      "loss": 0.8797,
      "step": 2080
    },
    {
      "epoch": 1.2358504507167134,
      "grad_norm": 0.6673275828361511,
      "learning_rate": 0.00020185291016053543,
      "loss": 1.0189,
      "step": 2090
    },
    {
      "epoch": 1.241761489581794,
      "grad_norm": 0.8586916327476501,
      "learning_rate": 0.00020094441607901458,
      "loss": 0.7789,
      "step": 2100
    },
    {
      "epoch": 1.2476725284468746,
      "grad_norm": 2.029791831970215,
      "learning_rate": 0.0002000338046018893,
      "loss": 0.7946,
      "step": 2110
    },
    {
      "epoch": 1.253583567311955,
      "grad_norm": 0.7835213541984558,
      "learning_rate": 0.00019912111357677653,
      "loss": 0.8786,
      "step": 2120
    },
    {
      "epoch": 1.2594946061770356,
      "grad_norm": 0.6661372184753418,
      "learning_rate": 0.00019820638093772495,
      "loss": 0.8794,
      "step": 2130
    },
    {
      "epoch": 1.265405645042116,
      "grad_norm": 1.2334216833114624,
      "learning_rate": 0.00019728964470363874,
      "loss": 0.7943,
      "step": 2140
    },
    {
      "epoch": 1.2713166839071968,
      "grad_norm": 0.8281213045120239,
      "learning_rate": 0.00019637094297669707,
      "loss": 0.5902,
      "step": 2150
    },
    {
      "epoch": 1.2772277227722773,
      "grad_norm": 1.3270446062088013,
      "learning_rate": 0.00019545031394077074,
      "loss": 0.7131,
      "step": 2160
    },
    {
      "epoch": 1.2831387616373577,
      "grad_norm": 0.46519675850868225,
      "learning_rate": 0.00019452779585983488,
      "loss": 0.8927,
      "step": 2170
    },
    {
      "epoch": 1.2890498005024382,
      "grad_norm": 1.7822357416152954,
      "learning_rate": 0.00019360342707637894,
      "loss": 0.8443,
      "step": 2180
    },
    {
      "epoch": 1.2949608393675187,
      "grad_norm": 0.5788727402687073,
      "learning_rate": 0.00019267724600981255,
      "loss": 0.7671,
      "step": 2190
    },
    {
      "epoch": 1.3008718782325994,
      "grad_norm": 0.3792121410369873,
      "learning_rate": 0.00019174929115486935,
      "loss": 0.6951,
      "step": 2200
    },
    {
      "epoch": 1.30678291709768,
      "grad_norm": 2.5573198795318604,
      "learning_rate": 0.00019081960108000644,
      "loss": 0.8488,
      "step": 2210
    },
    {
      "epoch": 1.3126939559627604,
      "grad_norm": 2.5192341804504395,
      "learning_rate": 0.00018988821442580178,
      "loss": 1.0607,
      "step": 2220
    },
    {
      "epoch": 1.318604994827841,
      "grad_norm": 1.1603440046310425,
      "learning_rate": 0.0001889551699033479,
      "loss": 0.9304,
      "step": 2230
    },
    {
      "epoch": 1.3245160336929216,
      "grad_norm": 0.9393249750137329,
      "learning_rate": 0.00018802050629264318,
      "loss": 1.0186,
      "step": 2240
    },
    {
      "epoch": 1.330427072558002,
      "grad_norm": 0.9996641278266907,
      "learning_rate": 0.00018708426244097985,
      "loss": 0.7664,
      "step": 2250
    },
    {
      "epoch": 1.3363381114230826,
      "grad_norm": 0.45620909333229065,
      "learning_rate": 0.00018614647726132954,
      "loss": 0.5333,
      "step": 2260
    },
    {
      "epoch": 1.342249150288163,
      "grad_norm": 2.4257633686065674,
      "learning_rate": 0.00018520718973072582,
      "loss": 0.7354,
      "step": 2270
    },
    {
      "epoch": 1.3481601891532438,
      "grad_norm": 0.6289002299308777,
      "learning_rate": 0.00018426643888864435,
      "loss": 1.2914,
      "step": 2280
    },
    {
      "epoch": 1.3540712280183242,
      "grad_norm": 0.8691869378089905,
      "learning_rate": 0.00018332426383538,
      "loss": 0.7397,
      "step": 2290
    },
    {
      "epoch": 1.3599822668834047,
      "grad_norm": 1.9503474235534668,
      "learning_rate": 0.00018238070373042224,
      "loss": 0.6551,
      "step": 2300
    },
    {
      "epoch": 1.3658933057484852,
      "grad_norm": 0.5521931052207947,
      "learning_rate": 0.000181435797790827,
      "loss": 0.8639,
      "step": 2310
    },
    {
      "epoch": 1.3718043446135657,
      "grad_norm": 0.77325439453125,
      "learning_rate": 0.00018048958528958705,
      "loss": 0.819,
      "step": 2320
    },
    {
      "epoch": 1.3777153834786464,
      "grad_norm": 1.0857123136520386,
      "learning_rate": 0.00017954210555399952,
      "loss": 0.7828,
      "step": 2330
    },
    {
      "epoch": 1.383626422343727,
      "grad_norm": 0.6124300956726074,
      "learning_rate": 0.00017859339796403152,
      "loss": 0.9829,
      "step": 2340
    },
    {
      "epoch": 1.3895374612088074,
      "grad_norm": 0.7765011787414551,
      "learning_rate": 0.0001776435019506832,
      "loss": 0.8812,
      "step": 2350
    },
    {
      "epoch": 1.395448500073888,
      "grad_norm": 1.100540280342102,
      "learning_rate": 0.00017669245699434915,
      "loss": 0.7633,
      "step": 2360
    },
    {
      "epoch": 1.4013595389389686,
      "grad_norm": 0.35430842638015747,
      "learning_rate": 0.0001757403026231771,
      "loss": 0.69,
      "step": 2370
    },
    {
      "epoch": 1.407270577804049,
      "grad_norm": 1.6914312839508057,
      "learning_rate": 0.0001747870784114254,
      "loss": 0.8889,
      "step": 2380
    },
    {
      "epoch": 1.4131816166691296,
      "grad_norm": 1.7468451261520386,
      "learning_rate": 0.00017383282397781793,
      "loss": 1.1388,
      "step": 2390
    },
    {
      "epoch": 1.41909265553421,
      "grad_norm": 1.1198307275772095,
      "learning_rate": 0.00017287757898389755,
      "loss": 0.9663,
      "step": 2400
    },
    {
      "epoch": 1.4250036943992908,
      "grad_norm": 1.2962219715118408,
      "learning_rate": 0.00017192138313237772,
      "loss": 0.929,
      "step": 2410
    },
    {
      "epoch": 1.4309147332643712,
      "grad_norm": 1.720551609992981,
      "learning_rate": 0.0001709642761654922,
      "loss": 0.9215,
      "step": 2420
    },
    {
      "epoch": 1.4368257721294517,
      "grad_norm": 1.288575530052185,
      "learning_rate": 0.00017000629786334334,
      "loss": 0.9216,
      "step": 2430
    },
    {
      "epoch": 1.4427368109945322,
      "grad_norm": 1.1617252826690674,
      "learning_rate": 0.0001690474880422486,
      "loss": 0.8377,
      "step": 2440
    },
    {
      "epoch": 1.4486478498596127,
      "grad_norm": 0.4232481122016907,
      "learning_rate": 0.00016808788655308578,
      "loss": 0.9058,
      "step": 2450
    },
    {
      "epoch": 1.4545588887246934,
      "grad_norm": 1.302380084991455,
      "learning_rate": 0.00016712753327963677,
      "loss": 0.9815,
      "step": 2460
    },
    {
      "epoch": 1.460469927589774,
      "grad_norm": 1.0714366436004639,
      "learning_rate": 0.00016616646813692962,
      "loss": 0.5586,
      "step": 2470
    },
    {
      "epoch": 1.4663809664548544,
      "grad_norm": 2.0691139698028564,
      "learning_rate": 0.00016520473106957978,
      "loss": 1.2145,
      "step": 2480
    },
    {
      "epoch": 1.472292005319935,
      "grad_norm": 0.8605973720550537,
      "learning_rate": 0.0001642423620501298,
      "loss": 0.6989,
      "step": 2490
    },
    {
      "epoch": 1.4782030441850156,
      "grad_norm": 0.9338710904121399,
      "learning_rate": 0.00016327940107738792,
      "loss": 0.7046,
      "step": 2500
    },
    {
      "epoch": 1.4782030441850156,
      "eval_loss": 0.8510778546333313,
      "eval_runtime": 217.5014,
      "eval_samples_per_second": 6.915,
      "eval_steps_per_second": 0.864,
      "step": 2500
    },
    {
      "epoch": 1.484114083050096,
      "grad_norm": 0.6322833299636841,
      "learning_rate": 0.0001623158881747657,
      "loss": 1.0393,
      "step": 2510
    },
    {
      "epoch": 1.4900251219151766,
      "grad_norm": 0.774755597114563,
      "learning_rate": 0.00016135186338861452,
      "loss": 0.7193,
      "step": 2520
    },
    {
      "epoch": 1.495936160780257,
      "grad_norm": 1.1852223873138428,
      "learning_rate": 0.000160387366786561,
      "loss": 0.8502,
      "step": 2530
    },
    {
      "epoch": 1.5018471996453377,
      "grad_norm": 1.6821094751358032,
      "learning_rate": 0.00015942243845584186,
      "loss": 1.0392,
      "step": 2540
    },
    {
      "epoch": 1.5077582385104182,
      "grad_norm": 1.7408852577209473,
      "learning_rate": 0.00015845711850163768,
      "loss": 1.0879,
      "step": 2550
    },
    {
      "epoch": 1.5136692773754987,
      "grad_norm": 1.2541487216949463,
      "learning_rate": 0.00015749144704540596,
      "loss": 0.5786,
      "step": 2560
    },
    {
      "epoch": 1.5195803162405794,
      "grad_norm": 2.714712381362915,
      "learning_rate": 0.0001565254642232138,
      "loss": 0.9306,
      "step": 2570
    },
    {
      "epoch": 1.5254913551056597,
      "grad_norm": 2.09464693069458,
      "learning_rate": 0.0001555592101840694,
      "loss": 1.1174,
      "step": 2580
    },
    {
      "epoch": 1.5314023939707404,
      "grad_norm": 1.0840226411819458,
      "learning_rate": 0.00015459272508825357,
      "loss": 0.7217,
      "step": 2590
    },
    {
      "epoch": 1.537313432835821,
      "grad_norm": 1.141671895980835,
      "learning_rate": 0.00015362604910565045,
      "loss": 1.0643,
      "step": 2600
    },
    {
      "epoch": 1.5432244717009014,
      "grad_norm": 0.6159616708755493,
      "learning_rate": 0.00015265922241407807,
      "loss": 0.7584,
      "step": 2610
    },
    {
      "epoch": 1.549135510565982,
      "grad_norm": 0.7701154351234436,
      "learning_rate": 0.00015169228519761827,
      "loss": 0.6287,
      "step": 2620
    },
    {
      "epoch": 1.5550465494310624,
      "grad_norm": 0.6685971617698669,
      "learning_rate": 0.0001507252776449467,
      "loss": 0.8382,
      "step": 2630
    },
    {
      "epoch": 1.560957588296143,
      "grad_norm": 0.4607510268688202,
      "learning_rate": 0.0001497582399476623,
      "loss": 0.7668,
      "step": 2640
    },
    {
      "epoch": 1.5668686271612235,
      "grad_norm": 0.687637448310852,
      "learning_rate": 0.000148791212298617,
      "loss": 0.8708,
      "step": 2650
    },
    {
      "epoch": 1.572779666026304,
      "grad_norm": 0.8656360507011414,
      "learning_rate": 0.00014782423489024502,
      "loss": 0.9299,
      "step": 2660
    },
    {
      "epoch": 1.5786907048913847,
      "grad_norm": 2.1467208862304688,
      "learning_rate": 0.00014685734791289247,
      "loss": 0.7076,
      "step": 2670
    },
    {
      "epoch": 1.5846017437564652,
      "grad_norm": 0.7212432026863098,
      "learning_rate": 0.00014589059155314685,
      "loss": 0.5995,
      "step": 2680
    },
    {
      "epoch": 1.5905127826215457,
      "grad_norm": 1.1517289876937866,
      "learning_rate": 0.00014492400599216686,
      "loss": 1.006,
      "step": 2690
    },
    {
      "epoch": 1.5964238214866264,
      "grad_norm": 2.463197708129883,
      "learning_rate": 0.00014395763140401228,
      "loss": 0.7759,
      "step": 2700
    },
    {
      "epoch": 1.6023348603517067,
      "grad_norm": 0.45252174139022827,
      "learning_rate": 0.0001429915079539743,
      "loss": 0.6464,
      "step": 2710
    },
    {
      "epoch": 1.6082458992167874,
      "grad_norm": 0.2781231999397278,
      "learning_rate": 0.00014202567579690606,
      "loss": 0.8268,
      "step": 2720
    },
    {
      "epoch": 1.6141569380818679,
      "grad_norm": 0.5327668190002441,
      "learning_rate": 0.00014106017507555375,
      "loss": 0.8026,
      "step": 2730
    },
    {
      "epoch": 1.6200679769469484,
      "grad_norm": 1.871917486190796,
      "learning_rate": 0.00014009504591888815,
      "loss": 0.617,
      "step": 2740
    },
    {
      "epoch": 1.625979015812029,
      "grad_norm": 0.8970045447349548,
      "learning_rate": 0.00013913032844043664,
      "loss": 0.7872,
      "step": 2750
    },
    {
      "epoch": 1.6318900546771093,
      "grad_norm": 0.718917965888977,
      "learning_rate": 0.00013816606273661625,
      "loss": 0.9034,
      "step": 2760
    },
    {
      "epoch": 1.63780109354219,
      "grad_norm": 0.7239982485771179,
      "learning_rate": 0.00013720228888506694,
      "loss": 0.9084,
      "step": 2770
    },
    {
      "epoch": 1.6437121324072705,
      "grad_norm": 0.9206106066703796,
      "learning_rate": 0.0001362390469429857,
      "loss": 0.8409,
      "step": 2780
    },
    {
      "epoch": 1.649623171272351,
      "grad_norm": 2.0410103797912598,
      "learning_rate": 0.00013527637694546205,
      "loss": 1.0262,
      "step": 2790
    },
    {
      "epoch": 1.6555342101374317,
      "grad_norm": 0.39525625109672546,
      "learning_rate": 0.00013431431890381377,
      "loss": 0.6851,
      "step": 2800
    },
    {
      "epoch": 1.6614452490025122,
      "grad_norm": 1.993753433227539,
      "learning_rate": 0.00013335291280392397,
      "loss": 0.7088,
      "step": 2810
    },
    {
      "epoch": 1.6673562878675927,
      "grad_norm": 0.8663133978843689,
      "learning_rate": 0.0001323921986045793,
      "loss": 0.9115,
      "step": 2820
    },
    {
      "epoch": 1.6732673267326734,
      "grad_norm": 0.9623103141784668,
      "learning_rate": 0.0001314322162358089,
      "loss": 0.7718,
      "step": 2830
    },
    {
      "epoch": 1.6791783655977537,
      "grad_norm": 1.1349341869354248,
      "learning_rate": 0.00013047300559722509,
      "loss": 0.8825,
      "step": 2840
    },
    {
      "epoch": 1.6850894044628344,
      "grad_norm": 0.7343268394470215,
      "learning_rate": 0.00012951460655636474,
      "loss": 0.711,
      "step": 2850
    },
    {
      "epoch": 1.6910004433279149,
      "grad_norm": 1.0986992120742798,
      "learning_rate": 0.0001285570589470325,
      "loss": 0.7903,
      "step": 2860
    },
    {
      "epoch": 1.6969114821929954,
      "grad_norm": 1.015511393547058,
      "learning_rate": 0.00012760040256764508,
      "loss": 0.9669,
      "step": 2870
    },
    {
      "epoch": 1.702822521058076,
      "grad_norm": 1.141070008277893,
      "learning_rate": 0.00012664467717957715,
      "loss": 1.0899,
      "step": 2880
    },
    {
      "epoch": 1.7087335599231563,
      "grad_norm": 0.8409292101860046,
      "learning_rate": 0.00012568992250550854,
      "loss": 0.833,
      "step": 2890
    },
    {
      "epoch": 1.714644598788237,
      "grad_norm": 0.6757439970970154,
      "learning_rate": 0.00012473617822777368,
      "loss": 0.5487,
      "step": 2900
    },
    {
      "epoch": 1.7205556376533175,
      "grad_norm": 0.635202169418335,
      "learning_rate": 0.0001237834839867118,
      "loss": 0.6931,
      "step": 2910
    },
    {
      "epoch": 1.726466676518398,
      "grad_norm": 0.9030903577804565,
      "learning_rate": 0.00012283187937901986,
      "loss": 0.6621,
      "step": 2920
    },
    {
      "epoch": 1.7323777153834787,
      "grad_norm": 1.6328026056289673,
      "learning_rate": 0.0001218814039561063,
      "loss": 1.0221,
      "step": 2930
    },
    {
      "epoch": 1.7382887542485592,
      "grad_norm": 1.6400623321533203,
      "learning_rate": 0.00012093209722244757,
      "loss": 0.8625,
      "step": 2940
    },
    {
      "epoch": 1.7441997931136397,
      "grad_norm": 0.993628740310669,
      "learning_rate": 0.00011998399863394596,
      "loss": 0.8102,
      "step": 2950
    },
    {
      "epoch": 1.7501108319787204,
      "grad_norm": 1.7681617736816406,
      "learning_rate": 0.00011903714759628982,
      "loss": 0.5145,
      "step": 2960
    },
    {
      "epoch": 1.7560218708438007,
      "grad_norm": 2.1477651596069336,
      "learning_rate": 0.0001180915834633158,
      "loss": 0.5937,
      "step": 2970
    },
    {
      "epoch": 1.7619329097088814,
      "grad_norm": 0.7229859828948975,
      "learning_rate": 0.00011714734553537304,
      "loss": 0.8139,
      "step": 2980
    },
    {
      "epoch": 1.7678439485739619,
      "grad_norm": 0.4363662600517273,
      "learning_rate": 0.00011620447305768973,
      "loss": 0.8964,
      "step": 2990
    },
    {
      "epoch": 1.7737549874390424,
      "grad_norm": 1.1696609258651733,
      "learning_rate": 0.00011526300521874222,
      "loss": 0.9399,
      "step": 3000
    },
    {
      "epoch": 1.7737549874390424,
      "eval_loss": 0.8171266317367554,
      "eval_runtime": 217.3911,
      "eval_samples_per_second": 6.918,
      "eval_steps_per_second": 0.865,
      "step": 3000
    },
    {
      "epoch": 1.779666026304123,
      "grad_norm": 1.0508556365966797,
      "learning_rate": 0.00011432298114862588,
      "loss": 1.0285,
      "step": 3010
    },
    {
      "epoch": 1.7855770651692033,
      "grad_norm": 0.9585235714912415,
      "learning_rate": 0.00011338443991742907,
      "loss": 0.6292,
      "step": 3020
    },
    {
      "epoch": 1.791488104034284,
      "grad_norm": 0.6775657534599304,
      "learning_rate": 0.00011244742053360896,
      "loss": 1.0955,
      "step": 3030
    },
    {
      "epoch": 1.7973991428993645,
      "grad_norm": 3.4950592517852783,
      "learning_rate": 0.00011151196194237056,
      "loss": 0.6661,
      "step": 3040
    },
    {
      "epoch": 1.803310181764445,
      "grad_norm": 0.4340488910675049,
      "learning_rate": 0.00011057810302404781,
      "loss": 0.7905,
      "step": 3050
    },
    {
      "epoch": 1.8092212206295257,
      "grad_norm": 0.6585454940795898,
      "learning_rate": 0.0001096458825924876,
      "loss": 0.5133,
      "step": 3060
    },
    {
      "epoch": 1.8151322594946062,
      "grad_norm": 0.636106014251709,
      "learning_rate": 0.0001087153393934367,
      "loss": 0.5344,
      "step": 3070
    },
    {
      "epoch": 1.8210432983596867,
      "grad_norm": 1.1860429048538208,
      "learning_rate": 0.00010778651210293136,
      "loss": 0.574,
      "step": 3080
    },
    {
      "epoch": 1.8269543372247674,
      "grad_norm": 1.218689203262329,
      "learning_rate": 0.00010685943932568961,
      "loss": 0.702,
      "step": 3090
    },
    {
      "epoch": 1.8328653760898477,
      "grad_norm": 0.5169245004653931,
      "learning_rate": 0.00010593415959350705,
      "loss": 0.6415,
      "step": 3100
    },
    {
      "epoch": 1.8387764149549284,
      "grad_norm": 0.9458039402961731,
      "learning_rate": 0.00010501071136365505,
      "loss": 0.6304,
      "step": 3110
    },
    {
      "epoch": 1.8446874538200089,
      "grad_norm": 1.14734947681427,
      "learning_rate": 0.0001040891330172826,
      "loss": 1.104,
      "step": 3120
    },
    {
      "epoch": 1.8505984926850894,
      "grad_norm": 1.360620141029358,
      "learning_rate": 0.00010316946285782101,
      "loss": 0.8799,
      "step": 3130
    },
    {
      "epoch": 1.85650953155017,
      "grad_norm": 0.706331729888916,
      "learning_rate": 0.00010225173910939178,
      "loss": 0.7152,
      "step": 3140
    },
    {
      "epoch": 1.8624205704152503,
      "grad_norm": 0.6349744200706482,
      "learning_rate": 0.00010133599991521815,
      "loss": 0.8997,
      "step": 3150
    },
    {
      "epoch": 1.868331609280331,
      "grad_norm": 0.7707855701446533,
      "learning_rate": 0.0001004222833360394,
      "loss": 1.2446,
      "step": 3160
    },
    {
      "epoch": 1.8742426481454115,
      "grad_norm": 1.0410871505737305,
      "learning_rate": 9.95106273485294e-05,
      "loss": 0.995,
      "step": 3170
    },
    {
      "epoch": 1.880153687010492,
      "grad_norm": 0.4975525438785553,
      "learning_rate": 9.860106984371767e-05,
      "loss": 0.6962,
      "step": 3180
    },
    {
      "epoch": 1.8860647258755727,
      "grad_norm": 1.0908316373825073,
      "learning_rate": 9.769364862541506e-05,
      "loss": 0.8063,
      "step": 3190
    },
    {
      "epoch": 1.8919757647406532,
      "grad_norm": 1.0335760116577148,
      "learning_rate": 9.678840140864204e-05,
      "loss": 0.9933,
      "step": 3200
    },
    {
      "epoch": 1.8978868036057337,
      "grad_norm": 1.4128656387329102,
      "learning_rate": 9.588536581806148e-05,
      "loss": 0.9878,
      "step": 3210
    },
    {
      "epoch": 1.9037978424708144,
      "grad_norm": 0.8027359247207642,
      "learning_rate": 9.498457938641464e-05,
      "loss": 0.627,
      "step": 3220
    },
    {
      "epoch": 1.9097088813358947,
      "grad_norm": 0.5811389684677124,
      "learning_rate": 9.408607955296143e-05,
      "loss": 0.6544,
      "step": 3230
    },
    {
      "epoch": 1.9156199202009754,
      "grad_norm": 1.5861387252807617,
      "learning_rate": 9.318990366192404e-05,
      "loss": 0.9301,
      "step": 3240
    },
    {
      "epoch": 1.9215309590660559,
      "grad_norm": 1.320826530456543,
      "learning_rate": 9.229608896093513e-05,
      "loss": 0.5712,
      "step": 3250
    },
    {
      "epoch": 1.9274419979311364,
      "grad_norm": 0.7956535816192627,
      "learning_rate": 9.140467259948942e-05,
      "loss": 0.9261,
      "step": 3260
    },
    {
      "epoch": 1.933353036796217,
      "grad_norm": 1.5749399662017822,
      "learning_rate": 9.051569162739987e-05,
      "loss": 0.7307,
      "step": 3270
    },
    {
      "epoch": 1.9392640756612973,
      "grad_norm": 1.49986732006073,
      "learning_rate": 8.962918299325764e-05,
      "loss": 0.8151,
      "step": 3280
    },
    {
      "epoch": 1.945175114526378,
      "grad_norm": 1.005713939666748,
      "learning_rate": 8.874518354289644e-05,
      "loss": 0.839,
      "step": 3290
    },
    {
      "epoch": 1.9510861533914585,
      "grad_norm": 1.1819324493408203,
      "learning_rate": 8.786373001786126e-05,
      "loss": 0.8009,
      "step": 3300
    },
    {
      "epoch": 1.956997192256539,
      "grad_norm": 0.8903324007987976,
      "learning_rate": 8.6984859053881e-05,
      "loss": 0.7293,
      "step": 3310
    },
    {
      "epoch": 1.9629082311216197,
      "grad_norm": 0.7952669262886047,
      "learning_rate": 8.610860717934605e-05,
      "loss": 0.7742,
      "step": 3320
    },
    {
      "epoch": 1.9688192699867002,
      "grad_norm": 2.3274848461151123,
      "learning_rate": 8.523501081378982e-05,
      "loss": 1.1077,
      "step": 3330
    },
    {
      "epoch": 1.9747303088517807,
      "grad_norm": 1.7487236261367798,
      "learning_rate": 8.436410626637529e-05,
      "loss": 0.655,
      "step": 3340
    },
    {
      "epoch": 1.9806413477168614,
      "grad_norm": 0.6048775315284729,
      "learning_rate": 8.349592973438572e-05,
      "loss": 0.6855,
      "step": 3350
    },
    {
      "epoch": 1.9865523865819417,
      "grad_norm": 0.9215231537818909,
      "learning_rate": 8.263051730172034e-05,
      "loss": 0.5115,
      "step": 3360
    },
    {
      "epoch": 1.9924634254470224,
      "grad_norm": 0.6528447270393372,
      "learning_rate": 8.176790493739437e-05,
      "loss": 0.5624,
      "step": 3370
    },
    {
      "epoch": 1.9983744643121029,
      "grad_norm": 0.6040571331977844,
      "learning_rate": 8.090812849404423e-05,
      "loss": 0.8423,
      "step": 3380
    },
    {
      "epoch": 2.0047288310920646,
      "grad_norm": 1.819597840309143,
      "learning_rate": 8.005122370643739e-05,
      "loss": 0.7226,
      "step": 3390
    },
    {
      "epoch": 2.010639869957145,
      "grad_norm": 1.912144660949707,
      "learning_rate": 7.919722618998724e-05,
      "loss": 0.8214,
      "step": 3400
    },
    {
      "epoch": 2.0165509088222255,
      "grad_norm": 0.48661375045776367,
      "learning_rate": 7.834617143927231e-05,
      "loss": 0.795,
      "step": 3410
    },
    {
      "epoch": 2.0224619476873062,
      "grad_norm": 0.8176171183586121,
      "learning_rate": 7.749809482656165e-05,
      "loss": 0.6451,
      "step": 3420
    },
    {
      "epoch": 2.0283729865523865,
      "grad_norm": 0.5116066336631775,
      "learning_rate": 7.665303160034432e-05,
      "loss": 0.5398,
      "step": 3430
    },
    {
      "epoch": 2.0342840254174672,
      "grad_norm": 0.38537970185279846,
      "learning_rate": 7.581101688386445e-05,
      "loss": 0.6803,
      "step": 3440
    },
    {
      "epoch": 2.0401950642825475,
      "grad_norm": 1.02964186668396,
      "learning_rate": 7.497208567366121e-05,
      "loss": 0.7538,
      "step": 3450
    },
    {
      "epoch": 2.046106103147628,
      "grad_norm": 1.1059420108795166,
      "learning_rate": 7.413627283811463e-05,
      "loss": 0.589,
      "step": 3460
    },
    {
      "epoch": 2.052017142012709,
      "grad_norm": 1.1188265085220337,
      "learning_rate": 7.330361311599611e-05,
      "loss": 0.9316,
      "step": 3470
    },
    {
      "epoch": 2.057928180877789,
      "grad_norm": 2.0850119590759277,
      "learning_rate": 7.247414111502461e-05,
      "loss": 0.9784,
      "step": 3480
    },
    {
      "epoch": 2.06383921974287,
      "grad_norm": 0.9620773196220398,
      "learning_rate": 7.164789131042842e-05,
      "loss": 1.0273,
      "step": 3490
    },
    {
      "epoch": 2.06975025860795,
      "grad_norm": 2.339158296585083,
      "learning_rate": 7.082489804351181e-05,
      "loss": 0.8009,
      "step": 3500
    },
    {
      "epoch": 2.06975025860795,
      "eval_loss": 0.7958757281303406,
      "eval_runtime": 216.7165,
      "eval_samples_per_second": 6.94,
      "eval_steps_per_second": 0.867,
      "step": 3500
    },
    {
      "epoch": 2.075661297473031,
      "grad_norm": 1.4927699565887451,
      "learning_rate": 7.000519552022843e-05,
      "loss": 0.8057,
      "step": 3510
    },
    {
      "epoch": 2.0815723363381116,
      "grad_norm": 0.9427938461303711,
      "learning_rate": 6.918881780975903e-05,
      "loss": 0.8929,
      "step": 3520
    },
    {
      "epoch": 2.087483375203192,
      "grad_norm": 0.5895576477050781,
      "learning_rate": 6.837579884309554e-05,
      "loss": 0.8359,
      "step": 3530
    },
    {
      "epoch": 2.0933944140682725,
      "grad_norm": 0.6642448902130127,
      "learning_rate": 6.7566172411631e-05,
      "loss": 0.9184,
      "step": 3540
    },
    {
      "epoch": 2.0993054529333532,
      "grad_norm": 1.91279935836792,
      "learning_rate": 6.675997216575503e-05,
      "loss": 0.885,
      "step": 3550
    },
    {
      "epoch": 2.1052164917984335,
      "grad_norm": 1.6055619716644287,
      "learning_rate": 6.595723161345506e-05,
      "loss": 0.9128,
      "step": 3560
    },
    {
      "epoch": 2.111127530663514,
      "grad_norm": 0.9843681454658508,
      "learning_rate": 6.515798411892394e-05,
      "loss": 0.3831,
      "step": 3570
    },
    {
      "epoch": 2.1170385695285945,
      "grad_norm": 2.3723793029785156,
      "learning_rate": 6.436226290117281e-05,
      "loss": 0.6262,
      "step": 3580
    },
    {
      "epoch": 2.122949608393675,
      "grad_norm": 0.8506283164024353,
      "learning_rate": 6.357010103265082e-05,
      "loss": 0.847,
      "step": 3590
    },
    {
      "epoch": 2.128860647258756,
      "grad_norm": 0.4192650616168976,
      "learning_rate": 6.278153143787038e-05,
      "loss": 0.6392,
      "step": 3600
    },
    {
      "epoch": 2.134771686123836,
      "grad_norm": 1.2198008298873901,
      "learning_rate": 6.199658689203872e-05,
      "loss": 0.5774,
      "step": 3610
    },
    {
      "epoch": 2.140682724988917,
      "grad_norm": 0.7610843777656555,
      "learning_rate": 6.121530001969564e-05,
      "loss": 0.6027,
      "step": 3620
    },
    {
      "epoch": 2.146593763853997,
      "grad_norm": 0.7474936842918396,
      "learning_rate": 6.04377032933576e-05,
      "loss": 0.6412,
      "step": 3630
    },
    {
      "epoch": 2.152504802719078,
      "grad_norm": 0.8744323253631592,
      "learning_rate": 5.966382903216802e-05,
      "loss": 0.8044,
      "step": 3640
    },
    {
      "epoch": 2.1584158415841586,
      "grad_norm": 0.811712920665741,
      "learning_rate": 5.889370940055412e-05,
      "loss": 0.6805,
      "step": 3650
    },
    {
      "epoch": 2.164326880449239,
      "grad_norm": 0.6412089467048645,
      "learning_rate": 5.8127376406889775e-05,
      "loss": 0.5735,
      "step": 3660
    },
    {
      "epoch": 2.1702379193143195,
      "grad_norm": 1.328116774559021,
      "learning_rate": 5.7364861902165506e-05,
      "loss": 0.7771,
      "step": 3670
    },
    {
      "epoch": 2.1761489581794002,
      "grad_norm": 1.4215295314788818,
      "learning_rate": 5.660619757866452e-05,
      "loss": 0.7894,
      "step": 3680
    },
    {
      "epoch": 2.1820599970444805,
      "grad_norm": 1.5372501611709595,
      "learning_rate": 5.585141496864551e-05,
      "loss": 0.9003,
      "step": 3690
    },
    {
      "epoch": 2.187971035909561,
      "grad_norm": 0.6534603238105774,
      "learning_rate": 5.5100545443031864e-05,
      "loss": 0.9832,
      "step": 3700
    },
    {
      "epoch": 2.1938820747746415,
      "grad_norm": 0.8194186687469482,
      "learning_rate": 5.435362021010818e-05,
      "loss": 0.5717,
      "step": 3710
    },
    {
      "epoch": 2.199793113639722,
      "grad_norm": 0.743380606174469,
      "learning_rate": 5.36106703142229e-05,
      "loss": 1.0343,
      "step": 3720
    },
    {
      "epoch": 2.205704152504803,
      "grad_norm": 1.1347907781600952,
      "learning_rate": 5.287172663449825e-05,
      "loss": 0.7995,
      "step": 3730
    },
    {
      "epoch": 2.211615191369883,
      "grad_norm": 1.0554360151290894,
      "learning_rate": 5.2136819883546346e-05,
      "loss": 0.8494,
      "step": 3740
    },
    {
      "epoch": 2.217526230234964,
      "grad_norm": 0.29485803842544556,
      "learning_rate": 5.140598060619326e-05,
      "loss": 0.6558,
      "step": 3750
    },
    {
      "epoch": 2.223437269100044,
      "grad_norm": 0.9659499526023865,
      "learning_rate": 5.0679239178209106e-05,
      "loss": 0.9674,
      "step": 3760
    },
    {
      "epoch": 2.229348307965125,
      "grad_norm": 0.5431637763977051,
      "learning_rate": 4.99566258050458e-05,
      "loss": 0.7972,
      "step": 3770
    },
    {
      "epoch": 2.2352593468302056,
      "grad_norm": 1.1634441614151,
      "learning_rate": 4.923817052058124e-05,
      "loss": 1.0142,
      "step": 3780
    },
    {
      "epoch": 2.241170385695286,
      "grad_norm": 1.2002689838409424,
      "learning_rate": 4.852390318587149e-05,
      "loss": 0.8378,
      "step": 3790
    },
    {
      "epoch": 2.2470814245603665,
      "grad_norm": 1.1004916429519653,
      "learning_rate": 4.781385348790941e-05,
      "loss": 0.9512,
      "step": 3800
    },
    {
      "epoch": 2.2529924634254472,
      "grad_norm": 0.8748389482498169,
      "learning_rate": 4.7108050938390764e-05,
      "loss": 0.8364,
      "step": 3810
    },
    {
      "epoch": 2.2589035022905275,
      "grad_norm": 0.4546831250190735,
      "learning_rate": 4.640652487248769e-05,
      "loss": 0.8057,
      "step": 3820
    },
    {
      "epoch": 2.264814541155608,
      "grad_norm": 2.3865737915039062,
      "learning_rate": 4.570930444762947e-05,
      "loss": 0.8058,
      "step": 3830
    },
    {
      "epoch": 2.2707255800206885,
      "grad_norm": 1.249647617340088,
      "learning_rate": 4.5016418642290624e-05,
      "loss": 0.7244,
      "step": 3840
    },
    {
      "epoch": 2.276636618885769,
      "grad_norm": 0.5904479622840881,
      "learning_rate": 4.432789625478651e-05,
      "loss": 0.7419,
      "step": 3850
    },
    {
      "epoch": 2.28254765775085,
      "grad_norm": 1.362351417541504,
      "learning_rate": 4.364376590207625e-05,
      "loss": 0.6808,
      "step": 3860
    },
    {
      "epoch": 2.28845869661593,
      "grad_norm": 0.37905487418174744,
      "learning_rate": 4.2964056018573586e-05,
      "loss": 0.6133,
      "step": 3870
    },
    {
      "epoch": 2.294369735481011,
      "grad_norm": 1.2001434564590454,
      "learning_rate": 4.228879485496484e-05,
      "loss": 0.7235,
      "step": 3880
    },
    {
      "epoch": 2.300280774346091,
      "grad_norm": 0.9977346658706665,
      "learning_rate": 4.1618010477034856e-05,
      "loss": 0.652,
      "step": 3890
    },
    {
      "epoch": 2.306191813211172,
      "grad_norm": 0.6327729225158691,
      "learning_rate": 4.095173076450047e-05,
      "loss": 0.6558,
      "step": 3900
    },
    {
      "epoch": 2.3121028520762525,
      "grad_norm": 1.1907110214233398,
      "learning_rate": 4.028998340985162e-05,
      "loss": 0.9611,
      "step": 3910
    },
    {
      "epoch": 2.318013890941333,
      "grad_norm": 2.266871452331543,
      "learning_rate": 3.963279591720067e-05,
      "loss": 0.8131,
      "step": 3920
    },
    {
      "epoch": 2.3239249298064135,
      "grad_norm": 1.4164915084838867,
      "learning_rate": 3.8980195601138954e-05,
      "loss": 0.8755,
      "step": 3930
    },
    {
      "epoch": 2.3298359686714942,
      "grad_norm": 0.9618287682533264,
      "learning_rate": 3.833220958560171e-05,
      "loss": 0.6584,
      "step": 3940
    },
    {
      "epoch": 2.3357470075365745,
      "grad_norm": 0.762345016002655,
      "learning_rate": 3.76888648027406e-05,
      "loss": 0.7183,
      "step": 3950
    },
    {
      "epoch": 2.341658046401655,
      "grad_norm": 0.5677629113197327,
      "learning_rate": 3.70501879918044e-05,
      "loss": 0.6139,
      "step": 3960
    },
    {
      "epoch": 2.3475690852667355,
      "grad_norm": 0.9138875603675842,
      "learning_rate": 3.641620569802762e-05,
      "loss": 0.5127,
      "step": 3970
    },
    {
      "epoch": 2.353480124131816,
      "grad_norm": 0.5430079698562622,
      "learning_rate": 3.578694427152728e-05,
      "loss": 0.8407,
      "step": 3980
    },
    {
      "epoch": 2.359391162996897,
      "grad_norm": 1.254920482635498,
      "learning_rate": 3.516242986620748e-05,
      "loss": 0.7242,
      "step": 3990
    },
    {
      "epoch": 2.365302201861977,
      "grad_norm": 0.3669591248035431,
      "learning_rate": 3.454268843867274e-05,
      "loss": 0.6986,
      "step": 4000
    },
    {
      "epoch": 2.365302201861977,
      "eval_loss": 0.7831401228904724,
      "eval_runtime": 216.4684,
      "eval_samples_per_second": 6.948,
      "eval_steps_per_second": 0.868,
      "step": 4000
    },
    {
      "epoch": 2.371213240727058,
      "grad_norm": 2.4419634342193604,
      "learning_rate": 3.392774574714889e-05,
      "loss": 0.914,
      "step": 4010
    },
    {
      "epoch": 2.377124279592138,
      "grad_norm": 0.3529356122016907,
      "learning_rate": 3.331762735041259e-05,
      "loss": 0.6925,
      "step": 4020
    },
    {
      "epoch": 2.383035318457219,
      "grad_norm": 1.2477484941482544,
      "learning_rate": 3.2712358606728935e-05,
      "loss": 0.8576,
      "step": 4030
    },
    {
      "epoch": 2.3889463573222995,
      "grad_norm": 2.377547264099121,
      "learning_rate": 3.2111964672797655e-05,
      "loss": 0.8389,
      "step": 4040
    },
    {
      "epoch": 2.39485739618738,
      "grad_norm": 0.3416961431503296,
      "learning_rate": 3.151647050270749e-05,
      "loss": 0.752,
      "step": 4050
    },
    {
      "epoch": 2.4007684350524605,
      "grad_norm": 0.7430485486984253,
      "learning_rate": 3.0925900846898935e-05,
      "loss": 0.6847,
      "step": 4060
    },
    {
      "epoch": 2.4066794739175412,
      "grad_norm": 0.8410356640815735,
      "learning_rate": 3.0340280251135484e-05,
      "loss": 0.76,
      "step": 4070
    },
    {
      "epoch": 2.4125905127826215,
      "grad_norm": 0.8880612254142761,
      "learning_rate": 2.975963305548371e-05,
      "loss": 0.7665,
      "step": 4080
    },
    {
      "epoch": 2.418501551647702,
      "grad_norm": 2.347659111022949,
      "learning_rate": 2.918398339330136e-05,
      "loss": 0.8567,
      "step": 4090
    },
    {
      "epoch": 2.4244125905127825,
      "grad_norm": 1.2962367534637451,
      "learning_rate": 2.8613355190234455e-05,
      "loss": 0.6219,
      "step": 4100
    },
    {
      "epoch": 2.430323629377863,
      "grad_norm": 1.4494484663009644,
      "learning_rate": 2.8047772163222747e-05,
      "loss": 0.5631,
      "step": 4110
    },
    {
      "epoch": 2.436234668242944,
      "grad_norm": 1.3036984205245972,
      "learning_rate": 2.7487257819514086e-05,
      "loss": 0.64,
      "step": 4120
    },
    {
      "epoch": 2.442145707108024,
      "grad_norm": 1.5717874765396118,
      "learning_rate": 2.693183545568737e-05,
      "loss": 0.9734,
      "step": 4130
    },
    {
      "epoch": 2.448056745973105,
      "grad_norm": 0.6067867875099182,
      "learning_rate": 2.6381528156684234e-05,
      "loss": 0.6546,
      "step": 4140
    },
    {
      "epoch": 2.453967784838185,
      "grad_norm": 0.9740867018699646,
      "learning_rate": 2.583635879484959e-05,
      "loss": 0.6385,
      "step": 4150
    },
    {
      "epoch": 2.459878823703266,
      "grad_norm": 0.9721751809120178,
      "learning_rate": 2.5296350028980965e-05,
      "loss": 0.8037,
      "step": 4160
    },
    {
      "epoch": 2.4657898625683465,
      "grad_norm": 0.9825825095176697,
      "learning_rate": 2.476152430338683e-05,
      "loss": 0.6831,
      "step": 4170
    },
    {
      "epoch": 2.471700901433427,
      "grad_norm": 0.5306864976882935,
      "learning_rate": 2.4231903846953647e-05,
      "loss": 0.6157,
      "step": 4180
    },
    {
      "epoch": 2.4776119402985075,
      "grad_norm": 1.1207877397537231,
      "learning_rate": 2.3707510672221942e-05,
      "loss": 0.6745,
      "step": 4190
    },
    {
      "epoch": 2.483522979163588,
      "grad_norm": 1.33757746219635,
      "learning_rate": 2.3188366574471538e-05,
      "loss": 0.9375,
      "step": 4200
    },
    {
      "epoch": 2.4894340180286685,
      "grad_norm": 1.711676001548767,
      "learning_rate": 2.2674493130815608e-05,
      "loss": 0.5645,
      "step": 4210
    },
    {
      "epoch": 2.495345056893749,
      "grad_norm": 0.5037164092063904,
      "learning_rate": 2.2165911699303873e-05,
      "loss": 0.6987,
      "step": 4220
    },
    {
      "epoch": 2.5012560957588295,
      "grad_norm": 0.416162371635437,
      "learning_rate": 2.1662643418034875e-05,
      "loss": 0.705,
      "step": 4230
    },
    {
      "epoch": 2.50716713462391,
      "grad_norm": 1.2607579231262207,
      "learning_rate": 2.116470920427743e-05,
      "loss": 0.5866,
      "step": 4240
    },
    {
      "epoch": 2.5130781734889904,
      "grad_norm": 1.388716459274292,
      "learning_rate": 2.0672129753601213e-05,
      "loss": 0.788,
      "step": 4250
    },
    {
      "epoch": 2.518989212354071,
      "grad_norm": 0.47237861156463623,
      "learning_rate": 2.0184925539016845e-05,
      "loss": 0.6764,
      "step": 4260
    },
    {
      "epoch": 2.524900251219152,
      "grad_norm": 0.9046260118484497,
      "learning_rate": 1.9703116810124488e-05,
      "loss": 0.6319,
      "step": 4270
    },
    {
      "epoch": 2.530811290084232,
      "grad_norm": 0.6046919822692871,
      "learning_rate": 1.9226723592272636e-05,
      "loss": 0.9056,
      "step": 4280
    },
    {
      "epoch": 2.536722328949313,
      "grad_norm": 0.6925544142723083,
      "learning_rate": 1.8755765685725655e-05,
      "loss": 0.5851,
      "step": 4290
    },
    {
      "epoch": 2.5426333678143935,
      "grad_norm": 1.8634053468704224,
      "learning_rate": 1.829026266484081e-05,
      "loss": 0.6579,
      "step": 4300
    },
    {
      "epoch": 2.548544406679474,
      "grad_norm": 0.7528061866760254,
      "learning_rate": 1.7830233877254745e-05,
      "loss": 0.8381,
      "step": 4310
    },
    {
      "epoch": 2.5544554455445545,
      "grad_norm": 3.158266067504883,
      "learning_rate": 1.737569844307924e-05,
      "loss": 0.9947,
      "step": 4320
    },
    {
      "epoch": 2.560366484409635,
      "grad_norm": 0.6410491466522217,
      "learning_rate": 1.6926675254106686e-05,
      "loss": 0.7088,
      "step": 4330
    },
    {
      "epoch": 2.5662775232747155,
      "grad_norm": 0.7416762113571167,
      "learning_rate": 1.6483182973024767e-05,
      "loss": 0.6659,
      "step": 4340
    },
    {
      "epoch": 2.572188562139796,
      "grad_norm": 0.6394035220146179,
      "learning_rate": 1.6045240032640844e-05,
      "loss": 0.7961,
      "step": 4350
    },
    {
      "epoch": 2.5780996010048765,
      "grad_norm": 0.5880299210548401,
      "learning_rate": 1.561286463511571e-05,
      "loss": 0.7214,
      "step": 4360
    },
    {
      "epoch": 2.584010639869957,
      "grad_norm": 0.9227321147918701,
      "learning_rate": 1.5186074751207306e-05,
      "loss": 0.7208,
      "step": 4370
    },
    {
      "epoch": 2.5899216787350374,
      "grad_norm": 0.3519112765789032,
      "learning_rate": 1.4764888119523571e-05,
      "loss": 0.595,
      "step": 4380
    },
    {
      "epoch": 2.595832717600118,
      "grad_norm": 0.9248803853988647,
      "learning_rate": 1.43493222457853e-05,
      "loss": 0.735,
      "step": 4390
    },
    {
      "epoch": 2.601743756465199,
      "grad_norm": 2.36378812789917,
      "learning_rate": 1.393939440209842e-05,
      "loss": 0.76,
      "step": 4400
    },
    {
      "epoch": 2.607654795330279,
      "grad_norm": 1.684300422668457,
      "learning_rate": 1.3535121626236306e-05,
      "loss": 0.9871,
      "step": 4410
    },
    {
      "epoch": 2.61356583419536,
      "grad_norm": 0.8449743986129761,
      "learning_rate": 1.3136520720931532e-05,
      "loss": 0.7172,
      "step": 4420
    },
    {
      "epoch": 2.6194768730604405,
      "grad_norm": 1.813154935836792,
      "learning_rate": 1.274360825317749e-05,
      "loss": 0.775,
      "step": 4430
    },
    {
      "epoch": 2.625387911925521,
      "grad_norm": 1.5046746730804443,
      "learning_rate": 1.2356400553539813e-05,
      "loss": 0.7755,
      "step": 4440
    },
    {
      "epoch": 2.6312989507906015,
      "grad_norm": 0.7537299990653992,
      "learning_rate": 1.1974913715477736e-05,
      "loss": 0.5475,
      "step": 4450
    },
    {
      "epoch": 2.637209989655682,
      "grad_norm": 2.244863271713257,
      "learning_rate": 1.1599163594675037e-05,
      "loss": 0.8211,
      "step": 4460
    },
    {
      "epoch": 2.6431210285207625,
      "grad_norm": 0.5555090308189392,
      "learning_rate": 1.1229165808381213e-05,
      "loss": 0.6531,
      "step": 4470
    },
    {
      "epoch": 2.649032067385843,
      "grad_norm": 1.23273503780365,
      "learning_rate": 1.0864935734762238e-05,
      "loss": 0.7228,
      "step": 4480
    },
    {
      "epoch": 2.6549431062509234,
      "grad_norm": 0.7961542010307312,
      "learning_rate": 1.0506488512261441e-05,
      "loss": 0.5834,
      "step": 4490
    },
    {
      "epoch": 2.660854145116004,
      "grad_norm": 1.0347894430160522,
      "learning_rate": 1.0153839038970352e-05,
      "loss": 0.5897,
      "step": 4500
    },
    {
      "epoch": 2.660854145116004,
      "eval_loss": 0.7774315476417542,
      "eval_runtime": 216.9898,
      "eval_samples_per_second": 6.931,
      "eval_steps_per_second": 0.866,
      "step": 4500
    },
    {
      "epoch": 2.6667651839810844,
      "grad_norm": 2.094717025756836,
      "learning_rate": 9.807001972009503e-06,
      "loss": 0.6304,
      "step": 4510
    },
    {
      "epoch": 2.672676222846165,
      "grad_norm": 0.7535915374755859,
      "learning_rate": 9.465991726919092e-06,
      "loss": 0.7261,
      "step": 4520
    },
    {
      "epoch": 2.678587261711246,
      "grad_norm": 1.9710290431976318,
      "learning_rate": 9.130822477060017e-06,
      "loss": 0.8981,
      "step": 4530
    },
    {
      "epoch": 2.684498300576326,
      "grad_norm": 0.7420013546943665,
      "learning_rate": 8.801508153024712e-06,
      "loss": 0.5728,
      "step": 4540
    },
    {
      "epoch": 2.690409339441407,
      "grad_norm": 0.7955825924873352,
      "learning_rate": 8.478062442058108e-06,
      "loss": 0.784,
      "step": 4550
    },
    {
      "epoch": 2.6963203783064875,
      "grad_norm": 1.2760019302368164,
      "learning_rate": 8.160498787488895e-06,
      "loss": 0.5478,
      "step": 4560
    },
    {
      "epoch": 2.702231417171568,
      "grad_norm": 0.5023969411849976,
      "learning_rate": 7.848830388170496e-06,
      "loss": 1.1851,
      "step": 4570
    },
    {
      "epoch": 2.7081424560366485,
      "grad_norm": 1.3517595529556274,
      "learning_rate": 7.543070197932871e-06,
      "loss": 0.6051,
      "step": 4580
    },
    {
      "epoch": 2.714053494901729,
      "grad_norm": 0.8358460068702698,
      "learning_rate": 7.243230925043808e-06,
      "loss": 0.7065,
      "step": 4590
    },
    {
      "epoch": 2.7199645337668095,
      "grad_norm": 1.015676736831665,
      "learning_rate": 6.949325031680847e-06,
      "loss": 0.6396,
      "step": 4600
    },
    {
      "epoch": 2.72587557263189,
      "grad_norm": 0.4526829123497009,
      "learning_rate": 6.66136473341331e-06,
      "loss": 0.8,
      "step": 4610
    },
    {
      "epoch": 2.7317866114969704,
      "grad_norm": 0.9037315249443054,
      "learning_rate": 6.37936199869461e-06,
      "loss": 1.1117,
      "step": 4620
    },
    {
      "epoch": 2.737697650362051,
      "grad_norm": 2.0437495708465576,
      "learning_rate": 6.103328548364761e-06,
      "loss": 0.9821,
      "step": 4630
    },
    {
      "epoch": 2.7436086892271314,
      "grad_norm": 1.127185344696045,
      "learning_rate": 5.833275855163288e-06,
      "loss": 0.6398,
      "step": 4640
    },
    {
      "epoch": 2.749519728092212,
      "grad_norm": 0.9567862749099731,
      "learning_rate": 5.5692151432522925e-06,
      "loss": 0.7446,
      "step": 4650
    },
    {
      "epoch": 2.755430766957293,
      "grad_norm": 0.369215190410614,
      "learning_rate": 5.311157387750003e-06,
      "loss": 0.5271,
      "step": 4660
    },
    {
      "epoch": 2.761341805822373,
      "grad_norm": 0.7931153774261475,
      "learning_rate": 5.059113314274682e-06,
      "loss": 0.7422,
      "step": 4670
    },
    {
      "epoch": 2.767252844687454,
      "grad_norm": 0.8392884731292725,
      "learning_rate": 4.813093398498691e-06,
      "loss": 0.8585,
      "step": 4680
    },
    {
      "epoch": 2.7731638835525345,
      "grad_norm": 1.7461066246032715,
      "learning_rate": 4.573107865713227e-06,
      "loss": 0.634,
      "step": 4690
    },
    {
      "epoch": 2.779074922417615,
      "grad_norm": 1.7114349603652954,
      "learning_rate": 4.339166690403262e-06,
      "loss": 0.4016,
      "step": 4700
    },
    {
      "epoch": 2.7849859612826955,
      "grad_norm": 1.4191147089004517,
      "learning_rate": 4.111279595832956e-06,
      "loss": 0.7684,
      "step": 4710
    },
    {
      "epoch": 2.790897000147776,
      "grad_norm": 1.7734280824661255,
      "learning_rate": 3.889456053641615e-06,
      "loss": 1.0732,
      "step": 4720
    },
    {
      "epoch": 2.7968080390128565,
      "grad_norm": 1.0439033508300781,
      "learning_rate": 3.673705283449907e-06,
      "loss": 0.5673,
      "step": 4730
    },
    {
      "epoch": 2.802719077877937,
      "grad_norm": 1.0640641450881958,
      "learning_rate": 3.4640362524767982e-06,
      "loss": 0.6807,
      "step": 4740
    },
    {
      "epoch": 2.8086301167430174,
      "grad_norm": 0.5727302432060242,
      "learning_rate": 3.2604576751667387e-06,
      "loss": 0.7394,
      "step": 4750
    },
    {
      "epoch": 2.814541155608098,
      "grad_norm": 1.154783844947815,
      "learning_rate": 3.062978012827516e-06,
      "loss": 0.6368,
      "step": 4760
    },
    {
      "epoch": 2.8204521944731784,
      "grad_norm": 1.43669855594635,
      "learning_rate": 2.871605473278571e-06,
      "loss": 1.0441,
      "step": 4770
    },
    {
      "epoch": 2.826363233338259,
      "grad_norm": 1.3689054250717163,
      "learning_rate": 2.6863480105098866e-06,
      "loss": 0.727,
      "step": 4780
    },
    {
      "epoch": 2.83227427220334,
      "grad_norm": 1.0765893459320068,
      "learning_rate": 2.5072133243513204e-06,
      "loss": 0.7399,
      "step": 4790
    },
    {
      "epoch": 2.83818531106842,
      "grad_norm": 1.8043493032455444,
      "learning_rate": 2.3342088601526756e-06,
      "loss": 0.7245,
      "step": 4800
    },
    {
      "epoch": 2.844096349933501,
      "grad_norm": 2.098195791244507,
      "learning_rate": 2.1673418084741657e-06,
      "loss": 0.6975,
      "step": 4810
    },
    {
      "epoch": 2.8500073887985815,
      "grad_norm": 0.6515109539031982,
      "learning_rate": 2.0066191047876045e-06,
      "loss": 0.8378,
      "step": 4820
    },
    {
      "epoch": 2.8559184276636618,
      "grad_norm": 1.7937151193618774,
      "learning_rate": 1.852047429188136e-06,
      "loss": 0.9913,
      "step": 4830
    },
    {
      "epoch": 2.8618294665287425,
      "grad_norm": 0.7201969623565674,
      "learning_rate": 1.7036332061166058e-06,
      "loss": 0.5893,
      "step": 4840
    },
    {
      "epoch": 2.867740505393823,
      "grad_norm": 0.6122762560844421,
      "learning_rate": 1.5613826040924594e-06,
      "loss": 0.5221,
      "step": 4850
    },
    {
      "epoch": 2.8736515442589035,
      "grad_norm": 1.1653082370758057,
      "learning_rate": 1.4253015354575127e-06,
      "loss": 0.4399,
      "step": 4860
    },
    {
      "epoch": 2.879562583123984,
      "grad_norm": 0.7428579926490784,
      "learning_rate": 1.295395656130066e-06,
      "loss": 0.9478,
      "step": 4870
    },
    {
      "epoch": 2.8854736219890644,
      "grad_norm": 0.7603387832641602,
      "learning_rate": 1.1716703653699254e-06,
      "loss": 0.6077,
      "step": 4880
    },
    {
      "epoch": 2.891384660854145,
      "grad_norm": 2.3287529945373535,
      "learning_rate": 1.0541308055539988e-06,
      "loss": 1.074,
      "step": 4890
    },
    {
      "epoch": 2.8972956997192254,
      "grad_norm": 0.728939414024353,
      "learning_rate": 9.427818619624161e-07,
      "loss": 0.6695,
      "step": 4900
    },
    {
      "epoch": 2.903206738584306,
      "grad_norm": 1.7719676494598389,
      "learning_rate": 8.37628162575743e-07,
      "loss": 0.5951,
      "step": 4910
    },
    {
      "epoch": 2.909117777449387,
      "grad_norm": 0.5204819440841675,
      "learning_rate": 7.386740778823507e-07,
      "loss": 0.9655,
      "step": 4920
    },
    {
      "epoch": 2.915028816314467,
      "grad_norm": 0.41195768117904663,
      "learning_rate": 6.459237206969781e-07,
      "loss": 1.0215,
      "step": 4930
    },
    {
      "epoch": 2.920939855179548,
      "grad_norm": 1.139504313468933,
      "learning_rate": 5.593809459896847e-07,
      "loss": 0.608,
      "step": 4940
    },
    {
      "epoch": 2.9268508940446285,
      "grad_norm": 0.49565747380256653,
      "learning_rate": 4.790493507256299e-07,
      "loss": 0.6537,
      "step": 4950
    },
    {
      "epoch": 2.9327619329097088,
      "grad_norm": 1.7623628377914429,
      "learning_rate": 4.049322737156413e-07,
      "loss": 0.6077,
      "step": 4960
    },
    {
      "epoch": 2.9386729717747895,
      "grad_norm": 1.7346184253692627,
      "learning_rate": 3.370327954773433e-07,
      "loss": 0.6485,
      "step": 4970
    },
    {
      "epoch": 2.94458401063987,
      "grad_norm": 0.7842738628387451,
      "learning_rate": 2.7535373810724236e-07,
      "loss": 0.6901,
      "step": 4980
    },
    {
      "epoch": 2.9504950495049505,
      "grad_norm": 0.5864334106445312,
      "learning_rate": 2.1989766516328776e-07,
      "loss": 0.6242,
      "step": 4990
    },
    {
      "epoch": 2.956406088370031,
      "grad_norm": 0.8400049805641174,
      "learning_rate": 1.7066688155848995e-07,
      "loss": 0.6539,
      "step": 5000
    },
    {
      "epoch": 2.956406088370031,
      "eval_loss": 0.7760370373725891,
      "eval_runtime": 216.3242,
      "eval_samples_per_second": 6.953,
      "eval_steps_per_second": 0.869,
      "step": 5000
    },
    {
      "epoch": 2.9623171272351114,
      "grad_norm": 0.7856336832046509,
      "learning_rate": 1.2766343346494735e-07,
      "loss": 0.8076,
      "step": 5010
    },
    {
      "epoch": 2.968228166100192,
      "grad_norm": 0.6695893406867981,
      "learning_rate": 9.088910822894757e-08,
      "loss": 0.5036,
      "step": 5020
    },
    {
      "epoch": 2.9741392049652724,
      "grad_norm": 1.2050156593322754,
      "learning_rate": 6.034543429659367e-08,
      "loss": 0.6537,
      "step": 5030
    },
    {
      "epoch": 2.980050243830353,
      "grad_norm": 1.1202857494354248,
      "learning_rate": 3.603368115032146e-08,
      "loss": 0.9312,
      "step": 5040
    },
    {
      "epoch": 2.985961282695434,
      "grad_norm": 0.8089534044265747,
      "learning_rate": 1.7954859256091858e-08,
      "loss": 0.6342,
      "step": 5050
    },
    {
      "epoch": 2.991872321560514,
      "grad_norm": 1.1870704889297485,
      "learning_rate": 6.109720021457709e-09,
      "loss": 0.8009,
      "step": 5060
    },
    {
      "epoch": 2.997783360425595,
      "grad_norm": 2.282778263092041,
      "learning_rate": 4.98755764272163e-10,
      "loss": 0.7797,
      "step": 5070
    }
  ],
  "logging_steps": 10,
  "max_steps": 5073,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.409728312822989e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
