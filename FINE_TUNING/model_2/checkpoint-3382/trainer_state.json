{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.999556672085119,
  "eval_steps": 500,
  "global_step": 3382,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005911038865080538,
      "grad_norm": 0.2276100516319275,
      "learning_rate": 1.3499999999999998e-05,
      "loss": 3.9956,
      "step": 10
    },
    {
      "epoch": 0.011822077730161076,
      "grad_norm": 0.28873324394226074,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 4.0319,
      "step": 20
    },
    {
      "epoch": 0.017733116595241613,
      "grad_norm": 0.33441638946533203,
      "learning_rate": 4.3499999999999993e-05,
      "loss": 4.2832,
      "step": 30
    },
    {
      "epoch": 0.023644155460322152,
      "grad_norm": 0.4686412811279297,
      "learning_rate": 5.85e-05,
      "loss": 3.9913,
      "step": 40
    },
    {
      "epoch": 0.02955519432540269,
      "grad_norm": 0.4693009853363037,
      "learning_rate": 7.35e-05,
      "loss": 3.7657,
      "step": 50
    },
    {
      "epoch": 0.035466233190483226,
      "grad_norm": 0.41090336441993713,
      "learning_rate": 8.849999999999998e-05,
      "loss": 3.6837,
      "step": 60
    },
    {
      "epoch": 0.04137727205556377,
      "grad_norm": 0.5891649127006531,
      "learning_rate": 0.00010349999999999998,
      "loss": 3.7584,
      "step": 70
    },
    {
      "epoch": 0.047288310920644304,
      "grad_norm": 0.65283203125,
      "learning_rate": 0.0001185,
      "loss": 3.4932,
      "step": 80
    },
    {
      "epoch": 0.05319934978572484,
      "grad_norm": 0.9791437983512878,
      "learning_rate": 0.0001335,
      "loss": 3.3853,
      "step": 90
    },
    {
      "epoch": 0.05911038865080538,
      "grad_norm": 0.8459473252296448,
      "learning_rate": 0.00014849999999999998,
      "loss": 2.8556,
      "step": 100
    },
    {
      "epoch": 0.06502142751588591,
      "grad_norm": 1.0254615545272827,
      "learning_rate": 0.0001635,
      "loss": 2.8705,
      "step": 110
    },
    {
      "epoch": 0.07093246638096645,
      "grad_norm": 0.929900050163269,
      "learning_rate": 0.00017849999999999997,
      "loss": 2.4165,
      "step": 120
    },
    {
      "epoch": 0.076843505246047,
      "grad_norm": 0.9569188952445984,
      "learning_rate": 0.0001935,
      "loss": 2.2081,
      "step": 130
    },
    {
      "epoch": 0.08275454411112754,
      "grad_norm": 1.4186241626739502,
      "learning_rate": 0.00020849999999999997,
      "loss": 1.972,
      "step": 140
    },
    {
      "epoch": 0.08866558297620807,
      "grad_norm": 1.543451189994812,
      "learning_rate": 0.00022349999999999998,
      "loss": 2.0021,
      "step": 150
    },
    {
      "epoch": 0.09457662184128861,
      "grad_norm": 1.427973747253418,
      "learning_rate": 0.0002385,
      "loss": 2.3088,
      "step": 160
    },
    {
      "epoch": 0.10048766070636915,
      "grad_norm": 1.124901533126831,
      "learning_rate": 0.0002535,
      "loss": 2.1868,
      "step": 170
    },
    {
      "epoch": 0.10639869957144968,
      "grad_norm": 0.8841444849967957,
      "learning_rate": 0.00026849999999999997,
      "loss": 1.5767,
      "step": 180
    },
    {
      "epoch": 0.11230973843653022,
      "grad_norm": 1.7946240901947021,
      "learning_rate": 0.00028349999999999995,
      "loss": 1.8317,
      "step": 190
    },
    {
      "epoch": 0.11822077730161076,
      "grad_norm": 0.7886484265327454,
      "learning_rate": 0.0002985,
      "loss": 1.5576,
      "step": 200
    },
    {
      "epoch": 0.12413181616669129,
      "grad_norm": 1.8757169246673584,
      "learning_rate": 0.00029999747505462783,
      "loss": 1.5806,
      "step": 210
    },
    {
      "epoch": 0.13004285503177182,
      "grad_norm": 0.6265178322792053,
      "learning_rate": 0.00029998874695753624,
      "loss": 1.885,
      "step": 220
    },
    {
      "epoch": 0.13595389389685236,
      "grad_norm": 0.8252459168434143,
      "learning_rate": 0.00029997378489923836,
      "loss": 1.7116,
      "step": 230
    },
    {
      "epoch": 0.1418649327619329,
      "grad_norm": 0.6410897970199585,
      "learning_rate": 0.0002999525895016001,
      "loss": 1.3141,
      "step": 240
    },
    {
      "epoch": 0.14777597162701345,
      "grad_norm": 0.5499432682991028,
      "learning_rate": 0.00029992516164556297,
      "loss": 1.81,
      "step": 250
    },
    {
      "epoch": 0.153687010492094,
      "grad_norm": 2.034027338027954,
      "learning_rate": 0.00029989150247110687,
      "loss": 1.6135,
      "step": 260
    },
    {
      "epoch": 0.15959804935717453,
      "grad_norm": 0.771909236907959,
      "learning_rate": 0.0002998516133772035,
      "loss": 1.5172,
      "step": 270
    },
    {
      "epoch": 0.16550908822225507,
      "grad_norm": 0.49242275953292847,
      "learning_rate": 0.0002998054960217575,
      "loss": 1.164,
      "step": 280
    },
    {
      "epoch": 0.1714201270873356,
      "grad_norm": 0.7855795621871948,
      "learning_rate": 0.0002997531523215382,
      "loss": 1.245,
      "step": 290
    },
    {
      "epoch": 0.17733116595241613,
      "grad_norm": 0.6119864583015442,
      "learning_rate": 0.00029969458445209933,
      "loss": 1.24,
      "step": 300
    },
    {
      "epoch": 0.18324220481749667,
      "grad_norm": 1.9419697523117065,
      "learning_rate": 0.0002996297948476891,
      "loss": 1.8625,
      "step": 310
    },
    {
      "epoch": 0.18915324368257722,
      "grad_norm": 1.0362036228179932,
      "learning_rate": 0.00029955878620114873,
      "loss": 1.3991,
      "step": 320
    },
    {
      "epoch": 0.19506428254765776,
      "grad_norm": 2.193244695663452,
      "learning_rate": 0.00029948156146380057,
      "loss": 1.2342,
      "step": 330
    },
    {
      "epoch": 0.2009753214127383,
      "grad_norm": 1.1690107583999634,
      "learning_rate": 0.0002993981238453255,
      "loss": 1.2931,
      "step": 340
    },
    {
      "epoch": 0.20688636027781881,
      "grad_norm": 0.3863016366958618,
      "learning_rate": 0.0002993084768136296,
      "loss": 1.1641,
      "step": 350
    },
    {
      "epoch": 0.21279739914289936,
      "grad_norm": 2.0656979084014893,
      "learning_rate": 0.0002992126240946998,
      "loss": 1.8293,
      "step": 360
    },
    {
      "epoch": 0.2187084380079799,
      "grad_norm": 1.1432462930679321,
      "learning_rate": 0.0002991105696724492,
      "loss": 1.6783,
      "step": 370
    },
    {
      "epoch": 0.22461947687306044,
      "grad_norm": 0.9850197434425354,
      "learning_rate": 0.00029900231778855137,
      "loss": 1.4363,
      "step": 380
    },
    {
      "epoch": 0.23053051573814098,
      "grad_norm": 0.4927749037742615,
      "learning_rate": 0.00029888787294226417,
      "loss": 1.1233,
      "step": 390
    },
    {
      "epoch": 0.23644155460322153,
      "grad_norm": 1.6659817695617676,
      "learning_rate": 0.0002987672398902427,
      "loss": 1.7724,
      "step": 400
    },
    {
      "epoch": 0.24235259346830204,
      "grad_norm": 0.7641460299491882,
      "learning_rate": 0.00029864042364634145,
      "loss": 1.4903,
      "step": 410
    },
    {
      "epoch": 0.24826363233338258,
      "grad_norm": 2.1336848735809326,
      "learning_rate": 0.00029850742948140627,
      "loss": 1.3408,
      "step": 420
    },
    {
      "epoch": 0.25417467119846315,
      "grad_norm": 1.7345354557037354,
      "learning_rate": 0.00029836826292305483,
      "loss": 1.5539,
      "step": 430
    },
    {
      "epoch": 0.26008571006354364,
      "grad_norm": 1.8847336769104004,
      "learning_rate": 0.0002982229297554473,
      "loss": 1.297,
      "step": 440
    },
    {
      "epoch": 0.2659967489286242,
      "grad_norm": 0.5554383397102356,
      "learning_rate": 0.0002980714360190456,
      "loss": 1.441,
      "step": 450
    },
    {
      "epoch": 0.2719077877937047,
      "grad_norm": 0.8399075269699097,
      "learning_rate": 0.0002979137880103627,
      "loss": 1.3417,
      "step": 460
    },
    {
      "epoch": 0.27781882665878527,
      "grad_norm": 1.0724455118179321,
      "learning_rate": 0.00029774999228170064,
      "loss": 1.328,
      "step": 470
    },
    {
      "epoch": 0.2837298655238658,
      "grad_norm": 2.457491636276245,
      "learning_rate": 0.00029758005564087817,
      "loss": 1.3979,
      "step": 480
    },
    {
      "epoch": 0.28964090438894635,
      "grad_norm": 0.5214126706123352,
      "learning_rate": 0.0002974039851509481,
      "loss": 1.1475,
      "step": 490
    },
    {
      "epoch": 0.2955519432540269,
      "grad_norm": 1.4495922327041626,
      "learning_rate": 0.0002972217881299033,
      "loss": 1.3412,
      "step": 500
    },
    {
      "epoch": 0.2955519432540269,
      "eval_loss": 1.3146368265151978,
      "eval_runtime": 216.4078,
      "eval_samples_per_second": 6.95,
      "eval_steps_per_second": 0.869,
      "step": 500
    },
    {
      "epoch": 0.30146298211910744,
      "grad_norm": 0.7506139874458313,
      "learning_rate": 0.00029703347215037315,
      "loss": 1.3206,
      "step": 510
    },
    {
      "epoch": 0.307374020984188,
      "grad_norm": 1.047113060951233,
      "learning_rate": 0.00029683904503930813,
      "loss": 1.1369,
      "step": 520
    },
    {
      "epoch": 0.3132850598492685,
      "grad_norm": 0.8574621677398682,
      "learning_rate": 0.0002966385148776549,
      "loss": 1.0607,
      "step": 530
    },
    {
      "epoch": 0.31919609871434906,
      "grad_norm": 1.2770711183547974,
      "learning_rate": 0.0002964318900000204,
      "loss": 1.4287,
      "step": 540
    },
    {
      "epoch": 0.3251071375794296,
      "grad_norm": 0.9782669544219971,
      "learning_rate": 0.00029621917899432546,
      "loss": 1.2887,
      "step": 550
    },
    {
      "epoch": 0.33101817644451015,
      "grad_norm": 0.8555631637573242,
      "learning_rate": 0.0002960003907014476,
      "loss": 1.0195,
      "step": 560
    },
    {
      "epoch": 0.33692921530959064,
      "grad_norm": 1.2670342922210693,
      "learning_rate": 0.0002957755342148539,
      "loss": 1.7348,
      "step": 570
    },
    {
      "epoch": 0.3428402541746712,
      "grad_norm": 0.8302950859069824,
      "learning_rate": 0.00029554461888022296,
      "loss": 1.5702,
      "step": 580
    },
    {
      "epoch": 0.3487512930397517,
      "grad_norm": 1.1769555807113647,
      "learning_rate": 0.00029530765429505636,
      "loss": 1.5087,
      "step": 590
    },
    {
      "epoch": 0.35466233190483226,
      "grad_norm": 1.7179492712020874,
      "learning_rate": 0.00029506465030827977,
      "loss": 1.1668,
      "step": 600
    },
    {
      "epoch": 0.3605733707699128,
      "grad_norm": 1.6589977741241455,
      "learning_rate": 0.0002948156170198338,
      "loss": 1.211,
      "step": 610
    },
    {
      "epoch": 0.36648440963499335,
      "grad_norm": 0.8197120428085327,
      "learning_rate": 0.000294560564780254,
      "loss": 1.3555,
      "step": 620
    },
    {
      "epoch": 0.3723954485000739,
      "grad_norm": 2.922220468521118,
      "learning_rate": 0.00029429950419024075,
      "loss": 1.7,
      "step": 630
    },
    {
      "epoch": 0.37830648736515443,
      "grad_norm": 0.39568033814430237,
      "learning_rate": 0.0002940324461002187,
      "loss": 1.1265,
      "step": 640
    },
    {
      "epoch": 0.384217526230235,
      "grad_norm": 0.767401933670044,
      "learning_rate": 0.0002937594016098856,
      "loss": 1.0005,
      "step": 650
    },
    {
      "epoch": 0.3901285650953155,
      "grad_norm": 0.4762462079524994,
      "learning_rate": 0.00029348038206775134,
      "loss": 1.0934,
      "step": 660
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 1.488071084022522,
      "learning_rate": 0.00029319539907066584,
      "loss": 1.5414,
      "step": 670
    },
    {
      "epoch": 0.4019506428254766,
      "grad_norm": 1.5370007753372192,
      "learning_rate": 0.0002929044644633374,
      "loss": 1.1256,
      "step": 680
    },
    {
      "epoch": 0.40786168169055714,
      "grad_norm": 2.742067813873291,
      "learning_rate": 0.0002926075903378402,
      "loss": 1.7847,
      "step": 690
    },
    {
      "epoch": 0.41377272055563763,
      "grad_norm": 0.6739020943641663,
      "learning_rate": 0.0002923047890331117,
      "loss": 1.2375,
      "step": 700
    },
    {
      "epoch": 0.41968375942071817,
      "grad_norm": 3.385519027709961,
      "learning_rate": 0.0002919960731344401,
      "loss": 1.0736,
      "step": 710
    },
    {
      "epoch": 0.4255947982857987,
      "grad_norm": 0.6010761857032776,
      "learning_rate": 0.00029168145547294085,
      "loss": 1.3674,
      "step": 720
    },
    {
      "epoch": 0.43150583715087926,
      "grad_norm": 0.5061453580856323,
      "learning_rate": 0.0002913609491250236,
      "loss": 0.7746,
      "step": 730
    },
    {
      "epoch": 0.4374168760159598,
      "grad_norm": 0.8537613153457642,
      "learning_rate": 0.00029103456741184854,
      "loss": 1.5548,
      "step": 740
    },
    {
      "epoch": 0.44332791488104034,
      "grad_norm": 0.6990466117858887,
      "learning_rate": 0.0002907023238987731,
      "loss": 1.1144,
      "step": 750
    },
    {
      "epoch": 0.4492389537461209,
      "grad_norm": 0.7418966293334961,
      "learning_rate": 0.0002903642323947876,
      "loss": 0.997,
      "step": 760
    },
    {
      "epoch": 0.4551499926112014,
      "grad_norm": 1.2348270416259766,
      "learning_rate": 0.0002900203069519417,
      "loss": 1.462,
      "step": 770
    },
    {
      "epoch": 0.46106103147628197,
      "grad_norm": 1.3047974109649658,
      "learning_rate": 0.00028967056186476024,
      "loss": 1.5589,
      "step": 780
    },
    {
      "epoch": 0.4669720703413625,
      "grad_norm": 1.1756370067596436,
      "learning_rate": 0.0002893150116696492,
      "loss": 1.2753,
      "step": 790
    },
    {
      "epoch": 0.47288310920644305,
      "grad_norm": 0.934016227722168,
      "learning_rate": 0.00028895367114429123,
      "loss": 0.9582,
      "step": 800
    },
    {
      "epoch": 0.4787941480715236,
      "grad_norm": 0.7867525219917297,
      "learning_rate": 0.0002885865553070319,
      "loss": 1.3415,
      "step": 810
    },
    {
      "epoch": 0.4847051869366041,
      "grad_norm": 0.7471956014633179,
      "learning_rate": 0.0002882136794162551,
      "loss": 1.0717,
      "step": 820
    },
    {
      "epoch": 0.4906162258016846,
      "grad_norm": 1.183523178100586,
      "learning_rate": 0.000287835058969749,
      "loss": 1.1274,
      "step": 830
    },
    {
      "epoch": 0.49652726466676517,
      "grad_norm": 2.128274440765381,
      "learning_rate": 0.00028745070970406214,
      "loss": 1.3969,
      "step": 840
    },
    {
      "epoch": 0.5024383035318457,
      "grad_norm": 0.6979519128799438,
      "learning_rate": 0.00028706064759384894,
      "loss": 1.1142,
      "step": 850
    },
    {
      "epoch": 0.5083493423969263,
      "grad_norm": 0.5899310111999512,
      "learning_rate": 0.000286664888851206,
      "loss": 0.7925,
      "step": 860
    },
    {
      "epoch": 0.5142603812620068,
      "grad_norm": 1.3585809469223022,
      "learning_rate": 0.0002862634499249985,
      "loss": 0.8093,
      "step": 870
    },
    {
      "epoch": 0.5201714201270873,
      "grad_norm": 0.9386045932769775,
      "learning_rate": 0.0002858563475001759,
      "loss": 1.0482,
      "step": 880
    },
    {
      "epoch": 0.5260824589921679,
      "grad_norm": 1.514111876487732,
      "learning_rate": 0.0002854435984970792,
      "loss": 1.3709,
      "step": 890
    },
    {
      "epoch": 0.5319934978572484,
      "grad_norm": 0.592781662940979,
      "learning_rate": 0.000285025220070737,
      "loss": 1.083,
      "step": 900
    },
    {
      "epoch": 0.537904536722329,
      "grad_norm": 0.8946014046669006,
      "learning_rate": 0.00028460122961015326,
      "loss": 0.9572,
      "step": 910
    },
    {
      "epoch": 0.5438155755874094,
      "grad_norm": 0.5024324059486389,
      "learning_rate": 0.00028417164473758374,
      "loss": 1.02,
      "step": 920
    },
    {
      "epoch": 0.54972661445249,
      "grad_norm": 0.688870370388031,
      "learning_rate": 0.0002837364833078042,
      "loss": 1.0437,
      "step": 930
    },
    {
      "epoch": 0.5556376533175705,
      "grad_norm": 0.8181172609329224,
      "learning_rate": 0.0002832957634073679,
      "loss": 1.4931,
      "step": 940
    },
    {
      "epoch": 0.5615486921826511,
      "grad_norm": 2.1315081119537354,
      "learning_rate": 0.00028284950335385415,
      "loss": 1.4226,
      "step": 950
    },
    {
      "epoch": 0.5674597310477316,
      "grad_norm": 1.5933620929718018,
      "learning_rate": 0.0002823977216951068,
      "loss": 0.72,
      "step": 960
    },
    {
      "epoch": 0.5733707699128122,
      "grad_norm": 3.1443240642547607,
      "learning_rate": 0.00028194043720846355,
      "loss": 1.3788,
      "step": 970
    },
    {
      "epoch": 0.5792818087778927,
      "grad_norm": 0.34296682476997375,
      "learning_rate": 0.00028147766889997504,
      "loss": 0.909,
      "step": 980
    },
    {
      "epoch": 0.5851928476429733,
      "grad_norm": 0.8706747889518738,
      "learning_rate": 0.0002810094360036155,
      "loss": 0.8898,
      "step": 990
    },
    {
      "epoch": 0.5911038865080538,
      "grad_norm": 1.1580030918121338,
      "learning_rate": 0.0002805357579804831,
      "loss": 1.5932,
      "step": 1000
    },
    {
      "epoch": 0.5911038865080538,
      "eval_loss": 1.0840506553649902,
      "eval_runtime": 217.1875,
      "eval_samples_per_second": 6.925,
      "eval_steps_per_second": 0.866,
      "step": 1000
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 1.5580549240112305,
      "learning_rate": 0.0002800566545179908,
      "loss": 1.2045,
      "step": 1010
    },
    {
      "epoch": 0.6029259642382149,
      "grad_norm": 1.199741005897522,
      "learning_rate": 0.0002795721455290485,
      "loss": 1.5051,
      "step": 1020
    },
    {
      "epoch": 0.6088370031032954,
      "grad_norm": 0.39826700091362,
      "learning_rate": 0.0002790822511512352,
      "loss": 0.9564,
      "step": 1030
    },
    {
      "epoch": 0.614748041968376,
      "grad_norm": 0.8212458491325378,
      "learning_rate": 0.00027858699174596215,
      "loss": 0.8803,
      "step": 1040
    },
    {
      "epoch": 0.6206590808334564,
      "grad_norm": 2.1567301750183105,
      "learning_rate": 0.00027808638789762616,
      "loss": 1.2964,
      "step": 1050
    },
    {
      "epoch": 0.626570119698537,
      "grad_norm": 0.815433919429779,
      "learning_rate": 0.0002775804604127549,
      "loss": 1.2126,
      "step": 1060
    },
    {
      "epoch": 0.6324811585636175,
      "grad_norm": 1.2142722606658936,
      "learning_rate": 0.00027706923031914105,
      "loss": 1.1671,
      "step": 1070
    },
    {
      "epoch": 0.6383921974286981,
      "grad_norm": 1.4374685287475586,
      "learning_rate": 0.00027655271886496935,
      "loss": 1.0652,
      "step": 1080
    },
    {
      "epoch": 0.6443032362937786,
      "grad_norm": 2.028388023376465,
      "learning_rate": 0.0002760309475179326,
      "loss": 1.3826,
      "step": 1090
    },
    {
      "epoch": 0.6502142751588592,
      "grad_norm": 1.4759591817855835,
      "learning_rate": 0.00027550393796434,
      "loss": 1.1697,
      "step": 1100
    },
    {
      "epoch": 0.6561253140239397,
      "grad_norm": 1.076900839805603,
      "learning_rate": 0.0002749717121082156,
      "loss": 0.9173,
      "step": 1110
    },
    {
      "epoch": 0.6620363528890203,
      "grad_norm": 1.2914588451385498,
      "learning_rate": 0.0002744342920703878,
      "loss": 0.9848,
      "step": 1120
    },
    {
      "epoch": 0.6679473917541008,
      "grad_norm": 0.7284201979637146,
      "learning_rate": 0.0002738917001875701,
      "loss": 1.1538,
      "step": 1130
    },
    {
      "epoch": 0.6738584306191813,
      "grad_norm": 1.2725015878677368,
      "learning_rate": 0.0002733439590114326,
      "loss": 1.0447,
      "step": 1140
    },
    {
      "epoch": 0.6797694694842619,
      "grad_norm": 0.8510275483131409,
      "learning_rate": 0.00027279109130766486,
      "loss": 1.1817,
      "step": 1150
    },
    {
      "epoch": 0.6856805083493424,
      "grad_norm": 1.7715678215026855,
      "learning_rate": 0.0002722331200550296,
      "loss": 1.122,
      "step": 1160
    },
    {
      "epoch": 0.691591547214423,
      "grad_norm": 1.3184545040130615,
      "learning_rate": 0.00027167006844440743,
      "loss": 1.3256,
      "step": 1170
    },
    {
      "epoch": 0.6975025860795034,
      "grad_norm": 1.1101163625717163,
      "learning_rate": 0.0002711019598778334,
      "loss": 0.8867,
      "step": 1180
    },
    {
      "epoch": 0.703413624944584,
      "grad_norm": 1.0922558307647705,
      "learning_rate": 0.0002705288179675239,
      "loss": 1.2342,
      "step": 1190
    },
    {
      "epoch": 0.7093246638096645,
      "grad_norm": 1.424505591392517,
      "learning_rate": 0.0002699506665348957,
      "loss": 1.0746,
      "step": 1200
    },
    {
      "epoch": 0.7152357026747451,
      "grad_norm": 0.6099621653556824,
      "learning_rate": 0.0002693675296095755,
      "loss": 1.0473,
      "step": 1210
    },
    {
      "epoch": 0.7211467415398256,
      "grad_norm": 1.2074164152145386,
      "learning_rate": 0.0002687794314284013,
      "loss": 1.1465,
      "step": 1220
    },
    {
      "epoch": 0.7270577804049062,
      "grad_norm": 0.6486126780509949,
      "learning_rate": 0.00026818639643441514,
      "loss": 0.9935,
      "step": 1230
    },
    {
      "epoch": 0.7329688192699867,
      "grad_norm": 0.9025202393531799,
      "learning_rate": 0.0002675884492758472,
      "loss": 0.7493,
      "step": 1240
    },
    {
      "epoch": 0.7388798581350673,
      "grad_norm": 1.1157371997833252,
      "learning_rate": 0.000266985614805091,
      "loss": 1.1788,
      "step": 1250
    },
    {
      "epoch": 0.7447908970001478,
      "grad_norm": 1.0885167121887207,
      "learning_rate": 0.00026637791807767117,
      "loss": 1.0209,
      "step": 1260
    },
    {
      "epoch": 0.7507019358652283,
      "grad_norm": 2.4428985118865967,
      "learning_rate": 0.00026576538435120115,
      "loss": 0.8735,
      "step": 1270
    },
    {
      "epoch": 0.7566129747303089,
      "grad_norm": 0.8540585041046143,
      "learning_rate": 0.00026514803908433423,
      "loss": 0.7358,
      "step": 1280
    },
    {
      "epoch": 0.7625240135953893,
      "grad_norm": 0.8274838328361511,
      "learning_rate": 0.00026452590793570486,
      "loss": 1.1572,
      "step": 1290
    },
    {
      "epoch": 0.76843505246047,
      "grad_norm": 0.8122208714485168,
      "learning_rate": 0.00026389901676286264,
      "loss": 1.1574,
      "step": 1300
    },
    {
      "epoch": 0.7743460913255504,
      "grad_norm": 0.8998039364814758,
      "learning_rate": 0.00026326739162119714,
      "loss": 1.3476,
      "step": 1310
    },
    {
      "epoch": 0.780257130190631,
      "grad_norm": 1.0652897357940674,
      "learning_rate": 0.0002626310587628554,
      "loss": 0.7719,
      "step": 1320
    },
    {
      "epoch": 0.7861681690557115,
      "grad_norm": 1.297702431678772,
      "learning_rate": 0.0002619900446356506,
      "loss": 0.9897,
      "step": 1330
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 2.369802951812744,
      "learning_rate": 0.00026134437588196276,
      "loss": 0.8323,
      "step": 1340
    },
    {
      "epoch": 0.7979902467858726,
      "grad_norm": 1.0665628910064697,
      "learning_rate": 0.0002606940793376315,
      "loss": 0.9712,
      "step": 1350
    },
    {
      "epoch": 0.8039012856509532,
      "grad_norm": 0.6863260269165039,
      "learning_rate": 0.00026003918203084075,
      "loss": 0.8202,
      "step": 1360
    },
    {
      "epoch": 0.8098123245160337,
      "grad_norm": 1.0297291278839111,
      "learning_rate": 0.0002593797111809952,
      "loss": 0.8177,
      "step": 1370
    },
    {
      "epoch": 0.8157233633811143,
      "grad_norm": 0.8568499088287354,
      "learning_rate": 0.0002587156941975891,
      "loss": 0.9803,
      "step": 1380
    },
    {
      "epoch": 0.8216344022461948,
      "grad_norm": 1.5378499031066895,
      "learning_rate": 0.00025804715867906687,
      "loss": 1.3927,
      "step": 1390
    },
    {
      "epoch": 0.8275454411112753,
      "grad_norm": 1.4208747148513794,
      "learning_rate": 0.0002573741324116764,
      "loss": 0.9625,
      "step": 1400
    },
    {
      "epoch": 0.8334564799763559,
      "grad_norm": 0.7616665363311768,
      "learning_rate": 0.0002566966433683138,
      "loss": 0.8857,
      "step": 1410
    },
    {
      "epoch": 0.8393675188414363,
      "grad_norm": 0.5649605393409729,
      "learning_rate": 0.00025601471970736103,
      "loss": 1.0454,
      "step": 1420
    },
    {
      "epoch": 0.8452785577065169,
      "grad_norm": 1.8511734008789062,
      "learning_rate": 0.0002553283897715152,
      "loss": 0.9013,
      "step": 1430
    },
    {
      "epoch": 0.8511895965715974,
      "grad_norm": 1.551117181777954,
      "learning_rate": 0.0002546376820866111,
      "loss": 1.0111,
      "step": 1440
    },
    {
      "epoch": 0.857100635436678,
      "grad_norm": 2.121812582015991,
      "learning_rate": 0.00025394262536043504,
      "loss": 1.0325,
      "step": 1450
    },
    {
      "epoch": 0.8630116743017585,
      "grad_norm": 2.537290573120117,
      "learning_rate": 0.0002532432484815321,
      "loss": 0.904,
      "step": 1460
    },
    {
      "epoch": 0.8689227131668391,
      "grad_norm": 1.2138627767562866,
      "learning_rate": 0.0002525395805180052,
      "loss": 0.8901,
      "step": 1470
    },
    {
      "epoch": 0.8748337520319196,
      "grad_norm": 0.9661867022514343,
      "learning_rate": 0.00025183165071630695,
      "loss": 0.8248,
      "step": 1480
    },
    {
      "epoch": 0.8807447908970002,
      "grad_norm": 0.527542769908905,
      "learning_rate": 0.00025111948850002413,
      "loss": 0.9494,
      "step": 1490
    },
    {
      "epoch": 0.8866558297620807,
      "grad_norm": 1.1769733428955078,
      "learning_rate": 0.00025040312346865496,
      "loss": 0.9115,
      "step": 1500
    },
    {
      "epoch": 0.8866558297620807,
      "eval_loss": 0.966824471950531,
      "eval_runtime": 217.3109,
      "eval_samples_per_second": 6.921,
      "eval_steps_per_second": 0.865,
      "step": 1500
    },
    {
      "epoch": 0.8925668686271612,
      "grad_norm": 0.8589854836463928,
      "learning_rate": 0.00024968258539637844,
      "loss": 0.8922,
      "step": 1510
    },
    {
      "epoch": 0.8984779074922418,
      "grad_norm": 0.6945375204086304,
      "learning_rate": 0.0002489579042308172,
      "loss": 0.7376,
      "step": 1520
    },
    {
      "epoch": 0.9043889463573223,
      "grad_norm": 0.36732661724090576,
      "learning_rate": 0.00024822911009179276,
      "loss": 0.8396,
      "step": 1530
    },
    {
      "epoch": 0.9102999852224029,
      "grad_norm": 1.337815761566162,
      "learning_rate": 0.00024749623327007333,
      "loss": 0.5952,
      "step": 1540
    },
    {
      "epoch": 0.9162110240874833,
      "grad_norm": 0.7494143843650818,
      "learning_rate": 0.00024675930422611526,
      "loss": 0.9009,
      "step": 1550
    },
    {
      "epoch": 0.9221220629525639,
      "grad_norm": 2.027194023132324,
      "learning_rate": 0.00024601835358879694,
      "loss": 0.8604,
      "step": 1560
    },
    {
      "epoch": 0.9280331018176444,
      "grad_norm": 0.3332920968532562,
      "learning_rate": 0.00024527341215414546,
      "loss": 0.8194,
      "step": 1570
    },
    {
      "epoch": 0.933944140682725,
      "grad_norm": 3.1836729049682617,
      "learning_rate": 0.00024452451088405693,
      "loss": 1.1178,
      "step": 1580
    },
    {
      "epoch": 0.9398551795478055,
      "grad_norm": 1.0708240270614624,
      "learning_rate": 0.00024377168090500956,
      "loss": 1.0784,
      "step": 1590
    },
    {
      "epoch": 0.9457662184128861,
      "grad_norm": 2.001142740249634,
      "learning_rate": 0.00024301495350677,
      "loss": 1.1588,
      "step": 1600
    },
    {
      "epoch": 0.9516772572779666,
      "grad_norm": 0.5157231092453003,
      "learning_rate": 0.00024225436014109272,
      "loss": 0.9851,
      "step": 1610
    },
    {
      "epoch": 0.9575882961430472,
      "grad_norm": 1.1543854475021362,
      "learning_rate": 0.0002414899324204129,
      "loss": 1.0326,
      "step": 1620
    },
    {
      "epoch": 0.9634993350081277,
      "grad_norm": 1.1339360475540161,
      "learning_rate": 0.00024072170211653237,
      "loss": 1.0898,
      "step": 1630
    },
    {
      "epoch": 0.9694103738732082,
      "grad_norm": 1.0551549196243286,
      "learning_rate": 0.00023994970115929928,
      "loss": 1.0719,
      "step": 1640
    },
    {
      "epoch": 0.9753214127382888,
      "grad_norm": 0.7559623718261719,
      "learning_rate": 0.00023917396163528092,
      "loss": 0.5571,
      "step": 1650
    },
    {
      "epoch": 0.9812324516033692,
      "grad_norm": 1.2452393770217896,
      "learning_rate": 0.00023839451578643004,
      "loss": 1.0676,
      "step": 1660
    },
    {
      "epoch": 0.9871434904684498,
      "grad_norm": 0.6703391075134277,
      "learning_rate": 0.00023761139600874496,
      "loss": 0.6167,
      "step": 1670
    },
    {
      "epoch": 0.9930545293335303,
      "grad_norm": 2.0774176120758057,
      "learning_rate": 0.00023682463485092287,
      "loss": 1.1952,
      "step": 1680
    },
    {
      "epoch": 0.9989655681986109,
      "grad_norm": 0.8266769647598267,
      "learning_rate": 0.00023603426501300718,
      "loss": 0.7562,
      "step": 1690
    },
    {
      "epoch": 1.0053199349785724,
      "grad_norm": 1.2659260034561157,
      "learning_rate": 0.0002352403193450284,
      "loss": 0.8698,
      "step": 1700
    },
    {
      "epoch": 1.0112309738436531,
      "grad_norm": 0.7206515073776245,
      "learning_rate": 0.00023444283084563867,
      "loss": 0.6852,
      "step": 1710
    },
    {
      "epoch": 1.0171420127087336,
      "grad_norm": 2.1051628589630127,
      "learning_rate": 0.00023364183266074047,
      "loss": 1.0288,
      "step": 1720
    },
    {
      "epoch": 1.023053051573814,
      "grad_norm": 1.2459101676940918,
      "learning_rate": 0.00023283735808210865,
      "loss": 0.7756,
      "step": 1730
    },
    {
      "epoch": 1.0289640904388946,
      "grad_norm": 0.8717842102050781,
      "learning_rate": 0.00023202944054600714,
      "loss": 0.8669,
      "step": 1740
    },
    {
      "epoch": 1.034875129303975,
      "grad_norm": 1.0875258445739746,
      "learning_rate": 0.0002312181136317989,
      "loss": 1.2068,
      "step": 1750
    },
    {
      "epoch": 1.0407861681690558,
      "grad_norm": 1.4109116792678833,
      "learning_rate": 0.00023040341106055033,
      "loss": 0.9505,
      "step": 1760
    },
    {
      "epoch": 1.0466972070341363,
      "grad_norm": 1.0999497175216675,
      "learning_rate": 0.00022958536669363003,
      "loss": 0.9594,
      "step": 1770
    },
    {
      "epoch": 1.0526082458992168,
      "grad_norm": 0.5027688145637512,
      "learning_rate": 0.0002287640145313009,
      "loss": 0.8496,
      "step": 1780
    },
    {
      "epoch": 1.0585192847642972,
      "grad_norm": 1.0464584827423096,
      "learning_rate": 0.00022793938871130753,
      "loss": 0.8856,
      "step": 1790
    },
    {
      "epoch": 1.064430323629378,
      "grad_norm": 1.0491294860839844,
      "learning_rate": 0.00022711152350745684,
      "loss": 0.9196,
      "step": 1800
    },
    {
      "epoch": 1.0703413624944584,
      "grad_norm": 1.4380377531051636,
      "learning_rate": 0.000226280453328194,
      "loss": 1.2361,
      "step": 1810
    },
    {
      "epoch": 1.076252401359539,
      "grad_norm": 1.8503388166427612,
      "learning_rate": 0.00022544621271517202,
      "loss": 1.3809,
      "step": 1820
    },
    {
      "epoch": 1.0821634402246194,
      "grad_norm": 0.9865618348121643,
      "learning_rate": 0.00022460883634181628,
      "loss": 0.9286,
      "step": 1830
    },
    {
      "epoch": 1.0880744790897001,
      "grad_norm": 1.408588171005249,
      "learning_rate": 0.00022376835901188319,
      "loss": 0.8711,
      "step": 1840
    },
    {
      "epoch": 1.0939855179547806,
      "grad_norm": 1.0152348279953003,
      "learning_rate": 0.00022292481565801397,
      "loss": 0.7668,
      "step": 1850
    },
    {
      "epoch": 1.099896556819861,
      "grad_norm": 2.4467756748199463,
      "learning_rate": 0.00022207824134028238,
      "loss": 1.0573,
      "step": 1860
    },
    {
      "epoch": 1.1058075956849416,
      "grad_norm": 1.2333484888076782,
      "learning_rate": 0.00022122867124473784,
      "loss": 1.0001,
      "step": 1870
    },
    {
      "epoch": 1.111718634550022,
      "grad_norm": 0.7063289284706116,
      "learning_rate": 0.00022037614068194276,
      "loss": 1.1188,
      "step": 1880
    },
    {
      "epoch": 1.1176296734151028,
      "grad_norm": 2.782322645187378,
      "learning_rate": 0.00021952068508550516,
      "loss": 0.8756,
      "step": 1890
    },
    {
      "epoch": 1.1235407122801833,
      "grad_norm": 0.707579493522644,
      "learning_rate": 0.00021866234001060566,
      "loss": 0.9134,
      "step": 1900
    },
    {
      "epoch": 1.1294517511452637,
      "grad_norm": 0.6689584255218506,
      "learning_rate": 0.00021780114113252003,
      "loss": 0.6925,
      "step": 1910
    },
    {
      "epoch": 1.1353627900103442,
      "grad_norm": 1.1791493892669678,
      "learning_rate": 0.00021693712424513607,
      "loss": 0.7996,
      "step": 1920
    },
    {
      "epoch": 1.141273828875425,
      "grad_norm": 0.6566810607910156,
      "learning_rate": 0.00021607032525946637,
      "loss": 0.8078,
      "step": 1930
    },
    {
      "epoch": 1.1471848677405054,
      "grad_norm": 2.270536184310913,
      "learning_rate": 0.0002152007802021552,
      "loss": 1.1544,
      "step": 1940
    },
    {
      "epoch": 1.153095906605586,
      "grad_norm": 0.9766626358032227,
      "learning_rate": 0.0002143285252139816,
      "loss": 1.2543,
      "step": 1950
    },
    {
      "epoch": 1.1590069454706664,
      "grad_norm": 1.4445466995239258,
      "learning_rate": 0.00021345359654835705,
      "loss": 0.8612,
      "step": 1960
    },
    {
      "epoch": 1.1649179843357471,
      "grad_norm": 0.4692048728466034,
      "learning_rate": 0.00021257603056981867,
      "loss": 0.9292,
      "step": 1970
    },
    {
      "epoch": 1.1708290232008276,
      "grad_norm": 1.3267546892166138,
      "learning_rate": 0.00021169586375251786,
      "loss": 0.9749,
      "step": 1980
    },
    {
      "epoch": 1.176740062065908,
      "grad_norm": 2.033278226852417,
      "learning_rate": 0.00021081313267870436,
      "loss": 0.7921,
      "step": 1990
    },
    {
      "epoch": 1.1826511009309886,
      "grad_norm": 1.5818259716033936,
      "learning_rate": 0.00020992787403720565,
      "loss": 0.8481,
      "step": 2000
    },
    {
      "epoch": 1.1826511009309886,
      "eval_loss": 0.8974140882492065,
      "eval_runtime": 217.5682,
      "eval_samples_per_second": 6.913,
      "eval_steps_per_second": 0.864,
      "step": 2000
    },
    {
      "epoch": 1.188562139796069,
      "grad_norm": 1.1696337461471558,
      "learning_rate": 0.00020904012462190223,
      "loss": 0.6901,
      "step": 2010
    },
    {
      "epoch": 1.1944731786611498,
      "grad_norm": 1.1192651987075806,
      "learning_rate": 0.00020814992133019833,
      "loss": 0.7199,
      "step": 2020
    },
    {
      "epoch": 1.2003842175262303,
      "grad_norm": 1.3145755529403687,
      "learning_rate": 0.00020725730116148813,
      "loss": 0.9976,
      "step": 2030
    },
    {
      "epoch": 1.2062952563913107,
      "grad_norm": 1.2743269205093384,
      "learning_rate": 0.00020636230121561826,
      "loss": 1.0017,
      "step": 2040
    },
    {
      "epoch": 1.2122062952563912,
      "grad_norm": 1.6741915941238403,
      "learning_rate": 0.00020546495869134567,
      "loss": 0.6079,
      "step": 2050
    },
    {
      "epoch": 1.218117334121472,
      "grad_norm": 1.3378978967666626,
      "learning_rate": 0.0002045653108847916,
      "loss": 1.4589,
      "step": 2060
    },
    {
      "epoch": 1.2240283729865524,
      "grad_norm": 0.26088035106658936,
      "learning_rate": 0.00020366339518789129,
      "loss": 0.7271,
      "step": 2070
    },
    {
      "epoch": 1.229939411851633,
      "grad_norm": 1.3203980922698975,
      "learning_rate": 0.00020275924908684016,
      "loss": 0.8797,
      "step": 2080
    },
    {
      "epoch": 1.2358504507167134,
      "grad_norm": 0.6673275828361511,
      "learning_rate": 0.00020185291016053543,
      "loss": 1.0189,
      "step": 2090
    },
    {
      "epoch": 1.241761489581794,
      "grad_norm": 0.8586916327476501,
      "learning_rate": 0.00020094441607901458,
      "loss": 0.7789,
      "step": 2100
    },
    {
      "epoch": 1.2476725284468746,
      "grad_norm": 2.029791831970215,
      "learning_rate": 0.0002000338046018893,
      "loss": 0.7946,
      "step": 2110
    },
    {
      "epoch": 1.253583567311955,
      "grad_norm": 0.7835213541984558,
      "learning_rate": 0.00019912111357677653,
      "loss": 0.8786,
      "step": 2120
    },
    {
      "epoch": 1.2594946061770356,
      "grad_norm": 0.6661372184753418,
      "learning_rate": 0.00019820638093772495,
      "loss": 0.8794,
      "step": 2130
    },
    {
      "epoch": 1.265405645042116,
      "grad_norm": 1.2334216833114624,
      "learning_rate": 0.00019728964470363874,
      "loss": 0.7943,
      "step": 2140
    },
    {
      "epoch": 1.2713166839071968,
      "grad_norm": 0.8281213045120239,
      "learning_rate": 0.00019637094297669707,
      "loss": 0.5902,
      "step": 2150
    },
    {
      "epoch": 1.2772277227722773,
      "grad_norm": 1.3270446062088013,
      "learning_rate": 0.00019545031394077074,
      "loss": 0.7131,
      "step": 2160
    },
    {
      "epoch": 1.2831387616373577,
      "grad_norm": 0.46519675850868225,
      "learning_rate": 0.00019452779585983488,
      "loss": 0.8927,
      "step": 2170
    },
    {
      "epoch": 1.2890498005024382,
      "grad_norm": 1.7822357416152954,
      "learning_rate": 0.00019360342707637894,
      "loss": 0.8443,
      "step": 2180
    },
    {
      "epoch": 1.2949608393675187,
      "grad_norm": 0.5788727402687073,
      "learning_rate": 0.00019267724600981255,
      "loss": 0.7671,
      "step": 2190
    },
    {
      "epoch": 1.3008718782325994,
      "grad_norm": 0.3792121410369873,
      "learning_rate": 0.00019174929115486935,
      "loss": 0.6951,
      "step": 2200
    },
    {
      "epoch": 1.30678291709768,
      "grad_norm": 2.5573198795318604,
      "learning_rate": 0.00019081960108000644,
      "loss": 0.8488,
      "step": 2210
    },
    {
      "epoch": 1.3126939559627604,
      "grad_norm": 2.5192341804504395,
      "learning_rate": 0.00018988821442580178,
      "loss": 1.0607,
      "step": 2220
    },
    {
      "epoch": 1.318604994827841,
      "grad_norm": 1.1603440046310425,
      "learning_rate": 0.0001889551699033479,
      "loss": 0.9304,
      "step": 2230
    },
    {
      "epoch": 1.3245160336929216,
      "grad_norm": 0.9393249750137329,
      "learning_rate": 0.00018802050629264318,
      "loss": 1.0186,
      "step": 2240
    },
    {
      "epoch": 1.330427072558002,
      "grad_norm": 0.9996641278266907,
      "learning_rate": 0.00018708426244097985,
      "loss": 0.7664,
      "step": 2250
    },
    {
      "epoch": 1.3363381114230826,
      "grad_norm": 0.45620909333229065,
      "learning_rate": 0.00018614647726132954,
      "loss": 0.5333,
      "step": 2260
    },
    {
      "epoch": 1.342249150288163,
      "grad_norm": 2.4257633686065674,
      "learning_rate": 0.00018520718973072582,
      "loss": 0.7354,
      "step": 2270
    },
    {
      "epoch": 1.3481601891532438,
      "grad_norm": 0.6289002299308777,
      "learning_rate": 0.00018426643888864435,
      "loss": 1.2914,
      "step": 2280
    },
    {
      "epoch": 1.3540712280183242,
      "grad_norm": 0.8691869378089905,
      "learning_rate": 0.00018332426383538,
      "loss": 0.7397,
      "step": 2290
    },
    {
      "epoch": 1.3599822668834047,
      "grad_norm": 1.9503474235534668,
      "learning_rate": 0.00018238070373042224,
      "loss": 0.6551,
      "step": 2300
    },
    {
      "epoch": 1.3658933057484852,
      "grad_norm": 0.5521931052207947,
      "learning_rate": 0.000181435797790827,
      "loss": 0.8639,
      "step": 2310
    },
    {
      "epoch": 1.3718043446135657,
      "grad_norm": 0.77325439453125,
      "learning_rate": 0.00018048958528958705,
      "loss": 0.819,
      "step": 2320
    },
    {
      "epoch": 1.3777153834786464,
      "grad_norm": 1.0857123136520386,
      "learning_rate": 0.00017954210555399952,
      "loss": 0.7828,
      "step": 2330
    },
    {
      "epoch": 1.383626422343727,
      "grad_norm": 0.6124300956726074,
      "learning_rate": 0.00017859339796403152,
      "loss": 0.9829,
      "step": 2340
    },
    {
      "epoch": 1.3895374612088074,
      "grad_norm": 0.7765011787414551,
      "learning_rate": 0.0001776435019506832,
      "loss": 0.8812,
      "step": 2350
    },
    {
      "epoch": 1.395448500073888,
      "grad_norm": 1.100540280342102,
      "learning_rate": 0.00017669245699434915,
      "loss": 0.7633,
      "step": 2360
    },
    {
      "epoch": 1.4013595389389686,
      "grad_norm": 0.35430842638015747,
      "learning_rate": 0.0001757403026231771,
      "loss": 0.69,
      "step": 2370
    },
    {
      "epoch": 1.407270577804049,
      "grad_norm": 1.6914312839508057,
      "learning_rate": 0.0001747870784114254,
      "loss": 0.8889,
      "step": 2380
    },
    {
      "epoch": 1.4131816166691296,
      "grad_norm": 1.7468451261520386,
      "learning_rate": 0.00017383282397781793,
      "loss": 1.1388,
      "step": 2390
    },
    {
      "epoch": 1.41909265553421,
      "grad_norm": 1.1198307275772095,
      "learning_rate": 0.00017287757898389755,
      "loss": 0.9663,
      "step": 2400
    },
    {
      "epoch": 1.4250036943992908,
      "grad_norm": 1.2962219715118408,
      "learning_rate": 0.00017192138313237772,
      "loss": 0.929,
      "step": 2410
    },
    {
      "epoch": 1.4309147332643712,
      "grad_norm": 1.720551609992981,
      "learning_rate": 0.0001709642761654922,
      "loss": 0.9215,
      "step": 2420
    },
    {
      "epoch": 1.4368257721294517,
      "grad_norm": 1.288575530052185,
      "learning_rate": 0.00017000629786334334,
      "loss": 0.9216,
      "step": 2430
    },
    {
      "epoch": 1.4427368109945322,
      "grad_norm": 1.1617252826690674,
      "learning_rate": 0.0001690474880422486,
      "loss": 0.8377,
      "step": 2440
    },
    {
      "epoch": 1.4486478498596127,
      "grad_norm": 0.4232481122016907,
      "learning_rate": 0.00016808788655308578,
      "loss": 0.9058,
      "step": 2450
    },
    {
      "epoch": 1.4545588887246934,
      "grad_norm": 1.302380084991455,
      "learning_rate": 0.00016712753327963677,
      "loss": 0.9815,
      "step": 2460
    },
    {
      "epoch": 1.460469927589774,
      "grad_norm": 1.0714366436004639,
      "learning_rate": 0.00016616646813692962,
      "loss": 0.5586,
      "step": 2470
    },
    {
      "epoch": 1.4663809664548544,
      "grad_norm": 2.0691139698028564,
      "learning_rate": 0.00016520473106957978,
      "loss": 1.2145,
      "step": 2480
    },
    {
      "epoch": 1.472292005319935,
      "grad_norm": 0.8605973720550537,
      "learning_rate": 0.0001642423620501298,
      "loss": 0.6989,
      "step": 2490
    },
    {
      "epoch": 1.4782030441850156,
      "grad_norm": 0.9338710904121399,
      "learning_rate": 0.00016327940107738792,
      "loss": 0.7046,
      "step": 2500
    },
    {
      "epoch": 1.4782030441850156,
      "eval_loss": 0.8510778546333313,
      "eval_runtime": 217.5014,
      "eval_samples_per_second": 6.915,
      "eval_steps_per_second": 0.864,
      "step": 2500
    },
    {
      "epoch": 1.484114083050096,
      "grad_norm": 0.6322833299636841,
      "learning_rate": 0.0001623158881747657,
      "loss": 1.0393,
      "step": 2510
    },
    {
      "epoch": 1.4900251219151766,
      "grad_norm": 0.774755597114563,
      "learning_rate": 0.00016135186338861452,
      "loss": 0.7193,
      "step": 2520
    },
    {
      "epoch": 1.495936160780257,
      "grad_norm": 1.1852223873138428,
      "learning_rate": 0.000160387366786561,
      "loss": 0.8502,
      "step": 2530
    },
    {
      "epoch": 1.5018471996453377,
      "grad_norm": 1.6821094751358032,
      "learning_rate": 0.00015942243845584186,
      "loss": 1.0392,
      "step": 2540
    },
    {
      "epoch": 1.5077582385104182,
      "grad_norm": 1.7408852577209473,
      "learning_rate": 0.00015845711850163768,
      "loss": 1.0879,
      "step": 2550
    },
    {
      "epoch": 1.5136692773754987,
      "grad_norm": 1.2541487216949463,
      "learning_rate": 0.00015749144704540596,
      "loss": 0.5786,
      "step": 2560
    },
    {
      "epoch": 1.5195803162405794,
      "grad_norm": 2.714712381362915,
      "learning_rate": 0.0001565254642232138,
      "loss": 0.9306,
      "step": 2570
    },
    {
      "epoch": 1.5254913551056597,
      "grad_norm": 2.09464693069458,
      "learning_rate": 0.0001555592101840694,
      "loss": 1.1174,
      "step": 2580
    },
    {
      "epoch": 1.5314023939707404,
      "grad_norm": 1.0840226411819458,
      "learning_rate": 0.00015459272508825357,
      "loss": 0.7217,
      "step": 2590
    },
    {
      "epoch": 1.537313432835821,
      "grad_norm": 1.141671895980835,
      "learning_rate": 0.00015362604910565045,
      "loss": 1.0643,
      "step": 2600
    },
    {
      "epoch": 1.5432244717009014,
      "grad_norm": 0.6159616708755493,
      "learning_rate": 0.00015265922241407807,
      "loss": 0.7584,
      "step": 2610
    },
    {
      "epoch": 1.549135510565982,
      "grad_norm": 0.7701154351234436,
      "learning_rate": 0.00015169228519761827,
      "loss": 0.6287,
      "step": 2620
    },
    {
      "epoch": 1.5550465494310624,
      "grad_norm": 0.6685971617698669,
      "learning_rate": 0.0001507252776449467,
      "loss": 0.8382,
      "step": 2630
    },
    {
      "epoch": 1.560957588296143,
      "grad_norm": 0.4607510268688202,
      "learning_rate": 0.0001497582399476623,
      "loss": 0.7668,
      "step": 2640
    },
    {
      "epoch": 1.5668686271612235,
      "grad_norm": 0.687637448310852,
      "learning_rate": 0.000148791212298617,
      "loss": 0.8708,
      "step": 2650
    },
    {
      "epoch": 1.572779666026304,
      "grad_norm": 0.8656360507011414,
      "learning_rate": 0.00014782423489024502,
      "loss": 0.9299,
      "step": 2660
    },
    {
      "epoch": 1.5786907048913847,
      "grad_norm": 2.1467208862304688,
      "learning_rate": 0.00014685734791289247,
      "loss": 0.7076,
      "step": 2670
    },
    {
      "epoch": 1.5846017437564652,
      "grad_norm": 0.7212432026863098,
      "learning_rate": 0.00014589059155314685,
      "loss": 0.5995,
      "step": 2680
    },
    {
      "epoch": 1.5905127826215457,
      "grad_norm": 1.1517289876937866,
      "learning_rate": 0.00014492400599216686,
      "loss": 1.006,
      "step": 2690
    },
    {
      "epoch": 1.5964238214866264,
      "grad_norm": 2.463197708129883,
      "learning_rate": 0.00014395763140401228,
      "loss": 0.7759,
      "step": 2700
    },
    {
      "epoch": 1.6023348603517067,
      "grad_norm": 0.45252174139022827,
      "learning_rate": 0.0001429915079539743,
      "loss": 0.6464,
      "step": 2710
    },
    {
      "epoch": 1.6082458992167874,
      "grad_norm": 0.2781231999397278,
      "learning_rate": 0.00014202567579690606,
      "loss": 0.8268,
      "step": 2720
    },
    {
      "epoch": 1.6141569380818679,
      "grad_norm": 0.5327668190002441,
      "learning_rate": 0.00014106017507555375,
      "loss": 0.8026,
      "step": 2730
    },
    {
      "epoch": 1.6200679769469484,
      "grad_norm": 1.871917486190796,
      "learning_rate": 0.00014009504591888815,
      "loss": 0.617,
      "step": 2740
    },
    {
      "epoch": 1.625979015812029,
      "grad_norm": 0.8970045447349548,
      "learning_rate": 0.00013913032844043664,
      "loss": 0.7872,
      "step": 2750
    },
    {
      "epoch": 1.6318900546771093,
      "grad_norm": 0.718917965888977,
      "learning_rate": 0.00013816606273661625,
      "loss": 0.9034,
      "step": 2760
    },
    {
      "epoch": 1.63780109354219,
      "grad_norm": 0.7239982485771179,
      "learning_rate": 0.00013720228888506694,
      "loss": 0.9084,
      "step": 2770
    },
    {
      "epoch": 1.6437121324072705,
      "grad_norm": 0.9206106066703796,
      "learning_rate": 0.0001362390469429857,
      "loss": 0.8409,
      "step": 2780
    },
    {
      "epoch": 1.649623171272351,
      "grad_norm": 2.0410103797912598,
      "learning_rate": 0.00013527637694546205,
      "loss": 1.0262,
      "step": 2790
    },
    {
      "epoch": 1.6555342101374317,
      "grad_norm": 0.39525625109672546,
      "learning_rate": 0.00013431431890381377,
      "loss": 0.6851,
      "step": 2800
    },
    {
      "epoch": 1.6614452490025122,
      "grad_norm": 1.993753433227539,
      "learning_rate": 0.00013335291280392397,
      "loss": 0.7088,
      "step": 2810
    },
    {
      "epoch": 1.6673562878675927,
      "grad_norm": 0.8663133978843689,
      "learning_rate": 0.0001323921986045793,
      "loss": 0.9115,
      "step": 2820
    },
    {
      "epoch": 1.6732673267326734,
      "grad_norm": 0.9623103141784668,
      "learning_rate": 0.0001314322162358089,
      "loss": 0.7718,
      "step": 2830
    },
    {
      "epoch": 1.6791783655977537,
      "grad_norm": 1.1349341869354248,
      "learning_rate": 0.00013047300559722509,
      "loss": 0.8825,
      "step": 2840
    },
    {
      "epoch": 1.6850894044628344,
      "grad_norm": 0.7343268394470215,
      "learning_rate": 0.00012951460655636474,
      "loss": 0.711,
      "step": 2850
    },
    {
      "epoch": 1.6910004433279149,
      "grad_norm": 1.0986992120742798,
      "learning_rate": 0.0001285570589470325,
      "loss": 0.7903,
      "step": 2860
    },
    {
      "epoch": 1.6969114821929954,
      "grad_norm": 1.015511393547058,
      "learning_rate": 0.00012760040256764508,
      "loss": 0.9669,
      "step": 2870
    },
    {
      "epoch": 1.702822521058076,
      "grad_norm": 1.141070008277893,
      "learning_rate": 0.00012664467717957715,
      "loss": 1.0899,
      "step": 2880
    },
    {
      "epoch": 1.7087335599231563,
      "grad_norm": 0.8409292101860046,
      "learning_rate": 0.00012568992250550854,
      "loss": 0.833,
      "step": 2890
    },
    {
      "epoch": 1.714644598788237,
      "grad_norm": 0.6757439970970154,
      "learning_rate": 0.00012473617822777368,
      "loss": 0.5487,
      "step": 2900
    },
    {
      "epoch": 1.7205556376533175,
      "grad_norm": 0.635202169418335,
      "learning_rate": 0.0001237834839867118,
      "loss": 0.6931,
      "step": 2910
    },
    {
      "epoch": 1.726466676518398,
      "grad_norm": 0.9030903577804565,
      "learning_rate": 0.00012283187937901986,
      "loss": 0.6621,
      "step": 2920
    },
    {
      "epoch": 1.7323777153834787,
      "grad_norm": 1.6328026056289673,
      "learning_rate": 0.0001218814039561063,
      "loss": 1.0221,
      "step": 2930
    },
    {
      "epoch": 1.7382887542485592,
      "grad_norm": 1.6400623321533203,
      "learning_rate": 0.00012093209722244757,
      "loss": 0.8625,
      "step": 2940
    },
    {
      "epoch": 1.7441997931136397,
      "grad_norm": 0.993628740310669,
      "learning_rate": 0.00011998399863394596,
      "loss": 0.8102,
      "step": 2950
    },
    {
      "epoch": 1.7501108319787204,
      "grad_norm": 1.7681617736816406,
      "learning_rate": 0.00011903714759628982,
      "loss": 0.5145,
      "step": 2960
    },
    {
      "epoch": 1.7560218708438007,
      "grad_norm": 2.1477651596069336,
      "learning_rate": 0.0001180915834633158,
      "loss": 0.5937,
      "step": 2970
    },
    {
      "epoch": 1.7619329097088814,
      "grad_norm": 0.7229859828948975,
      "learning_rate": 0.00011714734553537304,
      "loss": 0.8139,
      "step": 2980
    },
    {
      "epoch": 1.7678439485739619,
      "grad_norm": 0.4363662600517273,
      "learning_rate": 0.00011620447305768973,
      "loss": 0.8964,
      "step": 2990
    },
    {
      "epoch": 1.7737549874390424,
      "grad_norm": 1.1696609258651733,
      "learning_rate": 0.00011526300521874222,
      "loss": 0.9399,
      "step": 3000
    },
    {
      "epoch": 1.7737549874390424,
      "eval_loss": 0.8171266317367554,
      "eval_runtime": 217.3911,
      "eval_samples_per_second": 6.918,
      "eval_steps_per_second": 0.865,
      "step": 3000
    },
    {
      "epoch": 1.779666026304123,
      "grad_norm": 1.0508556365966797,
      "learning_rate": 0.00011432298114862588,
      "loss": 1.0285,
      "step": 3010
    },
    {
      "epoch": 1.7855770651692033,
      "grad_norm": 0.9585235714912415,
      "learning_rate": 0.00011338443991742907,
      "loss": 0.6292,
      "step": 3020
    },
    {
      "epoch": 1.791488104034284,
      "grad_norm": 0.6775657534599304,
      "learning_rate": 0.00011244742053360896,
      "loss": 1.0955,
      "step": 3030
    },
    {
      "epoch": 1.7973991428993645,
      "grad_norm": 3.4950592517852783,
      "learning_rate": 0.00011151196194237056,
      "loss": 0.6661,
      "step": 3040
    },
    {
      "epoch": 1.803310181764445,
      "grad_norm": 0.4340488910675049,
      "learning_rate": 0.00011057810302404781,
      "loss": 0.7905,
      "step": 3050
    },
    {
      "epoch": 1.8092212206295257,
      "grad_norm": 0.6585454940795898,
      "learning_rate": 0.0001096458825924876,
      "loss": 0.5133,
      "step": 3060
    },
    {
      "epoch": 1.8151322594946062,
      "grad_norm": 0.636106014251709,
      "learning_rate": 0.0001087153393934367,
      "loss": 0.5344,
      "step": 3070
    },
    {
      "epoch": 1.8210432983596867,
      "grad_norm": 1.1860429048538208,
      "learning_rate": 0.00010778651210293136,
      "loss": 0.574,
      "step": 3080
    },
    {
      "epoch": 1.8269543372247674,
      "grad_norm": 1.218689203262329,
      "learning_rate": 0.00010685943932568961,
      "loss": 0.702,
      "step": 3090
    },
    {
      "epoch": 1.8328653760898477,
      "grad_norm": 0.5169245004653931,
      "learning_rate": 0.00010593415959350705,
      "loss": 0.6415,
      "step": 3100
    },
    {
      "epoch": 1.8387764149549284,
      "grad_norm": 0.9458039402961731,
      "learning_rate": 0.00010501071136365505,
      "loss": 0.6304,
      "step": 3110
    },
    {
      "epoch": 1.8446874538200089,
      "grad_norm": 1.14734947681427,
      "learning_rate": 0.0001040891330172826,
      "loss": 1.104,
      "step": 3120
    },
    {
      "epoch": 1.8505984926850894,
      "grad_norm": 1.360620141029358,
      "learning_rate": 0.00010316946285782101,
      "loss": 0.8799,
      "step": 3130
    },
    {
      "epoch": 1.85650953155017,
      "grad_norm": 0.706331729888916,
      "learning_rate": 0.00010225173910939178,
      "loss": 0.7152,
      "step": 3140
    },
    {
      "epoch": 1.8624205704152503,
      "grad_norm": 0.6349744200706482,
      "learning_rate": 0.00010133599991521815,
      "loss": 0.8997,
      "step": 3150
    },
    {
      "epoch": 1.868331609280331,
      "grad_norm": 0.7707855701446533,
      "learning_rate": 0.0001004222833360394,
      "loss": 1.2446,
      "step": 3160
    },
    {
      "epoch": 1.8742426481454115,
      "grad_norm": 1.0410871505737305,
      "learning_rate": 9.95106273485294e-05,
      "loss": 0.995,
      "step": 3170
    },
    {
      "epoch": 1.880153687010492,
      "grad_norm": 0.4975525438785553,
      "learning_rate": 9.860106984371767e-05,
      "loss": 0.6962,
      "step": 3180
    },
    {
      "epoch": 1.8860647258755727,
      "grad_norm": 1.0908316373825073,
      "learning_rate": 9.769364862541506e-05,
      "loss": 0.8063,
      "step": 3190
    },
    {
      "epoch": 1.8919757647406532,
      "grad_norm": 1.0335760116577148,
      "learning_rate": 9.678840140864204e-05,
      "loss": 0.9933,
      "step": 3200
    },
    {
      "epoch": 1.8978868036057337,
      "grad_norm": 1.4128656387329102,
      "learning_rate": 9.588536581806148e-05,
      "loss": 0.9878,
      "step": 3210
    },
    {
      "epoch": 1.9037978424708144,
      "grad_norm": 0.8027359247207642,
      "learning_rate": 9.498457938641464e-05,
      "loss": 0.627,
      "step": 3220
    },
    {
      "epoch": 1.9097088813358947,
      "grad_norm": 0.5811389684677124,
      "learning_rate": 9.408607955296143e-05,
      "loss": 0.6544,
      "step": 3230
    },
    {
      "epoch": 1.9156199202009754,
      "grad_norm": 1.5861387252807617,
      "learning_rate": 9.318990366192404e-05,
      "loss": 0.9301,
      "step": 3240
    },
    {
      "epoch": 1.9215309590660559,
      "grad_norm": 1.320826530456543,
      "learning_rate": 9.229608896093513e-05,
      "loss": 0.5712,
      "step": 3250
    },
    {
      "epoch": 1.9274419979311364,
      "grad_norm": 0.7956535816192627,
      "learning_rate": 9.140467259948942e-05,
      "loss": 0.9261,
      "step": 3260
    },
    {
      "epoch": 1.933353036796217,
      "grad_norm": 1.5749399662017822,
      "learning_rate": 9.051569162739987e-05,
      "loss": 0.7307,
      "step": 3270
    },
    {
      "epoch": 1.9392640756612973,
      "grad_norm": 1.49986732006073,
      "learning_rate": 8.962918299325764e-05,
      "loss": 0.8151,
      "step": 3280
    },
    {
      "epoch": 1.945175114526378,
      "grad_norm": 1.005713939666748,
      "learning_rate": 8.874518354289644e-05,
      "loss": 0.839,
      "step": 3290
    },
    {
      "epoch": 1.9510861533914585,
      "grad_norm": 1.1819324493408203,
      "learning_rate": 8.786373001786126e-05,
      "loss": 0.8009,
      "step": 3300
    },
    {
      "epoch": 1.956997192256539,
      "grad_norm": 0.8903324007987976,
      "learning_rate": 8.6984859053881e-05,
      "loss": 0.7293,
      "step": 3310
    },
    {
      "epoch": 1.9629082311216197,
      "grad_norm": 0.7952669262886047,
      "learning_rate": 8.610860717934605e-05,
      "loss": 0.7742,
      "step": 3320
    },
    {
      "epoch": 1.9688192699867002,
      "grad_norm": 2.3274848461151123,
      "learning_rate": 8.523501081378982e-05,
      "loss": 1.1077,
      "step": 3330
    },
    {
      "epoch": 1.9747303088517807,
      "grad_norm": 1.7487236261367798,
      "learning_rate": 8.436410626637529e-05,
      "loss": 0.655,
      "step": 3340
    },
    {
      "epoch": 1.9806413477168614,
      "grad_norm": 0.6048775315284729,
      "learning_rate": 8.349592973438572e-05,
      "loss": 0.6855,
      "step": 3350
    },
    {
      "epoch": 1.9865523865819417,
      "grad_norm": 0.9215231537818909,
      "learning_rate": 8.263051730172034e-05,
      "loss": 0.5115,
      "step": 3360
    },
    {
      "epoch": 1.9924634254470224,
      "grad_norm": 0.6528447270393372,
      "learning_rate": 8.176790493739437e-05,
      "loss": 0.5624,
      "step": 3370
    },
    {
      "epoch": 1.9983744643121029,
      "grad_norm": 0.6040571331977844,
      "learning_rate": 8.090812849404423e-05,
      "loss": 0.8423,
      "step": 3380
    }
  ],
  "logging_steps": 10,
  "max_steps": 5073,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.271612096964198e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
