{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.998448237641321,
  "eval_steps": 500,
  "global_step": 5073,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005911475652109658,
      "grad_norm": 0.21855121850967407,
      "learning_rate": 5.399999999999999e-06,
      "loss": 3.9963,
      "step": 10
    },
    {
      "epoch": 0.011822951304219316,
      "grad_norm": 0.2550734579563141,
      "learning_rate": 1.14e-05,
      "loss": 4.0407,
      "step": 20
    },
    {
      "epoch": 0.017734426956328972,
      "grad_norm": 0.2610781788825989,
      "learning_rate": 1.74e-05,
      "loss": 4.3119,
      "step": 30
    },
    {
      "epoch": 0.023645902608438633,
      "grad_norm": 0.3536328077316284,
      "learning_rate": 2.34e-05,
      "loss": 4.0697,
      "step": 40
    },
    {
      "epoch": 0.02955737826054829,
      "grad_norm": 0.3950863778591156,
      "learning_rate": 2.94e-05,
      "loss": 3.9123,
      "step": 50
    },
    {
      "epoch": 0.035468853912657944,
      "grad_norm": 0.3810991048812866,
      "learning_rate": 3.539999999999999e-05,
      "loss": 3.8938,
      "step": 60
    },
    {
      "epoch": 0.04138032956476761,
      "grad_norm": 0.4553346335887909,
      "learning_rate": 4.14e-05,
      "loss": 4.0629,
      "step": 70
    },
    {
      "epoch": 0.047291805216877265,
      "grad_norm": 0.4710727632045746,
      "learning_rate": 4.7399999999999993e-05,
      "loss": 3.9297,
      "step": 80
    },
    {
      "epoch": 0.05320328086898692,
      "grad_norm": 0.5903026461601257,
      "learning_rate": 5.339999999999999e-05,
      "loss": 3.9447,
      "step": 90
    },
    {
      "epoch": 0.05911475652109658,
      "grad_norm": 0.4978746175765991,
      "learning_rate": 5.94e-05,
      "loss": 3.6104,
      "step": 100
    },
    {
      "epoch": 0.06502623217320623,
      "grad_norm": 0.4170621335506439,
      "learning_rate": 6.539999999999999e-05,
      "loss": 3.6523,
      "step": 110
    },
    {
      "epoch": 0.07093770782531589,
      "grad_norm": 0.6009400486946106,
      "learning_rate": 7.139999999999999e-05,
      "loss": 3.2434,
      "step": 120
    },
    {
      "epoch": 0.07684918347742556,
      "grad_norm": 0.6476964950561523,
      "learning_rate": 7.74e-05,
      "loss": 3.1342,
      "step": 130
    },
    {
      "epoch": 0.08276065912953522,
      "grad_norm": 0.8915241360664368,
      "learning_rate": 8.34e-05,
      "loss": 2.8407,
      "step": 140
    },
    {
      "epoch": 0.08867213478164487,
      "grad_norm": 1.5597630739212036,
      "learning_rate": 8.939999999999999e-05,
      "loss": 2.8,
      "step": 150
    },
    {
      "epoch": 0.09458361043375453,
      "grad_norm": 1.3148115873336792,
      "learning_rate": 9.539999999999999e-05,
      "loss": 3.0285,
      "step": 160
    },
    {
      "epoch": 0.10049508608586419,
      "grad_norm": 1.0388435125350952,
      "learning_rate": 0.0001014,
      "loss": 2.8272,
      "step": 170
    },
    {
      "epoch": 0.10640656173797385,
      "grad_norm": 1.1761771440505981,
      "learning_rate": 0.00010739999999999998,
      "loss": 2.2767,
      "step": 180
    },
    {
      "epoch": 0.1123180373900835,
      "grad_norm": 1.777435302734375,
      "learning_rate": 0.00011339999999999999,
      "loss": 2.4021,
      "step": 190
    },
    {
      "epoch": 0.11822951304219316,
      "grad_norm": 1.4255152940750122,
      "learning_rate": 0.0001194,
      "loss": 2.0731,
      "step": 200
    },
    {
      "epoch": 0.12414098869430282,
      "grad_norm": 1.954053282737732,
      "learning_rate": 0.00012539999999999999,
      "loss": 2.0045,
      "step": 210
    },
    {
      "epoch": 0.13005246434641246,
      "grad_norm": 1.1837152242660522,
      "learning_rate": 0.0001314,
      "loss": 2.1915,
      "step": 220
    },
    {
      "epoch": 0.13596393999852213,
      "grad_norm": 1.5257353782653809,
      "learning_rate": 0.0001374,
      "loss": 1.9796,
      "step": 230
    },
    {
      "epoch": 0.14187541565063178,
      "grad_norm": 1.2506072521209717,
      "learning_rate": 0.0001434,
      "loss": 1.4699,
      "step": 240
    },
    {
      "epoch": 0.14778689130274145,
      "grad_norm": 1.1530261039733887,
      "learning_rate": 0.0001494,
      "loss": 2.0078,
      "step": 250
    },
    {
      "epoch": 0.15369836695485112,
      "grad_norm": 2.1291584968566895,
      "learning_rate": 0.00015539999999999998,
      "loss": 1.8466,
      "step": 260
    },
    {
      "epoch": 0.15960984260696076,
      "grad_norm": 0.9238948225975037,
      "learning_rate": 0.0001614,
      "loss": 1.6766,
      "step": 270
    },
    {
      "epoch": 0.16552131825907043,
      "grad_norm": 0.8083561062812805,
      "learning_rate": 0.0001674,
      "loss": 1.2842,
      "step": 280
    },
    {
      "epoch": 0.17143279391118008,
      "grad_norm": 0.733826756477356,
      "learning_rate": 0.00017339999999999996,
      "loss": 1.4093,
      "step": 290
    },
    {
      "epoch": 0.17734426956328975,
      "grad_norm": 0.7897816300392151,
      "learning_rate": 0.00017939999999999997,
      "loss": 1.3763,
      "step": 300
    },
    {
      "epoch": 0.1832557452153994,
      "grad_norm": 2.1355347633361816,
      "learning_rate": 0.00018539999999999998,
      "loss": 2.084,
      "step": 310
    },
    {
      "epoch": 0.18916722086750906,
      "grad_norm": 0.9081681966781616,
      "learning_rate": 0.0001914,
      "loss": 1.5642,
      "step": 320
    },
    {
      "epoch": 0.1950786965196187,
      "grad_norm": 2.0413544178009033,
      "learning_rate": 0.0001974,
      "loss": 1.3998,
      "step": 330
    },
    {
      "epoch": 0.20099017217172838,
      "grad_norm": 1.1476536989212036,
      "learning_rate": 0.00020339999999999998,
      "loss": 1.3881,
      "step": 340
    },
    {
      "epoch": 0.20690164782383802,
      "grad_norm": 0.4750850796699524,
      "learning_rate": 0.00020939999999999997,
      "loss": 1.2549,
      "step": 350
    },
    {
      "epoch": 0.2128131234759477,
      "grad_norm": 2.610386610031128,
      "learning_rate": 0.00021539999999999998,
      "loss": 1.9993,
      "step": 360
    },
    {
      "epoch": 0.21872459912805733,
      "grad_norm": 1.2886751890182495,
      "learning_rate": 0.0002214,
      "loss": 1.803,
      "step": 370
    },
    {
      "epoch": 0.224636074780167,
      "grad_norm": 1.0942013263702393,
      "learning_rate": 0.00022739999999999997,
      "loss": 1.5721,
      "step": 380
    },
    {
      "epoch": 0.23054755043227665,
      "grad_norm": 0.6497766971588135,
      "learning_rate": 0.00023339999999999998,
      "loss": 1.2261,
      "step": 390
    },
    {
      "epoch": 0.23645902608438632,
      "grad_norm": 1.514673113822937,
      "learning_rate": 0.0002394,
      "loss": 1.8922,
      "step": 400
    },
    {
      "epoch": 0.24237050173649596,
      "grad_norm": 0.9680010676383972,
      "learning_rate": 0.00024539999999999995,
      "loss": 1.6252,
      "step": 410
    },
    {
      "epoch": 0.24828197738860563,
      "grad_norm": 2.0751240253448486,
      "learning_rate": 0.0002514,
      "loss": 1.5214,
      "step": 420
    },
    {
      "epoch": 0.2541934530407153,
      "grad_norm": 1.8613463640213013,
      "learning_rate": 0.00025739999999999997,
      "loss": 1.7016,
      "step": 430
    },
    {
      "epoch": 0.2601049286928249,
      "grad_norm": 2.130777359008789,
      "learning_rate": 0.00026339999999999995,
      "loss": 1.4288,
      "step": 440
    },
    {
      "epoch": 0.2660164043449346,
      "grad_norm": 0.646378755569458,
      "learning_rate": 0.0002694,
      "loss": 1.5949,
      "step": 450
    },
    {
      "epoch": 0.27192787999704426,
      "grad_norm": 1.3120288848876953,
      "learning_rate": 0.00027539999999999997,
      "loss": 1.4833,
      "step": 460
    },
    {
      "epoch": 0.2778393556491539,
      "grad_norm": 1.125124216079712,
      "learning_rate": 0.00028139999999999996,
      "loss": 1.4545,
      "step": 470
    },
    {
      "epoch": 0.28375083130126355,
      "grad_norm": 2.4735374450683594,
      "learning_rate": 0.00028739999999999994,
      "loss": 1.5317,
      "step": 480
    },
    {
      "epoch": 0.28966230695337325,
      "grad_norm": 0.6327743530273438,
      "learning_rate": 0.0002934,
      "loss": 1.2499,
      "step": 490
    },
    {
      "epoch": 0.2955737826054829,
      "grad_norm": 1.6242458820343018,
      "learning_rate": 0.00029939999999999996,
      "loss": 1.467,
      "step": 500
    },
    {
      "epoch": 0.30148525825759254,
      "grad_norm": 0.7589235901832581,
      "learning_rate": 0.0002999971329039567,
      "loss": 1.4417,
      "step": 510
    },
    {
      "epoch": 0.30739673390970224,
      "grad_norm": 1.0759592056274414,
      "learning_rate": 0.00029998722209538536,
      "loss": 1.2355,
      "step": 520
    },
    {
      "epoch": 0.3133082095618119,
      "grad_norm": 0.9292750954627991,
      "learning_rate": 0.00029997023264566715,
      "loss": 1.1428,
      "step": 530
    },
    {
      "epoch": 0.3192196852139215,
      "grad_norm": 1.2630168199539185,
      "learning_rate": 0.00029994616535661874,
      "loss": 1.5303,
      "step": 540
    },
    {
      "epoch": 0.32513116086603117,
      "grad_norm": 1.0667715072631836,
      "learning_rate": 0.00029991502136409473,
      "loss": 1.4133,
      "step": 550
    },
    {
      "epoch": 0.33104263651814086,
      "grad_norm": 0.9384068846702576,
      "learning_rate": 0.00029987680213793466,
      "loss": 1.1268,
      "step": 560
    },
    {
      "epoch": 0.3369541121702505,
      "grad_norm": 1.5506012439727783,
      "learning_rate": 0.0002998315094818935,
      "loss": 1.889,
      "step": 570
    },
    {
      "epoch": 0.34286558782236015,
      "grad_norm": 0.7173452973365784,
      "learning_rate": 0.00029977914553355636,
      "loss": 1.6893,
      "step": 580
    },
    {
      "epoch": 0.3487770634744698,
      "grad_norm": 1.4104084968566895,
      "learning_rate": 0.00029971971276423776,
      "loss": 1.6408,
      "step": 590
    },
    {
      "epoch": 0.3546885391265795,
      "grad_norm": 1.3333358764648438,
      "learning_rate": 0.00029965321397886494,
      "loss": 1.2775,
      "step": 600
    },
    {
      "epoch": 0.36060001477868914,
      "grad_norm": 1.7931455373764038,
      "learning_rate": 0.00029957965231584556,
      "loss": 1.3147,
      "step": 610
    },
    {
      "epoch": 0.3665114904307988,
      "grad_norm": 0.8781871795654297,
      "learning_rate": 0.0002994990312469194,
      "loss": 1.4476,
      "step": 620
    },
    {
      "epoch": 0.3724229660829084,
      "grad_norm": 3.202131509780884,
      "learning_rate": 0.0002994113545769947,
      "loss": 1.8031,
      "step": 630
    },
    {
      "epoch": 0.3783344417350181,
      "grad_norm": 0.4886491000652313,
      "learning_rate": 0.00029931662644396847,
      "loss": 1.1976,
      "step": 640
    },
    {
      "epoch": 0.38424591738712777,
      "grad_norm": 0.808239758014679,
      "learning_rate": 0.0002992148513185314,
      "loss": 1.097,
      "step": 650
    },
    {
      "epoch": 0.3901573930392374,
      "grad_norm": 0.6215143203735352,
      "learning_rate": 0.0002991060340039565,
      "loss": 1.1641,
      "step": 660
    },
    {
      "epoch": 0.39606886869134705,
      "grad_norm": 1.4069135189056396,
      "learning_rate": 0.00029899017963587275,
      "loss": 1.6598,
      "step": 670
    },
    {
      "epoch": 0.40198034434345675,
      "grad_norm": 1.5653876066207886,
      "learning_rate": 0.00029886729368202257,
      "loss": 1.2168,
      "step": 680
    },
    {
      "epoch": 0.4078918199955664,
      "grad_norm": 2.5063676834106445,
      "learning_rate": 0.0002987373819420038,
      "loss": 1.9217,
      "step": 690
    },
    {
      "epoch": 0.41380329564767604,
      "grad_norm": 0.8011614680290222,
      "learning_rate": 0.000298600450546996,
      "loss": 1.3334,
      "step": 700
    },
    {
      "epoch": 0.4197147712997857,
      "grad_norm": 2.6214349269866943,
      "learning_rate": 0.00029845650595947105,
      "loss": 1.161,
      "step": 710
    },
    {
      "epoch": 0.4256262469518954,
      "grad_norm": 0.6010191440582275,
      "learning_rate": 0.0002983055549728882,
      "loss": 1.4484,
      "step": 720
    },
    {
      "epoch": 0.431537722604005,
      "grad_norm": 0.669968843460083,
      "learning_rate": 0.0002981476047113734,
      "loss": 0.8403,
      "step": 730
    },
    {
      "epoch": 0.43744919825611467,
      "grad_norm": 0.8820887207984924,
      "learning_rate": 0.0002979826626293832,
      "loss": 1.6598,
      "step": 740
    },
    {
      "epoch": 0.44336067390822437,
      "grad_norm": 0.729704737663269,
      "learning_rate": 0.0002978107365113527,
      "loss": 1.1679,
      "step": 750
    },
    {
      "epoch": 0.449272149560334,
      "grad_norm": 0.7598924040794373,
      "learning_rate": 0.00029763183447132843,
      "loss": 1.0783,
      "step": 760
    },
    {
      "epoch": 0.45518362521244365,
      "grad_norm": 1.3059768676757812,
      "learning_rate": 0.00029744596495258525,
      "loss": 1.5463,
      "step": 770
    },
    {
      "epoch": 0.4610951008645533,
      "grad_norm": 1.5226678848266602,
      "learning_rate": 0.0002972531367272279,
      "loss": 1.6517,
      "step": 780
    },
    {
      "epoch": 0.467006576516663,
      "grad_norm": 1.2538323402404785,
      "learning_rate": 0.000297053358895777,
      "loss": 1.3817,
      "step": 790
    },
    {
      "epoch": 0.47291805216877264,
      "grad_norm": 0.9093683958053589,
      "learning_rate": 0.00029684664088673964,
      "loss": 1.0216,
      "step": 800
    },
    {
      "epoch": 0.4788295278208823,
      "grad_norm": 0.9371103644371033,
      "learning_rate": 0.00029663299245616425,
      "loss": 1.4138,
      "step": 810
    },
    {
      "epoch": 0.4847410034729919,
      "grad_norm": 0.9414723515510559,
      "learning_rate": 0.0002964124236871802,
      "loss": 1.1547,
      "step": 820
    },
    {
      "epoch": 0.4906524791251016,
      "grad_norm": 1.3631341457366943,
      "learning_rate": 0.0002961849449895221,
      "loss": 1.1962,
      "step": 830
    },
    {
      "epoch": 0.49656395477721127,
      "grad_norm": 2.2169299125671387,
      "learning_rate": 0.0002959505670990381,
      "loss": 1.4767,
      "step": 840
    },
    {
      "epoch": 0.502475430429321,
      "grad_norm": 0.7052278518676758,
      "learning_rate": 0.00029570930107718387,
      "loss": 1.198,
      "step": 850
    },
    {
      "epoch": 0.5083869060814306,
      "grad_norm": 0.7167408466339111,
      "learning_rate": 0.0002954611583104999,
      "loss": 0.8587,
      "step": 860
    },
    {
      "epoch": 0.5142983817335403,
      "grad_norm": 1.4745219945907593,
      "learning_rate": 0.00029520615051007457,
      "loss": 0.8759,
      "step": 870
    },
    {
      "epoch": 0.5202098573856498,
      "grad_norm": 0.9305026531219482,
      "learning_rate": 0.00029494428971099104,
      "loss": 1.1192,
      "step": 880
    },
    {
      "epoch": 0.5261213330377595,
      "grad_norm": 1.6259422302246094,
      "learning_rate": 0.0002946755882717596,
      "loss": 1.4497,
      "step": 890
    },
    {
      "epoch": 0.5320328086898692,
      "grad_norm": 0.5704475045204163,
      "learning_rate": 0.0002944000588737343,
      "loss": 1.1544,
      "step": 900
    },
    {
      "epoch": 0.5379442843419788,
      "grad_norm": 1.0011250972747803,
      "learning_rate": 0.00029411771452051447,
      "loss": 1.0339,
      "step": 910
    },
    {
      "epoch": 0.5438557599940885,
      "grad_norm": 0.5349895358085632,
      "learning_rate": 0.0002938285685373309,
      "loss": 1.0725,
      "step": 920
    },
    {
      "epoch": 0.5497672356461982,
      "grad_norm": 0.780112624168396,
      "learning_rate": 0.0002935326345704171,
      "loss": 1.104,
      "step": 930
    },
    {
      "epoch": 0.5556787112983078,
      "grad_norm": 0.8973895311355591,
      "learning_rate": 0.00029322992658636525,
      "loss": 1.5861,
      "step": 940
    },
    {
      "epoch": 0.5615901869504175,
      "grad_norm": 2.284956455230713,
      "learning_rate": 0.0002929204588714668,
      "loss": 1.4961,
      "step": 950
    },
    {
      "epoch": 0.5675016626025271,
      "grad_norm": 1.720916748046875,
      "learning_rate": 0.00029260424603103877,
      "loss": 0.7762,
      "step": 960
    },
    {
      "epoch": 0.5734131382546368,
      "grad_norm": 3.2334513664245605,
      "learning_rate": 0.00029228130298873385,
      "loss": 1.4747,
      "step": 970
    },
    {
      "epoch": 0.5793246139067465,
      "grad_norm": 0.3923763036727905,
      "learning_rate": 0.00029195164498583637,
      "loss": 0.9833,
      "step": 980
    },
    {
      "epoch": 0.5852360895588561,
      "grad_norm": 0.9399965405464172,
      "learning_rate": 0.0002916152875805431,
      "loss": 0.9483,
      "step": 990
    },
    {
      "epoch": 0.5911475652109658,
      "grad_norm": 1.4126980304718018,
      "learning_rate": 0.0002912722466472288,
      "loss": 1.6528,
      "step": 1000
    },
    {
      "epoch": 0.5970590408630755,
      "grad_norm": 1.4300436973571777,
      "learning_rate": 0.000290922538375697,
      "loss": 1.3048,
      "step": 1010
    },
    {
      "epoch": 0.6029705165151851,
      "grad_norm": 1.3519343137741089,
      "learning_rate": 0.0002905661792704161,
      "loss": 1.5751,
      "step": 1020
    },
    {
      "epoch": 0.6088819921672948,
      "grad_norm": 0.410167396068573,
      "learning_rate": 0.0002902031861497402,
      "loss": 1.0088,
      "step": 1030
    },
    {
      "epoch": 0.6147934678194045,
      "grad_norm": 0.8773415088653564,
      "learning_rate": 0.0002898335761451157,
      "loss": 0.9187,
      "step": 1040
    },
    {
      "epoch": 0.6207049434715141,
      "grad_norm": 2.3186562061309814,
      "learning_rate": 0.00028945736670027243,
      "loss": 1.3892,
      "step": 1050
    },
    {
      "epoch": 0.6266164191236238,
      "grad_norm": 1.0862361192703247,
      "learning_rate": 0.0002890745755704006,
      "loss": 1.2894,
      "step": 1060
    },
    {
      "epoch": 0.6325278947757333,
      "grad_norm": 0.8714461326599121,
      "learning_rate": 0.00028868522082131274,
      "loss": 1.219,
      "step": 1070
    },
    {
      "epoch": 0.638439370427843,
      "grad_norm": 1.4897832870483398,
      "learning_rate": 0.00028828932082859124,
      "loss": 1.1224,
      "step": 1080
    },
    {
      "epoch": 0.6443508460799527,
      "grad_norm": 2.1309995651245117,
      "learning_rate": 0.0002878868942767209,
      "loss": 1.4618,
      "step": 1090
    },
    {
      "epoch": 0.6502623217320623,
      "grad_norm": 1.5315428972244263,
      "learning_rate": 0.00028747796015820735,
      "loss": 1.2241,
      "step": 1100
    },
    {
      "epoch": 0.656173797384172,
      "grad_norm": 1.0699809789657593,
      "learning_rate": 0.0002870625377726804,
      "loss": 0.9697,
      "step": 1110
    },
    {
      "epoch": 0.6620852730362817,
      "grad_norm": 1.799970030784607,
      "learning_rate": 0.0002866406467259835,
      "loss": 1.0213,
      "step": 1120
    },
    {
      "epoch": 0.6679967486883913,
      "grad_norm": 0.8505222797393799,
      "learning_rate": 0.0002862123069292483,
      "loss": 1.2248,
      "step": 1130
    },
    {
      "epoch": 0.673908224340501,
      "grad_norm": 1.3934144973754883,
      "learning_rate": 0.00028577753859795494,
      "loss": 1.0988,
      "step": 1140
    },
    {
      "epoch": 0.6798196999926106,
      "grad_norm": 0.9352688789367676,
      "learning_rate": 0.000285336362250978,
      "loss": 1.2504,
      "step": 1150
    },
    {
      "epoch": 0.6857311756447203,
      "grad_norm": 1.4182519912719727,
      "learning_rate": 0.00028488879870961815,
      "loss": 1.1847,
      "step": 1160
    },
    {
      "epoch": 0.69164265129683,
      "grad_norm": 1.372120976448059,
      "learning_rate": 0.00028443486909661935,
      "loss": 1.3844,
      "step": 1170
    },
    {
      "epoch": 0.6975541269489396,
      "grad_norm": 1.2593580484390259,
      "learning_rate": 0.00028397459483517216,
      "loss": 0.9362,
      "step": 1180
    },
    {
      "epoch": 0.7034656026010493,
      "grad_norm": 1.0964024066925049,
      "learning_rate": 0.00028350799764790256,
      "loss": 1.286,
      "step": 1190
    },
    {
      "epoch": 0.709377078253159,
      "grad_norm": 1.644283652305603,
      "learning_rate": 0.0002830350995558468,
      "loss": 1.1282,
      "step": 1200
    },
    {
      "epoch": 0.7152885539052686,
      "grad_norm": 0.6417403817176819,
      "learning_rate": 0.00028255592287741195,
      "loss": 1.1041,
      "step": 1210
    },
    {
      "epoch": 0.7212000295573783,
      "grad_norm": 1.200995922088623,
      "learning_rate": 0.00028207049022732286,
      "loss": 1.1965,
      "step": 1220
    },
    {
      "epoch": 0.727111505209488,
      "grad_norm": 0.6340023875236511,
      "learning_rate": 0.0002815788245155548,
      "loss": 1.0427,
      "step": 1230
    },
    {
      "epoch": 0.7330229808615976,
      "grad_norm": 0.935417890548706,
      "learning_rate": 0.00028108094894625183,
      "loss": 0.7884,
      "step": 1240
    },
    {
      "epoch": 0.7389344565137073,
      "grad_norm": 1.2260072231292725,
      "learning_rate": 0.0002805768870166323,
      "loss": 1.2559,
      "step": 1250
    },
    {
      "epoch": 0.7448459321658168,
      "grad_norm": 1.202872395515442,
      "learning_rate": 0.00028006666251587944,
      "loss": 1.0569,
      "step": 1260
    },
    {
      "epoch": 0.7507574078179265,
      "grad_norm": 2.5879993438720703,
      "learning_rate": 0.0002795502995240188,
      "loss": 0.9203,
      "step": 1270
    },
    {
      "epoch": 0.7566688834700362,
      "grad_norm": 0.8845624923706055,
      "learning_rate": 0.00027902782241078194,
      "loss": 0.7764,
      "step": 1280
    },
    {
      "epoch": 0.7625803591221458,
      "grad_norm": 0.8509868383407593,
      "learning_rate": 0.000278499255834456,
      "loss": 1.2114,
      "step": 1290
    },
    {
      "epoch": 0.7684918347742555,
      "grad_norm": 0.7744777798652649,
      "learning_rate": 0.00027796462474071994,
      "loss": 1.1977,
      "step": 1300
    },
    {
      "epoch": 0.7744033104263652,
      "grad_norm": 0.9116315841674805,
      "learning_rate": 0.00027742395436146773,
      "loss": 1.3982,
      "step": 1310
    },
    {
      "epoch": 0.7803147860784748,
      "grad_norm": 1.059754490852356,
      "learning_rate": 0.0002768772702136169,
      "loss": 0.8068,
      "step": 1320
    },
    {
      "epoch": 0.7862262617305845,
      "grad_norm": 1.341540813446045,
      "learning_rate": 0.0002763245980979048,
      "loss": 1.0405,
      "step": 1330
    },
    {
      "epoch": 0.7921377373826941,
      "grad_norm": 3.170034885406494,
      "learning_rate": 0.00027576596409767043,
      "loss": 0.8736,
      "step": 1340
    },
    {
      "epoch": 0.7980492130348038,
      "grad_norm": 1.3075296878814697,
      "learning_rate": 0.0002752013945776238,
      "loss": 1.0294,
      "step": 1350
    },
    {
      "epoch": 0.8039606886869135,
      "grad_norm": 0.7042812705039978,
      "learning_rate": 0.0002746309161826018,
      "loss": 0.8612,
      "step": 1360
    },
    {
      "epoch": 0.8098721643390231,
      "grad_norm": 1.0310754776000977,
      "learning_rate": 0.00027405455583630995,
      "loss": 0.8773,
      "step": 1370
    },
    {
      "epoch": 0.8157836399911328,
      "grad_norm": 0.8753094673156738,
      "learning_rate": 0.0002734723407400527,
      "loss": 1.022,
      "step": 1380
    },
    {
      "epoch": 0.8216951156432425,
      "grad_norm": 1.4632091522216797,
      "learning_rate": 0.0002728842983714489,
      "loss": 1.4363,
      "step": 1390
    },
    {
      "epoch": 0.8276065912953521,
      "grad_norm": 1.3844555616378784,
      "learning_rate": 0.0002722904564831354,
      "loss": 1.0108,
      "step": 1400
    },
    {
      "epoch": 0.8335180669474618,
      "grad_norm": 0.8012031316757202,
      "learning_rate": 0.0002716908431014572,
      "loss": 0.9257,
      "step": 1410
    },
    {
      "epoch": 0.8394295425995714,
      "grad_norm": 0.48592811822891235,
      "learning_rate": 0.0002710854865251446,
      "loss": 1.0916,
      "step": 1420
    },
    {
      "epoch": 0.8453410182516811,
      "grad_norm": 2.0743398666381836,
      "learning_rate": 0.00027047441532397794,
      "loss": 0.9367,
      "step": 1430
    },
    {
      "epoch": 0.8512524939037908,
      "grad_norm": 1.422998309135437,
      "learning_rate": 0.0002698576583374388,
      "loss": 1.0587,
      "step": 1440
    },
    {
      "epoch": 0.8571639695559004,
      "grad_norm": 2.077911853790283,
      "learning_rate": 0.0002692352446733494,
      "loss": 1.0621,
      "step": 1450
    },
    {
      "epoch": 0.86307544520801,
      "grad_norm": 2.380415916442871,
      "learning_rate": 0.0002686072037064984,
      "loss": 0.9464,
      "step": 1460
    },
    {
      "epoch": 0.8689869208601197,
      "grad_norm": 1.3697080612182617,
      "learning_rate": 0.00026797356507725513,
      "loss": 0.9229,
      "step": 1470
    },
    {
      "epoch": 0.8748983965122293,
      "grad_norm": 0.9470162987709045,
      "learning_rate": 0.00026733435869017005,
      "loss": 0.8697,
      "step": 1480
    },
    {
      "epoch": 0.880809872164339,
      "grad_norm": 0.5239623188972473,
      "learning_rate": 0.00026668961471256397,
      "loss": 0.9903,
      "step": 1490
    },
    {
      "epoch": 0.8867213478164487,
      "grad_norm": 1.287339210510254,
      "learning_rate": 0.00026603936357310395,
      "loss": 0.9427,
      "step": 1500
    },
    {
      "epoch": 0.8926328234685583,
      "grad_norm": 0.9761462211608887,
      "learning_rate": 0.0002653836359603674,
      "loss": 0.9243,
      "step": 1510
    },
    {
      "epoch": 0.898544299120668,
      "grad_norm": 0.7651516199111938,
      "learning_rate": 0.0002647224628213936,
      "loss": 0.7775,
      "step": 1520
    },
    {
      "epoch": 0.9044557747727776,
      "grad_norm": 0.3834855854511261,
      "learning_rate": 0.00026405587536022324,
      "loss": 0.8652,
      "step": 1530
    },
    {
      "epoch": 0.9103672504248873,
      "grad_norm": 1.4622029066085815,
      "learning_rate": 0.00026338390503642586,
      "loss": 0.6217,
      "step": 1540
    },
    {
      "epoch": 0.916278726076997,
      "grad_norm": 0.7228372693061829,
      "learning_rate": 0.00026270658356361483,
      "loss": 0.9316,
      "step": 1550
    },
    {
      "epoch": 0.9221902017291066,
      "grad_norm": 2.2301530838012695,
      "learning_rate": 0.00026202394290795095,
      "loss": 0.89,
      "step": 1560
    },
    {
      "epoch": 0.9281016773812163,
      "grad_norm": 0.3718915581703186,
      "learning_rate": 0.0002613360152866335,
      "loss": 0.8487,
      "step": 1570
    },
    {
      "epoch": 0.934013153033326,
      "grad_norm": 2.8389663696289062,
      "learning_rate": 0.00026064283316638,
      "loss": 1.1533,
      "step": 1580
    },
    {
      "epoch": 0.9399246286854356,
      "grad_norm": 1.1260708570480347,
      "learning_rate": 0.00025994442926189384,
      "loss": 1.1317,
      "step": 1590
    },
    {
      "epoch": 0.9458361043375453,
      "grad_norm": 2.0674495697021484,
      "learning_rate": 0.0002592408365343202,
      "loss": 1.2019,
      "step": 1600
    },
    {
      "epoch": 0.9517475799896549,
      "grad_norm": 0.509918212890625,
      "learning_rate": 0.0002585320881896907,
      "loss": 1.0216,
      "step": 1610
    },
    {
      "epoch": 0.9576590556417646,
      "grad_norm": 1.1706818342208862,
      "learning_rate": 0.00025781821767735606,
      "loss": 1.0687,
      "step": 1620
    },
    {
      "epoch": 0.9635705312938743,
      "grad_norm": 1.2221193313598633,
      "learning_rate": 0.00025709925868840745,
      "loss": 1.1468,
      "step": 1630
    },
    {
      "epoch": 0.9694820069459839,
      "grad_norm": 1.1034770011901855,
      "learning_rate": 0.0002563752451540867,
      "loss": 1.097,
      "step": 1640
    },
    {
      "epoch": 0.9753934825980936,
      "grad_norm": 0.807884693145752,
      "learning_rate": 0.00025564621124418436,
      "loss": 0.5802,
      "step": 1650
    },
    {
      "epoch": 0.9813049582502033,
      "grad_norm": 1.3589593172073364,
      "learning_rate": 0.0002549121913654278,
      "loss": 1.0961,
      "step": 1660
    },
    {
      "epoch": 0.9872164339023128,
      "grad_norm": 0.7613115310668945,
      "learning_rate": 0.00025417322015985666,
      "loss": 0.6417,
      "step": 1670
    },
    {
      "epoch": 0.9931279095544225,
      "grad_norm": 2.2618770599365234,
      "learning_rate": 0.0002534293325031885,
      "loss": 1.2346,
      "step": 1680
    },
    {
      "epoch": 0.9990393852065321,
      "grad_norm": 0.9102917313575745,
      "learning_rate": 0.00025268056350317254,
      "loss": 0.7727,
      "step": 1690
    },
    {
      "epoch": 1.0047291805216878,
      "grad_norm": 0.6524246335029602,
      "learning_rate": 0.0002519269484979327,
      "loss": 0.7417,
      "step": 1700
    },
    {
      "epoch": 1.0106406561737973,
      "grad_norm": 0.8275794386863708,
      "learning_rate": 0.00025116852305430003,
      "loss": 0.8233,
      "step": 1710
    },
    {
      "epoch": 1.016552131825907,
      "grad_norm": 1.9044339656829834,
      "learning_rate": 0.00025040532296613396,
      "loss": 0.943,
      "step": 1720
    },
    {
      "epoch": 1.0224636074780167,
      "grad_norm": 1.0178546905517578,
      "learning_rate": 0.00024963738425263333,
      "loss": 0.8587,
      "step": 1730
    },
    {
      "epoch": 1.0283750831301264,
      "grad_norm": 0.5366402268409729,
      "learning_rate": 0.00024886474315663585,
      "loss": 0.9727,
      "step": 1740
    },
    {
      "epoch": 1.034286558782236,
      "grad_norm": 1.5807479619979858,
      "learning_rate": 0.0002480874361429082,
      "loss": 1.1956,
      "step": 1750
    },
    {
      "epoch": 1.0401980344343458,
      "grad_norm": 0.48487335443496704,
      "learning_rate": 0.0002473054998964249,
      "loss": 0.8652,
      "step": 1760
    },
    {
      "epoch": 1.0461095100864553,
      "grad_norm": 1.6157439947128296,
      "learning_rate": 0.00024651897132063676,
      "loss": 1.0114,
      "step": 1770
    },
    {
      "epoch": 1.052020985738565,
      "grad_norm": 2.582115888595581,
      "learning_rate": 0.00024572788753572954,
      "loss": 1.0355,
      "step": 1780
    },
    {
      "epoch": 1.0579324613906746,
      "grad_norm": 2.27616548538208,
      "learning_rate": 0.000244932285876872,
      "loss": 0.844,
      "step": 1790
    },
    {
      "epoch": 1.0638439370427843,
      "grad_norm": 0.7272843718528748,
      "learning_rate": 0.00024413220389245344,
      "loss": 0.9413,
      "step": 1800
    },
    {
      "epoch": 1.069755412694894,
      "grad_norm": 1.25956130027771,
      "learning_rate": 0.00024332767934231235,
      "loss": 1.2571,
      "step": 1810
    },
    {
      "epoch": 1.0756668883470035,
      "grad_norm": 1.153830885887146,
      "learning_rate": 0.00024251875019595367,
      "loss": 1.406,
      "step": 1820
    },
    {
      "epoch": 1.0815783639991132,
      "grad_norm": 0.990654468536377,
      "learning_rate": 0.00024170545463075727,
      "loss": 1.0171,
      "step": 1830
    },
    {
      "epoch": 1.087489839651223,
      "grad_norm": 1.2354010343551636,
      "learning_rate": 0.0002408878310301758,
      "loss": 0.8585,
      "step": 1840
    },
    {
      "epoch": 1.0934013153033326,
      "grad_norm": 1.4128049612045288,
      "learning_rate": 0.00024006591798192355,
      "loss": 0.7334,
      "step": 1850
    },
    {
      "epoch": 1.0993127909554423,
      "grad_norm": 2.367818593978882,
      "learning_rate": 0.000239239754276155,
      "loss": 1.062,
      "step": 1860
    },
    {
      "epoch": 1.1052242666075518,
      "grad_norm": 2.5342938899993896,
      "learning_rate": 0.0002384093789036343,
      "loss": 1.0655,
      "step": 1870
    },
    {
      "epoch": 1.1111357422596615,
      "grad_norm": 1.900240182876587,
      "learning_rate": 0.00023757483105389508,
      "loss": 1.2805,
      "step": 1880
    },
    {
      "epoch": 1.1170472179117712,
      "grad_norm": 0.6861922740936279,
      "learning_rate": 0.00023673615011339077,
      "loss": 0.815,
      "step": 1890
    },
    {
      "epoch": 1.122958693563881,
      "grad_norm": 0.6772130727767944,
      "learning_rate": 0.000235893375663636,
      "loss": 0.9904,
      "step": 1900
    },
    {
      "epoch": 1.1288701692159906,
      "grad_norm": 1.2433935403823853,
      "learning_rate": 0.0002350465474793383,
      "loss": 0.7559,
      "step": 1910
    },
    {
      "epoch": 1.1347816448681003,
      "grad_norm": 0.8172667622566223,
      "learning_rate": 0.0002341957055265211,
      "loss": 0.7927,
      "step": 1920
    },
    {
      "epoch": 1.1406931205202098,
      "grad_norm": 0.9793767929077148,
      "learning_rate": 0.00023334088996063745,
      "loss": 0.8528,
      "step": 1930
    },
    {
      "epoch": 1.1466045961723195,
      "grad_norm": 0.6232662796974182,
      "learning_rate": 0.0002324821411246749,
      "loss": 1.0596,
      "step": 1940
    },
    {
      "epoch": 1.1525160718244292,
      "grad_norm": 2.4136290550231934,
      "learning_rate": 0.00023161949954725162,
      "loss": 1.3745,
      "step": 1950
    },
    {
      "epoch": 1.1584275474765389,
      "grad_norm": 0.37645769119262695,
      "learning_rate": 0.00023075300594070335,
      "loss": 0.8194,
      "step": 1960
    },
    {
      "epoch": 1.1643390231286486,
      "grad_norm": 0.5415229201316833,
      "learning_rate": 0.00022988270119916256,
      "loss": 1.0719,
      "step": 1970
    },
    {
      "epoch": 1.1702504987807583,
      "grad_norm": 2.1367030143737793,
      "learning_rate": 0.00022900862639662768,
      "loss": 0.9426,
      "step": 1980
    },
    {
      "epoch": 1.1761619744328677,
      "grad_norm": 2.1803431510925293,
      "learning_rate": 0.00022813082278502536,
      "loss": 0.7771,
      "step": 1990
    },
    {
      "epoch": 1.1820734500849774,
      "grad_norm": 2.3459560871124268,
      "learning_rate": 0.000227249331792263,
      "loss": 0.8467,
      "step": 2000
    },
    {
      "epoch": 1.1879849257370871,
      "grad_norm": 2.3104922771453857,
      "learning_rate": 0.000226364195020274,
      "loss": 0.8154,
      "step": 2010
    },
    {
      "epoch": 1.1938964013891968,
      "grad_norm": 0.6474303007125854,
      "learning_rate": 0.000225475454243054,
      "loss": 0.7731,
      "step": 2020
    },
    {
      "epoch": 1.1998078770413065,
      "grad_norm": 1.341280460357666,
      "learning_rate": 0.00022458315140468964,
      "loss": 0.9333,
      "step": 2030
    },
    {
      "epoch": 1.205719352693416,
      "grad_norm": 0.8124862909317017,
      "learning_rate": 0.00022368732861737892,
      "loss": 1.0491,
      "step": 2040
    },
    {
      "epoch": 1.2116308283455257,
      "grad_norm": 1.1815528869628906,
      "learning_rate": 0.00022278802815944358,
      "loss": 0.642,
      "step": 2050
    },
    {
      "epoch": 1.2175423039976354,
      "grad_norm": 1.1570450067520142,
      "learning_rate": 0.000221885292473334,
      "loss": 1.3703,
      "step": 2060
    },
    {
      "epoch": 1.223453779649745,
      "grad_norm": 2.693166494369507,
      "learning_rate": 0.00022097916416362597,
      "loss": 0.9558,
      "step": 2070
    },
    {
      "epoch": 1.2293652553018548,
      "grad_norm": 1.3231215476989746,
      "learning_rate": 0.00022006968599501005,
      "loss": 0.8261,
      "step": 2080
    },
    {
      "epoch": 1.2352767309539643,
      "grad_norm": 1.1787503957748413,
      "learning_rate": 0.0002191569008902732,
      "loss": 1.0434,
      "step": 2090
    },
    {
      "epoch": 1.241188206606074,
      "grad_norm": 1.2227822542190552,
      "learning_rate": 0.00021824085192827312,
      "loss": 0.8278,
      "step": 2100
    },
    {
      "epoch": 1.2470996822581837,
      "grad_norm": 0.6513215899467468,
      "learning_rate": 0.0002173215823419052,
      "loss": 0.8118,
      "step": 2110
    },
    {
      "epoch": 1.2530111579102934,
      "grad_norm": 1.8179997205734253,
      "learning_rate": 0.00021639913551606195,
      "loss": 0.8811,
      "step": 2120
    },
    {
      "epoch": 1.258922633562403,
      "grad_norm": 2.0093889236450195,
      "learning_rate": 0.00021547355498558572,
      "loss": 0.9115,
      "step": 2130
    },
    {
      "epoch": 1.2648341092145126,
      "grad_norm": 1.0842578411102295,
      "learning_rate": 0.00021454488443321384,
      "loss": 0.8125,
      "step": 2140
    },
    {
      "epoch": 1.2707455848666223,
      "grad_norm": 0.261590838432312,
      "learning_rate": 0.00021361316768751706,
      "loss": 0.596,
      "step": 2150
    },
    {
      "epoch": 1.276657060518732,
      "grad_norm": 1.1249384880065918,
      "learning_rate": 0.0002126784487208313,
      "loss": 0.6771,
      "step": 2160
    },
    {
      "epoch": 1.2825685361708417,
      "grad_norm": 0.7610042095184326,
      "learning_rate": 0.00021174077164718184,
      "loss": 1.0096,
      "step": 2170
    },
    {
      "epoch": 1.2884800118229514,
      "grad_norm": 0.4077029526233673,
      "learning_rate": 0.0002108001807202021,
      "loss": 0.7857,
      "step": 2180
    },
    {
      "epoch": 1.2943914874750608,
      "grad_norm": 0.4888779819011688,
      "learning_rate": 0.00020985672033104437,
      "loss": 0.8763,
      "step": 2190
    },
    {
      "epoch": 1.3003029631271708,
      "grad_norm": 1.0512890815734863,
      "learning_rate": 0.0002089104350062853,
      "loss": 0.7233,
      "step": 2200
    },
    {
      "epoch": 1.3062144387792802,
      "grad_norm": 0.7906783819198608,
      "learning_rate": 0.00020796136940582423,
      "loss": 0.7202,
      "step": 2210
    },
    {
      "epoch": 1.31212591443139,
      "grad_norm": 2.1708078384399414,
      "learning_rate": 0.0002070095683207754,
      "loss": 1.0507,
      "step": 2220
    },
    {
      "epoch": 1.3180373900834996,
      "grad_norm": 1.8509455919265747,
      "learning_rate": 0.00020605507667135426,
      "loss": 1.0798,
      "step": 2230
    },
    {
      "epoch": 1.3239488657356093,
      "grad_norm": 0.9792012572288513,
      "learning_rate": 0.00020509793950475728,
      "loss": 1.0588,
      "step": 2240
    },
    {
      "epoch": 1.329860341387719,
      "grad_norm": 0.8537240624427795,
      "learning_rate": 0.00020413820199303604,
      "loss": 0.7792,
      "step": 2250
    },
    {
      "epoch": 1.3357718170398285,
      "grad_norm": 0.9870434999465942,
      "learning_rate": 0.0002031759094309653,
      "loss": 0.5972,
      "step": 2260
    },
    {
      "epoch": 1.3416832926919382,
      "grad_norm": 1.0740026235580444,
      "learning_rate": 0.00020221110723390518,
      "loss": 0.6142,
      "step": 2270
    },
    {
      "epoch": 1.347594768344048,
      "grad_norm": 1.9151570796966553,
      "learning_rate": 0.0002012438409356583,
      "loss": 1.4587,
      "step": 2280
    },
    {
      "epoch": 1.3535062439961576,
      "grad_norm": 1.179842233657837,
      "learning_rate": 0.00020027415618632002,
      "loss": 0.7372,
      "step": 2290
    },
    {
      "epoch": 1.3594177196482673,
      "grad_norm": 0.64407879114151,
      "learning_rate": 0.00019930209875012464,
      "loss": 0.5738,
      "step": 2300
    },
    {
      "epoch": 1.3653291953003768,
      "grad_norm": 2.1915836334228516,
      "learning_rate": 0.00019832771450328527,
      "loss": 0.9194,
      "step": 2310
    },
    {
      "epoch": 1.3712406709524865,
      "grad_norm": 2.678870439529419,
      "learning_rate": 0.00019735104943182874,
      "loss": 0.8864,
      "step": 2320
    },
    {
      "epoch": 1.3771521466045962,
      "grad_norm": 2.375553846359253,
      "learning_rate": 0.0001963721496294254,
      "loss": 0.7793,
      "step": 2330
    },
    {
      "epoch": 1.3830636222567059,
      "grad_norm": 0.7420865893363953,
      "learning_rate": 0.00019539106129521353,
      "loss": 0.9819,
      "step": 2340
    },
    {
      "epoch": 1.3889750979088156,
      "grad_norm": 1.8939887285232544,
      "learning_rate": 0.00019440783073161906,
      "loss": 0.9756,
      "step": 2350
    },
    {
      "epoch": 1.394886573560925,
      "grad_norm": 0.6966885328292847,
      "learning_rate": 0.0001934225043421705,
      "loss": 0.6723,
      "step": 2360
    },
    {
      "epoch": 1.4007980492130347,
      "grad_norm": 0.7439464926719666,
      "learning_rate": 0.00019243512862930867,
      "loss": 0.8128,
      "step": 2370
    },
    {
      "epoch": 1.4067095248651444,
      "grad_norm": 0.7100429534912109,
      "learning_rate": 0.00019144575019219216,
      "loss": 0.7444,
      "step": 2380
    },
    {
      "epoch": 1.4126210005172541,
      "grad_norm": 1.1208833456039429,
      "learning_rate": 0.00019045441572449804,
      "loss": 1.1657,
      "step": 2390
    },
    {
      "epoch": 1.4185324761693638,
      "grad_norm": 2.155616044998169,
      "learning_rate": 0.0001894611720122182,
      "loss": 1.1328,
      "step": 2400
    },
    {
      "epoch": 1.4244439518214733,
      "grad_norm": 1.2508968114852905,
      "learning_rate": 0.00018846606593145122,
      "loss": 0.9112,
      "step": 2410
    },
    {
      "epoch": 1.430355427473583,
      "grad_norm": 1.179256558418274,
      "learning_rate": 0.00018746914444619007,
      "loss": 0.8409,
      "step": 2420
    },
    {
      "epoch": 1.4362669031256927,
      "grad_norm": 1.6556161642074585,
      "learning_rate": 0.00018647045460610577,
      "loss": 1.0699,
      "step": 2430
    },
    {
      "epoch": 1.4421783787778024,
      "grad_norm": 1.0756868124008179,
      "learning_rate": 0.00018547004354432662,
      "loss": 0.8248,
      "step": 2440
    },
    {
      "epoch": 1.4480898544299121,
      "grad_norm": 1.7845070362091064,
      "learning_rate": 0.00018446795847521403,
      "loss": 1.0005,
      "step": 2450
    },
    {
      "epoch": 1.4540013300820216,
      "grad_norm": 1.0783575773239136,
      "learning_rate": 0.0001834642466921341,
      "loss": 0.8991,
      "step": 2460
    },
    {
      "epoch": 1.4599128057341315,
      "grad_norm": 1.214661955833435,
      "learning_rate": 0.00018245895556522561,
      "loss": 0.6395,
      "step": 2470
    },
    {
      "epoch": 1.465824281386241,
      "grad_norm": 1.3360155820846558,
      "learning_rate": 0.00018145213253916443,
      "loss": 1.1235,
      "step": 2480
    },
    {
      "epoch": 1.4717357570383507,
      "grad_norm": 1.346426010131836,
      "learning_rate": 0.0001804438251309243,
      "loss": 0.833,
      "step": 2490
    },
    {
      "epoch": 1.4776472326904604,
      "grad_norm": 0.8642708659172058,
      "learning_rate": 0.00017943408092753438,
      "loss": 0.7257,
      "step": 2500
    },
    {
      "epoch": 1.48355870834257,
      "grad_norm": 0.7588538527488708,
      "learning_rate": 0.0001784229475838333,
      "loss": 1.0598,
      "step": 2510
    },
    {
      "epoch": 1.4894701839946798,
      "grad_norm": 1.9235574007034302,
      "learning_rate": 0.00017741047282022015,
      "loss": 0.7295,
      "step": 2520
    },
    {
      "epoch": 1.4953816596467893,
      "grad_norm": 0.7834384441375732,
      "learning_rate": 0.00017639670442040221,
      "loss": 0.8316,
      "step": 2530
    },
    {
      "epoch": 1.501293135298899,
      "grad_norm": 1.1548680067062378,
      "learning_rate": 0.00017538169022913994,
      "loss": 0.9067,
      "step": 2540
    },
    {
      "epoch": 1.5072046109510087,
      "grad_norm": 1.2101777791976929,
      "learning_rate": 0.0001743654781499888,
      "loss": 1.162,
      "step": 2550
    },
    {
      "epoch": 1.5131160866031181,
      "grad_norm": 0.8461191058158875,
      "learning_rate": 0.00017334811614303858,
      "loss": 0.6402,
      "step": 2560
    },
    {
      "epoch": 1.519027562255228,
      "grad_norm": 2.3957488536834717,
      "learning_rate": 0.0001723296522226499,
      "loss": 0.8285,
      "step": 2570
    },
    {
      "epoch": 1.5249390379073375,
      "grad_norm": 0.9712939262390137,
      "learning_rate": 0.00017131013445518804,
      "loss": 1.2019,
      "step": 2580
    },
    {
      "epoch": 1.5308505135594472,
      "grad_norm": 1.127554178237915,
      "learning_rate": 0.00017028961095675462,
      "loss": 0.8436,
      "step": 2590
    },
    {
      "epoch": 1.536761989211557,
      "grad_norm": 1.059274435043335,
      "learning_rate": 0.00016926812989091671,
      "loss": 0.9869,
      "step": 2600
    },
    {
      "epoch": 1.5426734648636666,
      "grad_norm": 1.179976224899292,
      "learning_rate": 0.00016824573946643372,
      "loss": 0.9163,
      "step": 2610
    },
    {
      "epoch": 1.5485849405157763,
      "grad_norm": 0.6430381536483765,
      "learning_rate": 0.00016722248793498218,
      "loss": 0.6111,
      "step": 2620
    },
    {
      "epoch": 1.5544964161678858,
      "grad_norm": 2.6561145782470703,
      "learning_rate": 0.00016619842358887852,
      "loss": 0.872,
      "step": 2630
    },
    {
      "epoch": 1.5604078918199957,
      "grad_norm": 0.8799120187759399,
      "learning_rate": 0.00016517359475880003,
      "loss": 0.7811,
      "step": 2640
    },
    {
      "epoch": 1.5663193674721052,
      "grad_norm": 1.3347978591918945,
      "learning_rate": 0.00016414804981150366,
      "loss": 0.877,
      "step": 2650
    },
    {
      "epoch": 1.572230843124215,
      "grad_norm": 0.5052967071533203,
      "learning_rate": 0.0001631218371475435,
      "loss": 0.9272,
      "step": 2660
    },
    {
      "epoch": 1.5781423187763246,
      "grad_norm": 0.7100911140441895,
      "learning_rate": 0.0001620950051989866,
      "loss": 0.6659,
      "step": 2670
    },
    {
      "epoch": 1.584053794428434,
      "grad_norm": 1.43625807762146,
      "learning_rate": 0.00016106760242712707,
      "loss": 0.6979,
      "step": 2680
    },
    {
      "epoch": 1.589965270080544,
      "grad_norm": 0.6965720653533936,
      "learning_rate": 0.00016003967732019895,
      "loss": 0.9908,
      "step": 2690
    },
    {
      "epoch": 1.5958767457326535,
      "grad_norm": 0.9757608771324158,
      "learning_rate": 0.00015901127839108796,
      "loss": 0.6739,
      "step": 2700
    },
    {
      "epoch": 1.6017882213847632,
      "grad_norm": 0.9225283861160278,
      "learning_rate": 0.00015798245417504177,
      "loss": 0.7846,
      "step": 2710
    },
    {
      "epoch": 1.6076996970368729,
      "grad_norm": 1.127559781074524,
      "learning_rate": 0.00015695325322737948,
      "loss": 0.8621,
      "step": 2720
    },
    {
      "epoch": 1.6136111726889824,
      "grad_norm": 1.0685465335845947,
      "learning_rate": 0.00015592372412119996,
      "loss": 0.7809,
      "step": 2730
    },
    {
      "epoch": 1.6195226483410923,
      "grad_norm": 0.38803714513778687,
      "learning_rate": 0.00015489391544508966,
      "loss": 0.5176,
      "step": 2740
    },
    {
      "epoch": 1.6254341239932018,
      "grad_norm": 0.868606448173523,
      "learning_rate": 0.0001538638758008292,
      "loss": 0.9009,
      "step": 2750
    },
    {
      "epoch": 1.6313455996453115,
      "grad_norm": 2.0540428161621094,
      "learning_rate": 0.00015283365380109972,
      "loss": 0.9264,
      "step": 2760
    },
    {
      "epoch": 1.6372570752974211,
      "grad_norm": 0.28869742155075073,
      "learning_rate": 0.00015180329806718863,
      "loss": 0.9188,
      "step": 2770
    },
    {
      "epoch": 1.6431685509495306,
      "grad_norm": 0.44368094205856323,
      "learning_rate": 0.00015077285722669504,
      "loss": 0.8543,
      "step": 2780
    },
    {
      "epoch": 1.6490800266016405,
      "grad_norm": 2.3192813396453857,
      "learning_rate": 0.0001497423799112345,
      "loss": 0.954,
      "step": 2790
    },
    {
      "epoch": 1.65499150225375,
      "grad_norm": 1.0396865606307983,
      "learning_rate": 0.0001487119147541441,
      "loss": 0.794,
      "step": 2800
    },
    {
      "epoch": 1.6609029779058597,
      "grad_norm": 2.0185909271240234,
      "learning_rate": 0.00014768151038818702,
      "loss": 0.5997,
      "step": 2810
    },
    {
      "epoch": 1.6668144535579694,
      "grad_norm": 0.6434000730514526,
      "learning_rate": 0.00014665121544325756,
      "loss": 1.0243,
      "step": 2820
    },
    {
      "epoch": 1.6727259292100791,
      "grad_norm": 0.8992692232131958,
      "learning_rate": 0.00014562107854408578,
      "loss": 0.694,
      "step": 2830
    },
    {
      "epoch": 1.6786374048621888,
      "grad_norm": 1.2462797164916992,
      "learning_rate": 0.0001445911483079427,
      "loss": 0.8783,
      "step": 2840
    },
    {
      "epoch": 1.6845488805142983,
      "grad_norm": 1.0014230012893677,
      "learning_rate": 0.00014356147334234612,
      "loss": 0.8297,
      "step": 2850
    },
    {
      "epoch": 1.690460356166408,
      "grad_norm": 0.6426254510879517,
      "learning_rate": 0.0001425321022427662,
      "loss": 0.7868,
      "step": 2860
    },
    {
      "epoch": 1.6963718318185177,
      "grad_norm": 1.8866610527038574,
      "learning_rate": 0.0001415030835903321,
      "loss": 0.9552,
      "step": 2870
    },
    {
      "epoch": 1.7022833074706274,
      "grad_norm": 0.6876729130744934,
      "learning_rate": 0.0001404744659495394,
      "loss": 1.0935,
      "step": 2880
    },
    {
      "epoch": 1.708194783122737,
      "grad_norm": 0.9220296740531921,
      "learning_rate": 0.0001394462978659578,
      "loss": 0.8855,
      "step": 2890
    },
    {
      "epoch": 1.7141062587748466,
      "grad_norm": 0.5275249481201172,
      "learning_rate": 0.00013841862786394035,
      "loss": 0.5477,
      "step": 2900
    },
    {
      "epoch": 1.7200177344269565,
      "grad_norm": 0.8627028465270996,
      "learning_rate": 0.000137391504444333,
      "loss": 0.7085,
      "step": 2910
    },
    {
      "epoch": 1.725929210079066,
      "grad_norm": 1.0705426931381226,
      "learning_rate": 0.00013636497608218582,
      "loss": 0.6891,
      "step": 2920
    },
    {
      "epoch": 1.7318406857311757,
      "grad_norm": 1.1687321662902832,
      "learning_rate": 0.00013533909122446518,
      "loss": 0.9965,
      "step": 2930
    },
    {
      "epoch": 1.7377521613832854,
      "grad_norm": 2.8418073654174805,
      "learning_rate": 0.0001343138982877673,
      "loss": 0.8583,
      "step": 2940
    },
    {
      "epoch": 1.7436636370353948,
      "grad_norm": 0.6591094136238098,
      "learning_rate": 0.00013328944565603308,
      "loss": 0.8673,
      "step": 2950
    },
    {
      "epoch": 1.7495751126875048,
      "grad_norm": 1.2974783182144165,
      "learning_rate": 0.00013226578167826493,
      "loss": 0.4298,
      "step": 2960
    },
    {
      "epoch": 1.7554865883396142,
      "grad_norm": 0.9439045190811157,
      "learning_rate": 0.0001312429546662447,
      "loss": 0.6123,
      "step": 2970
    },
    {
      "epoch": 1.761398063991724,
      "grad_norm": 0.613162100315094,
      "learning_rate": 0.00013022101289225367,
      "loss": 0.8558,
      "step": 2980
    },
    {
      "epoch": 1.7673095396438336,
      "grad_norm": 1.274375557899475,
      "learning_rate": 0.0001292000045867943,
      "loss": 0.9439,
      "step": 2990
    },
    {
      "epoch": 1.7732210152959431,
      "grad_norm": 0.6581901907920837,
      "learning_rate": 0.00012817997793631414,
      "loss": 0.9134,
      "step": 3000
    },
    {
      "epoch": 1.779132490948053,
      "grad_norm": 1.673659086227417,
      "learning_rate": 0.00012716098108093147,
      "loss": 0.9171,
      "step": 3010
    },
    {
      "epoch": 1.7850439666001625,
      "grad_norm": 1.229710340499878,
      "learning_rate": 0.00012614306211216356,
      "loss": 0.837,
      "step": 3020
    },
    {
      "epoch": 1.7909554422522722,
      "grad_norm": 1.624604344367981,
      "learning_rate": 0.00012512626907065663,
      "loss": 1.1144,
      "step": 3030
    },
    {
      "epoch": 1.796866917904382,
      "grad_norm": 0.52573561668396,
      "learning_rate": 0.00012411064994391905,
      "loss": 0.5209,
      "step": 3040
    },
    {
      "epoch": 1.8027783935564914,
      "grad_norm": 1.3669898509979248,
      "learning_rate": 0.00012309625266405626,
      "loss": 0.9976,
      "step": 3050
    },
    {
      "epoch": 1.8086898692086013,
      "grad_norm": 0.8688726425170898,
      "learning_rate": 0.0001220831251055086,
      "loss": 0.5048,
      "step": 3060
    },
    {
      "epoch": 1.8146013448607108,
      "grad_norm": 0.6759256720542908,
      "learning_rate": 0.00012107131508279199,
      "loss": 0.5322,
      "step": 3070
    },
    {
      "epoch": 1.8205128205128205,
      "grad_norm": 0.9678435325622559,
      "learning_rate": 0.00012006087034824142,
      "loss": 0.5554,
      "step": 3080
    },
    {
      "epoch": 1.8264242961649302,
      "grad_norm": 0.7976598739624023,
      "learning_rate": 0.00011905183858975697,
      "loss": 0.7071,
      "step": 3090
    },
    {
      "epoch": 1.8323357718170399,
      "grad_norm": 1.5325818061828613,
      "learning_rate": 0.0001180442674285535,
      "loss": 0.7068,
      "step": 3100
    },
    {
      "epoch": 1.8382472474691496,
      "grad_norm": 0.7884787321090698,
      "learning_rate": 0.00011703820441691307,
      "loss": 0.6314,
      "step": 3110
    },
    {
      "epoch": 1.844158723121259,
      "grad_norm": 1.2543799877166748,
      "learning_rate": 0.00011603369703594049,
      "loss": 1.0833,
      "step": 3120
    },
    {
      "epoch": 1.8500701987733688,
      "grad_norm": 2.686657428741455,
      "learning_rate": 0.0001150307926933228,
      "loss": 0.8732,
      "step": 3130
    },
    {
      "epoch": 1.8559816744254785,
      "grad_norm": 0.7824208736419678,
      "learning_rate": 0.00011402953872109175,
      "loss": 0.7741,
      "step": 3140
    },
    {
      "epoch": 1.8618931500775882,
      "grad_norm": 0.7668352127075195,
      "learning_rate": 0.0001130299823733897,
      "loss": 0.8694,
      "step": 3150
    },
    {
      "epoch": 1.8678046257296979,
      "grad_norm": 1.4269648790359497,
      "learning_rate": 0.00011203217082423994,
      "loss": 1.2301,
      "step": 3160
    },
    {
      "epoch": 1.8737161013818073,
      "grad_norm": 0.7646162509918213,
      "learning_rate": 0.00011103615116531988,
      "loss": 1.0477,
      "step": 3170
    },
    {
      "epoch": 1.8796275770339173,
      "grad_norm": 0.9072762131690979,
      "learning_rate": 0.00011004197040373884,
      "loss": 0.7265,
      "step": 3180
    },
    {
      "epoch": 1.8855390526860267,
      "grad_norm": 0.9282490015029907,
      "learning_rate": 0.0001090496754598194,
      "loss": 0.7552,
      "step": 3190
    },
    {
      "epoch": 1.8914505283381364,
      "grad_norm": 2.4503073692321777,
      "learning_rate": 0.00010805931316488307,
      "loss": 1.0163,
      "step": 3200
    },
    {
      "epoch": 1.8973620039902461,
      "grad_norm": 2.219407081604004,
      "learning_rate": 0.00010707093025904005,
      "loss": 1.0082,
      "step": 3210
    },
    {
      "epoch": 1.9032734796423556,
      "grad_norm": 0.9543687105178833,
      "learning_rate": 0.00010608457338898345,
      "loss": 0.6858,
      "step": 3220
    },
    {
      "epoch": 1.9091849552944655,
      "grad_norm": 1.668184518814087,
      "learning_rate": 0.00010510028910578747,
      "loss": 0.6405,
      "step": 3230
    },
    {
      "epoch": 1.915096430946575,
      "grad_norm": 0.8261764049530029,
      "learning_rate": 0.00010411812386271085,
      "loss": 0.8216,
      "step": 3240
    },
    {
      "epoch": 1.9210079065986847,
      "grad_norm": 0.9872915744781494,
      "learning_rate": 0.00010313812401300435,
      "loss": 0.6595,
      "step": 3250
    },
    {
      "epoch": 1.9269193822507944,
      "grad_norm": 2.2113444805145264,
      "learning_rate": 0.00010216033580772281,
      "loss": 0.959,
      "step": 3260
    },
    {
      "epoch": 1.9328308579029039,
      "grad_norm": 0.5234913229942322,
      "learning_rate": 0.00010118480539354289,
      "loss": 0.6429,
      "step": 3270
    },
    {
      "epoch": 1.9387423335550138,
      "grad_norm": 0.729966402053833,
      "learning_rate": 0.0001002115788105847,
      "loss": 0.8704,
      "step": 3280
    },
    {
      "epoch": 1.9446538092071233,
      "grad_norm": 3.393608808517456,
      "learning_rate": 9.92407019902392e-05,
      "loss": 0.9067,
      "step": 3290
    },
    {
      "epoch": 1.950565284859233,
      "grad_norm": 1.157718539237976,
      "learning_rate": 9.827222075300047e-05,
      "loss": 0.7414,
      "step": 3300
    },
    {
      "epoch": 1.9564767605113427,
      "grad_norm": 0.5869197249412537,
      "learning_rate": 9.73061808063029e-05,
      "loss": 0.7809,
      "step": 3310
    },
    {
      "epoch": 1.9623882361634521,
      "grad_norm": 0.6229377388954163,
      "learning_rate": 9.634262774236451e-05,
      "loss": 0.7887,
      "step": 3320
    },
    {
      "epoch": 1.968299711815562,
      "grad_norm": 0.5162950158119202,
      "learning_rate": 9.538160703603492e-05,
      "loss": 1.0686,
      "step": 3330
    },
    {
      "epoch": 1.9742111874676715,
      "grad_norm": 1.9393310546875,
      "learning_rate": 9.442316404264912e-05,
      "loss": 0.7283,
      "step": 3340
    },
    {
      "epoch": 1.9801226631197812,
      "grad_norm": 2.146913528442383,
      "learning_rate": 9.346734399588715e-05,
      "loss": 0.7099,
      "step": 3350
    },
    {
      "epoch": 1.986034138771891,
      "grad_norm": 0.3855842351913452,
      "learning_rate": 9.251419200563917e-05,
      "loss": 0.5049,
      "step": 3360
    },
    {
      "epoch": 1.9919456144240006,
      "grad_norm": 1.7744780778884888,
      "learning_rate": 9.156375305587648e-05,
      "loss": 0.5695,
      "step": 3370
    },
    {
      "epoch": 1.9978570900761103,
      "grad_norm": 1.5964759588241577,
      "learning_rate": 9.061607200252859e-05,
      "loss": 0.8764,
      "step": 3380
    },
    {
      "epoch": 2.0035468853912657,
      "grad_norm": 1.052750587463379,
      "learning_rate": 8.967119357136615e-05,
      "loss": 0.6444,
      "step": 3390
    },
    {
      "epoch": 2.0094583610433756,
      "grad_norm": 1.1859277486801147,
      "learning_rate": 8.872916235589018e-05,
      "loss": 0.7387,
      "step": 3400
    },
    {
      "epoch": 2.015369836695485,
      "grad_norm": 1.4475314617156982,
      "learning_rate": 8.779002281522749e-05,
      "loss": 0.9549,
      "step": 3410
    },
    {
      "epoch": 2.0212813123475946,
      "grad_norm": 1.9343997240066528,
      "learning_rate": 8.68538192720323e-05,
      "loss": 0.6635,
      "step": 3420
    },
    {
      "epoch": 2.0271927879997045,
      "grad_norm": 0.4448454678058624,
      "learning_rate": 8.59205959103947e-05,
      "loss": 0.5205,
      "step": 3430
    },
    {
      "epoch": 2.033104263651814,
      "grad_norm": 0.9201733469963074,
      "learning_rate": 8.49903967737552e-05,
      "loss": 0.6983,
      "step": 3440
    },
    {
      "epoch": 2.039015739303924,
      "grad_norm": 0.7760902643203735,
      "learning_rate": 8.406326576282596e-05,
      "loss": 0.5961,
      "step": 3450
    },
    {
      "epoch": 2.0449272149560334,
      "grad_norm": 0.283441424369812,
      "learning_rate": 8.313924663351926e-05,
      "loss": 0.7269,
      "step": 3460
    },
    {
      "epoch": 2.050838690608143,
      "grad_norm": 1.174414873123169,
      "learning_rate": 8.221838299488223e-05,
      "loss": 0.9124,
      "step": 3470
    },
    {
      "epoch": 2.0567501662602528,
      "grad_norm": 1.166863203048706,
      "learning_rate": 8.130071830703855e-05,
      "loss": 0.9012,
      "step": 3480
    },
    {
      "epoch": 2.0626616419123622,
      "grad_norm": 0.7462505102157593,
      "learning_rate": 8.038629587913776e-05,
      "loss": 1.0634,
      "step": 3490
    },
    {
      "epoch": 2.068573117564472,
      "grad_norm": 2.3239376544952393,
      "learning_rate": 7.947515886731097e-05,
      "loss": 0.5877,
      "step": 3500
    },
    {
      "epoch": 2.0744845932165816,
      "grad_norm": 1.129707932472229,
      "learning_rate": 7.856735027263428e-05,
      "loss": 1.0698,
      "step": 3510
    },
    {
      "epoch": 2.0803960688686916,
      "grad_norm": 2.2795071601867676,
      "learning_rate": 7.76629129390991e-05,
      "loss": 0.9746,
      "step": 3520
    },
    {
      "epoch": 2.086307544520801,
      "grad_norm": 1.147071361541748,
      "learning_rate": 7.676188955159046e-05,
      "loss": 0.7404,
      "step": 3530
    },
    {
      "epoch": 2.0922190201729105,
      "grad_norm": 0.6926111578941345,
      "learning_rate": 7.586432263387231e-05,
      "loss": 0.9164,
      "step": 3540
    },
    {
      "epoch": 2.0981304958250204,
      "grad_norm": 0.49554747343063354,
      "learning_rate": 7.497025454658065e-05,
      "loss": 0.9108,
      "step": 3550
    },
    {
      "epoch": 2.10404197147713,
      "grad_norm": 0.873499870300293,
      "learning_rate": 7.40797274852242e-05,
      "loss": 0.7231,
      "step": 3560
    },
    {
      "epoch": 2.10995344712924,
      "grad_norm": 0.6804977655410767,
      "learning_rate": 7.319278347819322e-05,
      "loss": 0.6815,
      "step": 3570
    },
    {
      "epoch": 2.1158649227813493,
      "grad_norm": 0.6452701091766357,
      "learning_rate": 7.23094643847759e-05,
      "loss": 0.5257,
      "step": 3580
    },
    {
      "epoch": 2.1217763984334588,
      "grad_norm": 1.2437492609024048,
      "learning_rate": 7.142981189318255e-05,
      "loss": 0.8765,
      "step": 3590
    },
    {
      "epoch": 2.1276878740855687,
      "grad_norm": 1.3263859748840332,
      "learning_rate": 7.05538675185786e-05,
      "loss": 0.7543,
      "step": 3600
    },
    {
      "epoch": 2.133599349737678,
      "grad_norm": 0.9840863347053528,
      "learning_rate": 6.968167260112493e-05,
      "loss": 0.5468,
      "step": 3610
    },
    {
      "epoch": 2.139510825389788,
      "grad_norm": 0.8987053036689758,
      "learning_rate": 6.881326830402687e-05,
      "loss": 0.5943,
      "step": 3620
    },
    {
      "epoch": 2.1454223010418976,
      "grad_norm": 1.488179087638855,
      "learning_rate": 6.794869561159172e-05,
      "loss": 0.6477,
      "step": 3630
    },
    {
      "epoch": 2.151333776694007,
      "grad_norm": 1.820414662361145,
      "learning_rate": 6.708799532729426e-05,
      "loss": 0.7997,
      "step": 3640
    },
    {
      "epoch": 2.157245252346117,
      "grad_norm": 0.4153795838356018,
      "learning_rate": 6.623120807185107e-05,
      "loss": 0.5743,
      "step": 3650
    },
    {
      "epoch": 2.1631567279982264,
      "grad_norm": 0.7466774582862854,
      "learning_rate": 6.537837428130354e-05,
      "loss": 0.7201,
      "step": 3660
    },
    {
      "epoch": 2.1690682036503364,
      "grad_norm": 1.8089007139205933,
      "learning_rate": 6.452953420510954e-05,
      "loss": 0.6882,
      "step": 3670
    },
    {
      "epoch": 2.174979679302446,
      "grad_norm": 1.0947054624557495,
      "learning_rate": 6.36847279042435e-05,
      "loss": 0.6852,
      "step": 3680
    },
    {
      "epoch": 2.1808911549545558,
      "grad_norm": 1.4526841640472412,
      "learning_rate": 6.284399524930617e-05,
      "loss": 0.9862,
      "step": 3690
    },
    {
      "epoch": 2.1868026306066652,
      "grad_norm": 1.2129902839660645,
      "learning_rate": 6.200737591864274e-05,
      "loss": 1.0908,
      "step": 3700
    },
    {
      "epoch": 2.1927141062587747,
      "grad_norm": 2.281780481338501,
      "learning_rate": 6.117490939647015e-05,
      "loss": 0.6205,
      "step": 3710
    },
    {
      "epoch": 2.1986255819108846,
      "grad_norm": 0.8913925290107727,
      "learning_rate": 6.034663497101367e-05,
      "loss": 1.0239,
      "step": 3720
    },
    {
      "epoch": 2.204537057562994,
      "grad_norm": 3.598144769668579,
      "learning_rate": 5.952259173265281e-05,
      "loss": 0.7318,
      "step": 3730
    },
    {
      "epoch": 2.2104485332151036,
      "grad_norm": 0.5066977143287659,
      "learning_rate": 5.870281857207634e-05,
      "loss": 0.877,
      "step": 3740
    },
    {
      "epoch": 2.2163600088672135,
      "grad_norm": 0.7661294341087341,
      "learning_rate": 5.788735417844693e-05,
      "loss": 0.628,
      "step": 3750
    },
    {
      "epoch": 2.222271484519323,
      "grad_norm": 0.9893051385879517,
      "learning_rate": 5.7076237037575014e-05,
      "loss": 0.883,
      "step": 3760
    },
    {
      "epoch": 2.228182960171433,
      "grad_norm": 1.4790452718734741,
      "learning_rate": 5.626950543010269e-05,
      "loss": 0.9609,
      "step": 3770
    },
    {
      "epoch": 2.2340944358235424,
      "grad_norm": 1.388301134109497,
      "learning_rate": 5.5467197429697085e-05,
      "loss": 0.9404,
      "step": 3780
    },
    {
      "epoch": 2.2400059114756523,
      "grad_norm": 1.719220519065857,
      "learning_rate": 5.466935090125312e-05,
      "loss": 0.8743,
      "step": 3790
    },
    {
      "epoch": 2.245917387127762,
      "grad_norm": 1.3027782440185547,
      "learning_rate": 5.38760034991069e-05,
      "loss": 0.9608,
      "step": 3800
    },
    {
      "epoch": 2.2518288627798713,
      "grad_norm": 1.0167053937911987,
      "learning_rate": 5.308719266525847e-05,
      "loss": 0.9156,
      "step": 3810
    },
    {
      "epoch": 2.257740338431981,
      "grad_norm": 1.5089694261550903,
      "learning_rate": 5.2302955627604496e-05,
      "loss": 0.7605,
      "step": 3820
    },
    {
      "epoch": 2.2636518140840907,
      "grad_norm": 0.9827752113342285,
      "learning_rate": 5.152332939818175e-05,
      "loss": 0.6494,
      "step": 3830
    },
    {
      "epoch": 2.2695632897362006,
      "grad_norm": 1.0695838928222656,
      "learning_rate": 5.07483507714199e-05,
      "loss": 0.7474,
      "step": 3840
    },
    {
      "epoch": 2.27547476538831,
      "grad_norm": 0.5940055251121521,
      "learning_rate": 4.997805632240531e-05,
      "loss": 0.9102,
      "step": 3850
    },
    {
      "epoch": 2.2813862410404195,
      "grad_norm": 0.7087309956550598,
      "learning_rate": 4.921248240515474e-05,
      "loss": 0.6407,
      "step": 3860
    },
    {
      "epoch": 2.2872977166925295,
      "grad_norm": 0.5292789936065674,
      "learning_rate": 4.8451665150899556e-05,
      "loss": 0.6402,
      "step": 3870
    },
    {
      "epoch": 2.293209192344639,
      "grad_norm": 1.6491916179656982,
      "learning_rate": 4.769564046638065e-05,
      "loss": 0.7628,
      "step": 3880
    },
    {
      "epoch": 2.299120667996749,
      "grad_norm": 1.4231927394866943,
      "learning_rate": 4.694444403215381e-05,
      "loss": 0.6604,
      "step": 3890
    },
    {
      "epoch": 2.3050321436488583,
      "grad_norm": 0.6799333691596985,
      "learning_rate": 4.619811130090568e-05,
      "loss": 0.6757,
      "step": 3900
    },
    {
      "epoch": 2.310943619300968,
      "grad_norm": 0.900615394115448,
      "learning_rate": 4.545667749578072e-05,
      "loss": 0.7778,
      "step": 3910
    },
    {
      "epoch": 2.3168550949530777,
      "grad_norm": 1.0130841732025146,
      "learning_rate": 4.4720177608718564e-05,
      "loss": 0.7971,
      "step": 3920
    },
    {
      "epoch": 2.322766570605187,
      "grad_norm": 0.6497480273246765,
      "learning_rate": 4.3988646398802986e-05,
      "loss": 0.96,
      "step": 3930
    },
    {
      "epoch": 2.328678046257297,
      "grad_norm": 1.7506275177001953,
      "learning_rate": 4.326211839062111e-05,
      "loss": 0.6659,
      "step": 3940
    },
    {
      "epoch": 2.3345895219094066,
      "grad_norm": 0.6876859664916992,
      "learning_rate": 4.254062787263426e-05,
      "loss": 0.8055,
      "step": 3950
    },
    {
      "epoch": 2.3405009975615165,
      "grad_norm": 1.3025952577590942,
      "learning_rate": 4.182420889555943e-05,
      "loss": 0.6561,
      "step": 3960
    },
    {
      "epoch": 2.346412473213626,
      "grad_norm": 0.7275381088256836,
      "learning_rate": 4.111289527076265e-05,
      "loss": 0.4364,
      "step": 3970
    },
    {
      "epoch": 2.3523239488657355,
      "grad_norm": 2.917642116546631,
      "learning_rate": 4.040672056866282e-05,
      "loss": 0.9347,
      "step": 3980
    },
    {
      "epoch": 2.3582354245178454,
      "grad_norm": 1.0721228122711182,
      "learning_rate": 3.970571811714773e-05,
      "loss": 0.6566,
      "step": 3990
    },
    {
      "epoch": 2.364146900169955,
      "grad_norm": 0.9597646594047546,
      "learning_rate": 3.900992100000107e-05,
      "loss": 0.802,
      "step": 4000
    },
    {
      "epoch": 2.3700583758220644,
      "grad_norm": 0.5823359489440918,
      "learning_rate": 3.831936205534074e-05,
      "loss": 0.687,
      "step": 4010
    },
    {
      "epoch": 2.3759698514741743,
      "grad_norm": 0.9762411117553711,
      "learning_rate": 3.7634073874069486e-05,
      "loss": 0.8637,
      "step": 4020
    },
    {
      "epoch": 2.3818813271262838,
      "grad_norm": 2.6935064792633057,
      "learning_rate": 3.695408879833657e-05,
      "loss": 0.8014,
      "step": 4030
    },
    {
      "epoch": 2.3877928027783937,
      "grad_norm": 0.33311429619789124,
      "learning_rate": 3.627943892001126e-05,
      "loss": 0.8303,
      "step": 4040
    },
    {
      "epoch": 2.393704278430503,
      "grad_norm": 0.6854402422904968,
      "learning_rate": 3.561015607916853e-05,
      "loss": 0.8908,
      "step": 4050
    },
    {
      "epoch": 2.399615754082613,
      "grad_norm": 1.3722631931304932,
      "learning_rate": 3.4946271862586175e-05,
      "loss": 0.7267,
      "step": 4060
    },
    {
      "epoch": 2.4055272297347225,
      "grad_norm": 1.088506817817688,
      "learning_rate": 3.428781760225408e-05,
      "loss": 0.666,
      "step": 4070
    },
    {
      "epoch": 2.411438705386832,
      "grad_norm": 1.1638840436935425,
      "learning_rate": 3.363482437389563e-05,
      "loss": 0.7735,
      "step": 4080
    },
    {
      "epoch": 2.417350181038942,
      "grad_norm": 1.0793628692626953,
      "learning_rate": 3.298732299550097e-05,
      "loss": 0.8089,
      "step": 4090
    },
    {
      "epoch": 2.4232616566910514,
      "grad_norm": 0.7373725771903992,
      "learning_rate": 3.234534402587264e-05,
      "loss": 0.6525,
      "step": 4100
    },
    {
      "epoch": 2.4291731323431613,
      "grad_norm": 0.7605533599853516,
      "learning_rate": 3.17089177631833e-05,
      "loss": 0.6101,
      "step": 4110
    },
    {
      "epoch": 2.435084607995271,
      "grad_norm": 1.2107417583465576,
      "learning_rate": 3.1078074243545756e-05,
      "loss": 0.6149,
      "step": 4120
    },
    {
      "epoch": 2.4409960836473803,
      "grad_norm": 1.2944657802581787,
      "learning_rate": 3.0452843239595536e-05,
      "loss": 0.8333,
      "step": 4130
    },
    {
      "epoch": 2.44690755929949,
      "grad_norm": 0.48125678300857544,
      "learning_rate": 2.9833254259085747e-05,
      "loss": 0.816,
      "step": 4140
    },
    {
      "epoch": 2.4528190349515997,
      "grad_norm": 0.43809694051742554,
      "learning_rate": 2.9219336543494278e-05,
      "loss": 0.7017,
      "step": 4150
    },
    {
      "epoch": 2.4587305106037096,
      "grad_norm": 0.7329030632972717,
      "learning_rate": 2.8611119066644027e-05,
      "loss": 0.7809,
      "step": 4160
    },
    {
      "epoch": 2.464641986255819,
      "grad_norm": 0.7379255890846252,
      "learning_rate": 2.800863053333531e-05,
      "loss": 0.6727,
      "step": 4170
    },
    {
      "epoch": 2.4705534619079286,
      "grad_norm": 0.764703631401062,
      "learning_rate": 2.7411899377991114e-05,
      "loss": 0.6404,
      "step": 4180
    },
    {
      "epoch": 2.4764649375600385,
      "grad_norm": 2.111982822418213,
      "learning_rate": 2.682095376331525e-05,
      "loss": 0.7211,
      "step": 4190
    },
    {
      "epoch": 2.482376413212148,
      "grad_norm": 1.7491860389709473,
      "learning_rate": 2.6235821578963202e-05,
      "loss": 0.919,
      "step": 4200
    },
    {
      "epoch": 2.488287888864258,
      "grad_norm": 0.38796254992485046,
      "learning_rate": 2.5656530440225685e-05,
      "loss": 0.5318,
      "step": 4210
    },
    {
      "epoch": 2.4941993645163674,
      "grad_norm": 2.27361798286438,
      "learning_rate": 2.5083107686725673e-05,
      "loss": 0.7279,
      "step": 4220
    },
    {
      "epoch": 2.5001108401684773,
      "grad_norm": 2.2895419597625732,
      "learning_rate": 2.4515580381127836e-05,
      "loss": 0.679,
      "step": 4230
    },
    {
      "epoch": 2.5060223158205868,
      "grad_norm": 0.4957699477672577,
      "learning_rate": 2.3953975307861368e-05,
      "loss": 0.5724,
      "step": 4240
    },
    {
      "epoch": 2.5119337914726962,
      "grad_norm": 0.5360196232795715,
      "learning_rate": 2.339831897185605e-05,
      "loss": 0.7756,
      "step": 4250
    },
    {
      "epoch": 2.517845267124806,
      "grad_norm": 0.631817638874054,
      "learning_rate": 2.2848637597291098e-05,
      "loss": 0.7509,
      "step": 4260
    },
    {
      "epoch": 2.5237567427769156,
      "grad_norm": 0.7030420899391174,
      "learning_rate": 2.230495712635777e-05,
      "loss": 0.6319,
      "step": 4270
    },
    {
      "epoch": 2.529668218429025,
      "grad_norm": 0.7935531139373779,
      "learning_rate": 2.176730321803486e-05,
      "loss": 0.9131,
      "step": 4280
    },
    {
      "epoch": 2.535579694081135,
      "grad_norm": 1.2224440574645996,
      "learning_rate": 2.1235701246877788e-05,
      "loss": 0.5984,
      "step": 4290
    },
    {
      "epoch": 2.5414911697332445,
      "grad_norm": 1.0811012983322144,
      "learning_rate": 2.071017630182101e-05,
      "loss": 0.6225,
      "step": 4300
    },
    {
      "epoch": 2.5474026453853544,
      "grad_norm": 1.8793902397155762,
      "learning_rate": 2.019075318499403e-05,
      "loss": 0.777,
      "step": 4310
    },
    {
      "epoch": 2.553314121037464,
      "grad_norm": 1.0976089239120483,
      "learning_rate": 1.9677456410550695e-05,
      "loss": 0.8912,
      "step": 4320
    },
    {
      "epoch": 2.559225596689574,
      "grad_norm": 0.6748432517051697,
      "learning_rate": 1.917031020351246e-05,
      "loss": 0.9369,
      "step": 4330
    },
    {
      "epoch": 2.5651370723416833,
      "grad_norm": 0.3951241374015808,
      "learning_rate": 1.8669338498624997e-05,
      "loss": 0.6127,
      "step": 4340
    },
    {
      "epoch": 2.571048547993793,
      "grad_norm": 0.47593528032302856,
      "learning_rate": 1.8174564939228454e-05,
      "loss": 0.7418,
      "step": 4350
    },
    {
      "epoch": 2.5769600236459027,
      "grad_norm": 0.7712517976760864,
      "learning_rate": 1.768601287614189e-05,
      "loss": 0.7815,
      "step": 4360
    },
    {
      "epoch": 2.582871499298012,
      "grad_norm": 2.779448986053467,
      "learning_rate": 1.7203705366561033e-05,
      "loss": 0.6929,
      "step": 4370
    },
    {
      "epoch": 2.5887829749501217,
      "grad_norm": 1.2710086107254028,
      "learning_rate": 1.672766517297012e-05,
      "loss": 0.6936,
      "step": 4380
    },
    {
      "epoch": 2.5946944506022316,
      "grad_norm": 1.026863694190979,
      "learning_rate": 1.6257914762067713e-05,
      "loss": 0.6246,
      "step": 4390
    },
    {
      "epoch": 2.6006059262543415,
      "grad_norm": 0.6119465231895447,
      "learning_rate": 1.5794476303706245e-05,
      "loss": 0.7627,
      "step": 4400
    },
    {
      "epoch": 2.606517401906451,
      "grad_norm": 0.435059130191803,
      "learning_rate": 1.5337371669845873e-05,
      "loss": 0.9749,
      "step": 4410
    },
    {
      "epoch": 2.6124288775585605,
      "grad_norm": 2.1609342098236084,
      "learning_rate": 1.4886622433522105e-05,
      "loss": 0.7767,
      "step": 4420
    },
    {
      "epoch": 2.6183403532106704,
      "grad_norm": 1.7148268222808838,
      "learning_rate": 1.4442249867827716e-05,
      "loss": 0.7048,
      "step": 4430
    },
    {
      "epoch": 2.62425182886278,
      "grad_norm": 1.1148688793182373,
      "learning_rate": 1.4004274944908739e-05,
      "loss": 0.8948,
      "step": 4440
    },
    {
      "epoch": 2.6301633045148893,
      "grad_norm": 1.9539780616760254,
      "learning_rate": 1.3572718334974785e-05,
      "loss": 0.5867,
      "step": 4450
    },
    {
      "epoch": 2.6360747801669993,
      "grad_norm": 1.567539095878601,
      "learning_rate": 1.3147600405323305e-05,
      "loss": 0.6899,
      "step": 4460
    },
    {
      "epoch": 2.6419862558191087,
      "grad_norm": 1.067142367362976,
      "learning_rate": 1.2728941219378586e-05,
      "loss": 0.7282,
      "step": 4470
    },
    {
      "epoch": 2.6478977314712187,
      "grad_norm": 0.6219662427902222,
      "learning_rate": 1.2316760535744696e-05,
      "loss": 0.5813,
      "step": 4480
    },
    {
      "epoch": 2.653809207123328,
      "grad_norm": 0.6565268635749817,
      "learning_rate": 1.1911077807273084e-05,
      "loss": 0.6991,
      "step": 4490
    },
    {
      "epoch": 2.659720682775438,
      "grad_norm": 2.1466076374053955,
      "learning_rate": 1.1511912180144423e-05,
      "loss": 0.6338,
      "step": 4500
    },
    {
      "epoch": 2.6656321584275475,
      "grad_norm": 1.2942527532577515,
      "learning_rate": 1.111928249296507e-05,
      "loss": 0.5902,
      "step": 4510
    },
    {
      "epoch": 2.671543634079657,
      "grad_norm": 0.9279811978340149,
      "learning_rate": 1.073320727587788e-05,
      "loss": 0.7447,
      "step": 4520
    },
    {
      "epoch": 2.677455109731767,
      "grad_norm": 2.081831455230713,
      "learning_rate": 1.0353704749687852e-05,
      "loss": 0.8421,
      "step": 4530
    },
    {
      "epoch": 2.6833665853838764,
      "grad_norm": 0.7336634397506714,
      "learning_rate": 9.980792825001982e-06,
      "loss": 0.6522,
      "step": 4540
    },
    {
      "epoch": 2.689278061035986,
      "grad_norm": 1.8728134632110596,
      "learning_rate": 9.61448910138416e-06,
      "loss": 0.6339,
      "step": 4550
    },
    {
      "epoch": 2.695189536688096,
      "grad_norm": 1.346036672592163,
      "learning_rate": 9.254810866524465e-06,
      "loss": 0.5937,
      "step": 4560
    },
    {
      "epoch": 2.7011010123402053,
      "grad_norm": 1.1082837581634521,
      "learning_rate": 8.90177509542327e-06,
      "loss": 1.2512,
      "step": 4570
    },
    {
      "epoch": 2.707012487992315,
      "grad_norm": 0.492722749710083,
      "learning_rate": 8.555398449590112e-06,
      "loss": 0.6059,
      "step": 4580
    },
    {
      "epoch": 2.7129239636444247,
      "grad_norm": 0.5797241926193237,
      "learning_rate": 8.215697276257477e-06,
      "loss": 0.7966,
      "step": 4590
    },
    {
      "epoch": 2.7188354392965346,
      "grad_norm": 0.3882758319377899,
      "learning_rate": 7.882687607609011e-06,
      "loss": 0.5282,
      "step": 4600
    },
    {
      "epoch": 2.724746914948644,
      "grad_norm": 0.547718346118927,
      "learning_rate": 7.556385160023198e-06,
      "loss": 0.651,
      "step": 4610
    },
    {
      "epoch": 2.7306583906007535,
      "grad_norm": 2.027658462524414,
      "learning_rate": 7.236805333331447e-06,
      "loss": 1.3069,
      "step": 4620
    },
    {
      "epoch": 2.7365698662528635,
      "grad_norm": 1.5063749551773071,
      "learning_rate": 6.9239632100912765e-06,
      "loss": 0.888,
      "step": 4630
    },
    {
      "epoch": 2.742481341904973,
      "grad_norm": 1.1332964897155762,
      "learning_rate": 6.61787355487462e-06,
      "loss": 0.7851,
      "step": 4640
    },
    {
      "epoch": 2.7483928175570824,
      "grad_norm": 1.0478250980377197,
      "learning_rate": 6.3185508135709275e-06,
      "loss": 0.6319,
      "step": 4650
    },
    {
      "epoch": 2.7543042932091923,
      "grad_norm": 1.139695405960083,
      "learning_rate": 6.026009112705382e-06,
      "loss": 0.6681,
      "step": 4660
    },
    {
      "epoch": 2.7602157688613023,
      "grad_norm": 0.9862475991249084,
      "learning_rate": 5.740262258772249e-06,
      "loss": 0.6155,
      "step": 4670
    },
    {
      "epoch": 2.7661272445134117,
      "grad_norm": 0.8533873558044434,
      "learning_rate": 5.4613237375832454e-06,
      "loss": 0.9115,
      "step": 4680
    },
    {
      "epoch": 2.772038720165521,
      "grad_norm": 0.6726348400115967,
      "learning_rate": 5.189206713631117e-06,
      "loss": 0.6037,
      "step": 4690
    },
    {
      "epoch": 2.777950195817631,
      "grad_norm": 0.5500934720039368,
      "learning_rate": 4.923924029468268e-06,
      "loss": 0.4589,
      "step": 4700
    },
    {
      "epoch": 2.7838616714697406,
      "grad_norm": 0.674471378326416,
      "learning_rate": 4.665488205100698e-06,
      "loss": 0.7702,
      "step": 4710
    },
    {
      "epoch": 2.78977314712185,
      "grad_norm": 1.835411787033081,
      "learning_rate": 4.413911437397172e-06,
      "loss": 1.0021,
      "step": 4720
    },
    {
      "epoch": 2.79568462277396,
      "grad_norm": 0.9664218425750732,
      "learning_rate": 4.169205599513531e-06,
      "loss": 0.6718,
      "step": 4730
    },
    {
      "epoch": 2.8015960984260695,
      "grad_norm": 1.0203169584274292,
      "learning_rate": 3.931382240332276e-06,
      "loss": 0.617,
      "step": 4740
    },
    {
      "epoch": 2.8075075740781794,
      "grad_norm": 0.6849484443664551,
      "learning_rate": 3.7004525839176844e-06,
      "loss": 0.7784,
      "step": 4750
    },
    {
      "epoch": 2.813419049730289,
      "grad_norm": 0.7749413251876831,
      "learning_rate": 3.47642752898597e-06,
      "loss": 0.6189,
      "step": 4760
    },
    {
      "epoch": 2.819330525382399,
      "grad_norm": 1.3107142448425293,
      "learning_rate": 3.2593176483908912e-06,
      "loss": 1.0674,
      "step": 4770
    },
    {
      "epoch": 2.8252420010345083,
      "grad_norm": 0.5056261420249939,
      "learning_rate": 3.049133188624919e-06,
      "loss": 0.6709,
      "step": 4780
    },
    {
      "epoch": 2.8311534766866178,
      "grad_norm": 0.704131007194519,
      "learning_rate": 2.8458840693354744e-06,
      "loss": 0.8815,
      "step": 4790
    },
    {
      "epoch": 2.8370649523387277,
      "grad_norm": 1.4412237405776978,
      "learning_rate": 2.6495798828568836e-06,
      "loss": 0.5613,
      "step": 4800
    },
    {
      "epoch": 2.842976427990837,
      "grad_norm": 0.5193577408790588,
      "learning_rate": 2.460229893757659e-06,
      "loss": 0.6967,
      "step": 4810
    },
    {
      "epoch": 2.8488879036429466,
      "grad_norm": 0.7831656336784363,
      "learning_rate": 2.2778430384031823e-06,
      "loss": 0.8557,
      "step": 4820
    },
    {
      "epoch": 2.8547993792950566,
      "grad_norm": 0.38411185145378113,
      "learning_rate": 2.102427924534056e-06,
      "loss": 0.8061,
      "step": 4830
    },
    {
      "epoch": 2.860710854947166,
      "grad_norm": 0.9332822561264038,
      "learning_rate": 1.9339928308597987e-06,
      "loss": 0.8597,
      "step": 4840
    },
    {
      "epoch": 2.866622330599276,
      "grad_norm": 2.4762656688690186,
      "learning_rate": 1.772545706668138e-06,
      "loss": 0.586,
      "step": 4850
    },
    {
      "epoch": 2.8725338062513854,
      "grad_norm": 0.2633223235607147,
      "learning_rate": 1.6180941714498462e-06,
      "loss": 0.4442,
      "step": 4860
    },
    {
      "epoch": 2.8784452819034954,
      "grad_norm": 1.2015960216522217,
      "learning_rate": 1.470645514539176e-06,
      "loss": 0.9937,
      "step": 4870
    },
    {
      "epoch": 2.884356757555605,
      "grad_norm": 0.9612688422203064,
      "learning_rate": 1.33020669476977e-06,
      "loss": 0.5673,
      "step": 4880
    },
    {
      "epoch": 2.8902682332077143,
      "grad_norm": 0.7924848794937134,
      "learning_rate": 1.1967843401463062e-06,
      "loss": 0.9437,
      "step": 4890
    },
    {
      "epoch": 2.8961797088598242,
      "grad_norm": 0.7569703459739685,
      "learning_rate": 1.0703847475316495e-06,
      "loss": 0.8538,
      "step": 4900
    },
    {
      "epoch": 2.9020911845119337,
      "grad_norm": 0.9832948446273804,
      "learning_rate": 9.510138823496883e-07,
      "loss": 0.5235,
      "step": 4910
    },
    {
      "epoch": 2.908002660164043,
      "grad_norm": 1.1758450269699097,
      "learning_rate": 8.386773783037926e-07,
      "loss": 0.8703,
      "step": 4920
    },
    {
      "epoch": 2.913914135816153,
      "grad_norm": 1.1674331426620483,
      "learning_rate": 7.33380537110928e-07,
      "loss": 1.1671,
      "step": 4930
    },
    {
      "epoch": 2.919825611468263,
      "grad_norm": 0.7247626781463623,
      "learning_rate": 6.35128328251422e-07,
      "loss": 0.4716,
      "step": 4940
    },
    {
      "epoch": 2.9257370871203725,
      "grad_norm": 0.9340316653251648,
      "learning_rate": 5.43925388734484e-07,
      "loss": 0.7584,
      "step": 4950
    },
    {
      "epoch": 2.931648562772482,
      "grad_norm": 0.5793424844741821,
      "learning_rate": 4.597760228793157e-07,
      "loss": 0.6079,
      "step": 4960
    },
    {
      "epoch": 2.937560038424592,
      "grad_norm": 1.0582811832427979,
      "learning_rate": 3.826842021120047e-07,
      "loss": 0.5889,
      "step": 4970
    },
    {
      "epoch": 2.9434715140767014,
      "grad_norm": 0.1966504007577896,
      "learning_rate": 3.1265356477802596e-07,
      "loss": 0.7168,
      "step": 4980
    },
    {
      "epoch": 2.949382989728811,
      "grad_norm": 0.672924816608429,
      "learning_rate": 2.4968741597061193e-07,
      "loss": 0.6101,
      "step": 4990
    },
    {
      "epoch": 2.9552944653809208,
      "grad_norm": 1.0457605123519897,
      "learning_rate": 1.9378872737471073e-07,
      "loss": 0.714,
      "step": 5000
    },
    {
      "epoch": 2.9612059410330303,
      "grad_norm": 0.42853084206581116,
      "learning_rate": 1.4496013712678168e-07,
      "loss": 0.8027,
      "step": 5010
    },
    {
      "epoch": 2.96711741668514,
      "grad_norm": 0.8010374903678894,
      "learning_rate": 1.0320394969024481e-07,
      "loss": 0.59,
      "step": 5020
    },
    {
      "epoch": 2.9730288923372497,
      "grad_norm": 2.3561248779296875,
      "learning_rate": 6.852213574675136e-08,
      "loss": 0.5662,
      "step": 5030
    },
    {
      "epoch": 2.9789403679893596,
      "grad_norm": 0.7117406129837036,
      "learning_rate": 4.091633210317469e-08,
      "loss": 0.9962,
      "step": 5040
    },
    {
      "epoch": 2.984851843641469,
      "grad_norm": 1.1211745738983154,
      "learning_rate": 2.0387841614338818e-08,
      "loss": 0.6768,
      "step": 5050
    },
    {
      "epoch": 2.9907633192935785,
      "grad_norm": 0.9021694660186768,
      "learning_rate": 6.93763312158424e-09,
      "loss": 0.7856,
      "step": 5060
    },
    {
      "epoch": 2.9966747949456884,
      "grad_norm": 0.6383291482925415,
      "learning_rate": 5.663414069545514e-10,
      "loss": 0.6182,
      "step": 5070
    }
  ],
  "logging_steps": 10,
  "max_steps": 5073,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.405122097925325e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
