{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1692,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005911475652109658,
      "grad_norm": 0.21855121850967407,
      "learning_rate": 5.399999999999999e-06,
      "loss": 3.9963,
      "step": 10
    },
    {
      "epoch": 0.011822951304219316,
      "grad_norm": 0.2550734579563141,
      "learning_rate": 1.14e-05,
      "loss": 4.0407,
      "step": 20
    },
    {
      "epoch": 0.017734426956328972,
      "grad_norm": 0.2610781788825989,
      "learning_rate": 1.74e-05,
      "loss": 4.3119,
      "step": 30
    },
    {
      "epoch": 0.023645902608438633,
      "grad_norm": 0.3536328077316284,
      "learning_rate": 2.34e-05,
      "loss": 4.0697,
      "step": 40
    },
    {
      "epoch": 0.02955737826054829,
      "grad_norm": 0.3950863778591156,
      "learning_rate": 2.94e-05,
      "loss": 3.9123,
      "step": 50
    },
    {
      "epoch": 0.035468853912657944,
      "grad_norm": 0.3810991048812866,
      "learning_rate": 3.539999999999999e-05,
      "loss": 3.8938,
      "step": 60
    },
    {
      "epoch": 0.04138032956476761,
      "grad_norm": 0.4553346335887909,
      "learning_rate": 4.14e-05,
      "loss": 4.0629,
      "step": 70
    },
    {
      "epoch": 0.047291805216877265,
      "grad_norm": 0.4710727632045746,
      "learning_rate": 4.7399999999999993e-05,
      "loss": 3.9297,
      "step": 80
    },
    {
      "epoch": 0.05320328086898692,
      "grad_norm": 0.5903026461601257,
      "learning_rate": 5.339999999999999e-05,
      "loss": 3.9447,
      "step": 90
    },
    {
      "epoch": 0.05911475652109658,
      "grad_norm": 0.4978746175765991,
      "learning_rate": 5.94e-05,
      "loss": 3.6104,
      "step": 100
    },
    {
      "epoch": 0.06502623217320623,
      "grad_norm": 0.4170621335506439,
      "learning_rate": 6.539999999999999e-05,
      "loss": 3.6523,
      "step": 110
    },
    {
      "epoch": 0.07093770782531589,
      "grad_norm": 0.6009400486946106,
      "learning_rate": 7.139999999999999e-05,
      "loss": 3.2434,
      "step": 120
    },
    {
      "epoch": 0.07684918347742556,
      "grad_norm": 0.6476964950561523,
      "learning_rate": 7.74e-05,
      "loss": 3.1342,
      "step": 130
    },
    {
      "epoch": 0.08276065912953522,
      "grad_norm": 0.8915241360664368,
      "learning_rate": 8.34e-05,
      "loss": 2.8407,
      "step": 140
    },
    {
      "epoch": 0.08867213478164487,
      "grad_norm": 1.5597630739212036,
      "learning_rate": 8.939999999999999e-05,
      "loss": 2.8,
      "step": 150
    },
    {
      "epoch": 0.09458361043375453,
      "grad_norm": 1.3148115873336792,
      "learning_rate": 9.539999999999999e-05,
      "loss": 3.0285,
      "step": 160
    },
    {
      "epoch": 0.10049508608586419,
      "grad_norm": 1.0388435125350952,
      "learning_rate": 0.0001014,
      "loss": 2.8272,
      "step": 170
    },
    {
      "epoch": 0.10640656173797385,
      "grad_norm": 1.1761771440505981,
      "learning_rate": 0.00010739999999999998,
      "loss": 2.2767,
      "step": 180
    },
    {
      "epoch": 0.1123180373900835,
      "grad_norm": 1.777435302734375,
      "learning_rate": 0.00011339999999999999,
      "loss": 2.4021,
      "step": 190
    },
    {
      "epoch": 0.11822951304219316,
      "grad_norm": 1.4255152940750122,
      "learning_rate": 0.0001194,
      "loss": 2.0731,
      "step": 200
    },
    {
      "epoch": 0.12414098869430282,
      "grad_norm": 1.954053282737732,
      "learning_rate": 0.00012539999999999999,
      "loss": 2.0045,
      "step": 210
    },
    {
      "epoch": 0.13005246434641246,
      "grad_norm": 1.1837152242660522,
      "learning_rate": 0.0001314,
      "loss": 2.1915,
      "step": 220
    },
    {
      "epoch": 0.13596393999852213,
      "grad_norm": 1.5257353782653809,
      "learning_rate": 0.0001374,
      "loss": 1.9796,
      "step": 230
    },
    {
      "epoch": 0.14187541565063178,
      "grad_norm": 1.2506072521209717,
      "learning_rate": 0.0001434,
      "loss": 1.4699,
      "step": 240
    },
    {
      "epoch": 0.14778689130274145,
      "grad_norm": 1.1530261039733887,
      "learning_rate": 0.0001494,
      "loss": 2.0078,
      "step": 250
    },
    {
      "epoch": 0.15369836695485112,
      "grad_norm": 2.1291584968566895,
      "learning_rate": 0.00015539999999999998,
      "loss": 1.8466,
      "step": 260
    },
    {
      "epoch": 0.15960984260696076,
      "grad_norm": 0.9238948225975037,
      "learning_rate": 0.0001614,
      "loss": 1.6766,
      "step": 270
    },
    {
      "epoch": 0.16552131825907043,
      "grad_norm": 0.8083561062812805,
      "learning_rate": 0.0001674,
      "loss": 1.2842,
      "step": 280
    },
    {
      "epoch": 0.17143279391118008,
      "grad_norm": 0.733826756477356,
      "learning_rate": 0.00017339999999999996,
      "loss": 1.4093,
      "step": 290
    },
    {
      "epoch": 0.17734426956328975,
      "grad_norm": 0.7897816300392151,
      "learning_rate": 0.00017939999999999997,
      "loss": 1.3763,
      "step": 300
    },
    {
      "epoch": 0.1832557452153994,
      "grad_norm": 2.1355347633361816,
      "learning_rate": 0.00018539999999999998,
      "loss": 2.084,
      "step": 310
    },
    {
      "epoch": 0.18916722086750906,
      "grad_norm": 0.9081681966781616,
      "learning_rate": 0.0001914,
      "loss": 1.5642,
      "step": 320
    },
    {
      "epoch": 0.1950786965196187,
      "grad_norm": 2.0413544178009033,
      "learning_rate": 0.0001974,
      "loss": 1.3998,
      "step": 330
    },
    {
      "epoch": 0.20099017217172838,
      "grad_norm": 1.1476536989212036,
      "learning_rate": 0.00020339999999999998,
      "loss": 1.3881,
      "step": 340
    },
    {
      "epoch": 0.20690164782383802,
      "grad_norm": 0.4750850796699524,
      "learning_rate": 0.00020939999999999997,
      "loss": 1.2549,
      "step": 350
    },
    {
      "epoch": 0.2128131234759477,
      "grad_norm": 2.610386610031128,
      "learning_rate": 0.00021539999999999998,
      "loss": 1.9993,
      "step": 360
    },
    {
      "epoch": 0.21872459912805733,
      "grad_norm": 1.2886751890182495,
      "learning_rate": 0.0002214,
      "loss": 1.803,
      "step": 370
    },
    {
      "epoch": 0.224636074780167,
      "grad_norm": 1.0942013263702393,
      "learning_rate": 0.00022739999999999997,
      "loss": 1.5721,
      "step": 380
    },
    {
      "epoch": 0.23054755043227665,
      "grad_norm": 0.6497766971588135,
      "learning_rate": 0.00023339999999999998,
      "loss": 1.2261,
      "step": 390
    },
    {
      "epoch": 0.23645902608438632,
      "grad_norm": 1.514673113822937,
      "learning_rate": 0.0002394,
      "loss": 1.8922,
      "step": 400
    },
    {
      "epoch": 0.24237050173649596,
      "grad_norm": 0.9680010676383972,
      "learning_rate": 0.00024539999999999995,
      "loss": 1.6252,
      "step": 410
    },
    {
      "epoch": 0.24828197738860563,
      "grad_norm": 2.0751240253448486,
      "learning_rate": 0.0002514,
      "loss": 1.5214,
      "step": 420
    },
    {
      "epoch": 0.2541934530407153,
      "grad_norm": 1.8613463640213013,
      "learning_rate": 0.00025739999999999997,
      "loss": 1.7016,
      "step": 430
    },
    {
      "epoch": 0.2601049286928249,
      "grad_norm": 2.130777359008789,
      "learning_rate": 0.00026339999999999995,
      "loss": 1.4288,
      "step": 440
    },
    {
      "epoch": 0.2660164043449346,
      "grad_norm": 0.646378755569458,
      "learning_rate": 0.0002694,
      "loss": 1.5949,
      "step": 450
    },
    {
      "epoch": 0.27192787999704426,
      "grad_norm": 1.3120288848876953,
      "learning_rate": 0.00027539999999999997,
      "loss": 1.4833,
      "step": 460
    },
    {
      "epoch": 0.2778393556491539,
      "grad_norm": 1.125124216079712,
      "learning_rate": 0.00028139999999999996,
      "loss": 1.4545,
      "step": 470
    },
    {
      "epoch": 0.28375083130126355,
      "grad_norm": 2.4735374450683594,
      "learning_rate": 0.00028739999999999994,
      "loss": 1.5317,
      "step": 480
    },
    {
      "epoch": 0.28966230695337325,
      "grad_norm": 0.6327743530273438,
      "learning_rate": 0.0002934,
      "loss": 1.2499,
      "step": 490
    },
    {
      "epoch": 0.2955737826054829,
      "grad_norm": 1.6242458820343018,
      "learning_rate": 0.00029939999999999996,
      "loss": 1.467,
      "step": 500
    },
    {
      "epoch": 0.30148525825759254,
      "grad_norm": 0.7589235901832581,
      "learning_rate": 0.0002999971329039567,
      "loss": 1.4417,
      "step": 510
    },
    {
      "epoch": 0.30739673390970224,
      "grad_norm": 1.0759592056274414,
      "learning_rate": 0.00029998722209538536,
      "loss": 1.2355,
      "step": 520
    },
    {
      "epoch": 0.3133082095618119,
      "grad_norm": 0.9292750954627991,
      "learning_rate": 0.00029997023264566715,
      "loss": 1.1428,
      "step": 530
    },
    {
      "epoch": 0.3192196852139215,
      "grad_norm": 1.2630168199539185,
      "learning_rate": 0.00029994616535661874,
      "loss": 1.5303,
      "step": 540
    },
    {
      "epoch": 0.32513116086603117,
      "grad_norm": 1.0667715072631836,
      "learning_rate": 0.00029991502136409473,
      "loss": 1.4133,
      "step": 550
    },
    {
      "epoch": 0.33104263651814086,
      "grad_norm": 0.9384068846702576,
      "learning_rate": 0.00029987680213793466,
      "loss": 1.1268,
      "step": 560
    },
    {
      "epoch": 0.3369541121702505,
      "grad_norm": 1.5506012439727783,
      "learning_rate": 0.0002998315094818935,
      "loss": 1.889,
      "step": 570
    },
    {
      "epoch": 0.34286558782236015,
      "grad_norm": 0.7173452973365784,
      "learning_rate": 0.00029977914553355636,
      "loss": 1.6893,
      "step": 580
    },
    {
      "epoch": 0.3487770634744698,
      "grad_norm": 1.4104084968566895,
      "learning_rate": 0.00029971971276423776,
      "loss": 1.6408,
      "step": 590
    },
    {
      "epoch": 0.3546885391265795,
      "grad_norm": 1.3333358764648438,
      "learning_rate": 0.00029965321397886494,
      "loss": 1.2775,
      "step": 600
    },
    {
      "epoch": 0.36060001477868914,
      "grad_norm": 1.7931455373764038,
      "learning_rate": 0.00029957965231584556,
      "loss": 1.3147,
      "step": 610
    },
    {
      "epoch": 0.3665114904307988,
      "grad_norm": 0.8781871795654297,
      "learning_rate": 0.0002994990312469194,
      "loss": 1.4476,
      "step": 620
    },
    {
      "epoch": 0.3724229660829084,
      "grad_norm": 3.202131509780884,
      "learning_rate": 0.0002994113545769947,
      "loss": 1.8031,
      "step": 630
    },
    {
      "epoch": 0.3783344417350181,
      "grad_norm": 0.4886491000652313,
      "learning_rate": 0.00029931662644396847,
      "loss": 1.1976,
      "step": 640
    },
    {
      "epoch": 0.38424591738712777,
      "grad_norm": 0.808239758014679,
      "learning_rate": 0.0002992148513185314,
      "loss": 1.097,
      "step": 650
    },
    {
      "epoch": 0.3901573930392374,
      "grad_norm": 0.6215143203735352,
      "learning_rate": 0.0002991060340039565,
      "loss": 1.1641,
      "step": 660
    },
    {
      "epoch": 0.39606886869134705,
      "grad_norm": 1.4069135189056396,
      "learning_rate": 0.00029899017963587275,
      "loss": 1.6598,
      "step": 670
    },
    {
      "epoch": 0.40198034434345675,
      "grad_norm": 1.5653876066207886,
      "learning_rate": 0.00029886729368202257,
      "loss": 1.2168,
      "step": 680
    },
    {
      "epoch": 0.4078918199955664,
      "grad_norm": 2.5063676834106445,
      "learning_rate": 0.0002987373819420038,
      "loss": 1.9217,
      "step": 690
    },
    {
      "epoch": 0.41380329564767604,
      "grad_norm": 0.8011614680290222,
      "learning_rate": 0.000298600450546996,
      "loss": 1.3334,
      "step": 700
    },
    {
      "epoch": 0.4197147712997857,
      "grad_norm": 2.6214349269866943,
      "learning_rate": 0.00029845650595947105,
      "loss": 1.161,
      "step": 710
    },
    {
      "epoch": 0.4256262469518954,
      "grad_norm": 0.6010191440582275,
      "learning_rate": 0.0002983055549728882,
      "loss": 1.4484,
      "step": 720
    },
    {
      "epoch": 0.431537722604005,
      "grad_norm": 0.669968843460083,
      "learning_rate": 0.0002981476047113734,
      "loss": 0.8403,
      "step": 730
    },
    {
      "epoch": 0.43744919825611467,
      "grad_norm": 0.8820887207984924,
      "learning_rate": 0.0002979826626293832,
      "loss": 1.6598,
      "step": 740
    },
    {
      "epoch": 0.44336067390822437,
      "grad_norm": 0.729704737663269,
      "learning_rate": 0.0002978107365113527,
      "loss": 1.1679,
      "step": 750
    },
    {
      "epoch": 0.449272149560334,
      "grad_norm": 0.7598924040794373,
      "learning_rate": 0.00029763183447132843,
      "loss": 1.0783,
      "step": 760
    },
    {
      "epoch": 0.45518362521244365,
      "grad_norm": 1.3059768676757812,
      "learning_rate": 0.00029744596495258525,
      "loss": 1.5463,
      "step": 770
    },
    {
      "epoch": 0.4610951008645533,
      "grad_norm": 1.5226678848266602,
      "learning_rate": 0.0002972531367272279,
      "loss": 1.6517,
      "step": 780
    },
    {
      "epoch": 0.467006576516663,
      "grad_norm": 1.2538323402404785,
      "learning_rate": 0.000297053358895777,
      "loss": 1.3817,
      "step": 790
    },
    {
      "epoch": 0.47291805216877264,
      "grad_norm": 0.9093683958053589,
      "learning_rate": 0.00029684664088673964,
      "loss": 1.0216,
      "step": 800
    },
    {
      "epoch": 0.4788295278208823,
      "grad_norm": 0.9371103644371033,
      "learning_rate": 0.00029663299245616425,
      "loss": 1.4138,
      "step": 810
    },
    {
      "epoch": 0.4847410034729919,
      "grad_norm": 0.9414723515510559,
      "learning_rate": 0.0002964124236871802,
      "loss": 1.1547,
      "step": 820
    },
    {
      "epoch": 0.4906524791251016,
      "grad_norm": 1.3631341457366943,
      "learning_rate": 0.0002961849449895221,
      "loss": 1.1962,
      "step": 830
    },
    {
      "epoch": 0.49656395477721127,
      "grad_norm": 2.2169299125671387,
      "learning_rate": 0.0002959505670990381,
      "loss": 1.4767,
      "step": 840
    },
    {
      "epoch": 0.502475430429321,
      "grad_norm": 0.7052278518676758,
      "learning_rate": 0.00029570930107718387,
      "loss": 1.198,
      "step": 850
    },
    {
      "epoch": 0.5083869060814306,
      "grad_norm": 0.7167408466339111,
      "learning_rate": 0.0002954611583104999,
      "loss": 0.8587,
      "step": 860
    },
    {
      "epoch": 0.5142983817335403,
      "grad_norm": 1.4745219945907593,
      "learning_rate": 0.00029520615051007457,
      "loss": 0.8759,
      "step": 870
    },
    {
      "epoch": 0.5202098573856498,
      "grad_norm": 0.9305026531219482,
      "learning_rate": 0.00029494428971099104,
      "loss": 1.1192,
      "step": 880
    },
    {
      "epoch": 0.5261213330377595,
      "grad_norm": 1.6259422302246094,
      "learning_rate": 0.0002946755882717596,
      "loss": 1.4497,
      "step": 890
    },
    {
      "epoch": 0.5320328086898692,
      "grad_norm": 0.5704475045204163,
      "learning_rate": 0.0002944000588737343,
      "loss": 1.1544,
      "step": 900
    },
    {
      "epoch": 0.5379442843419788,
      "grad_norm": 1.0011250972747803,
      "learning_rate": 0.00029411771452051447,
      "loss": 1.0339,
      "step": 910
    },
    {
      "epoch": 0.5438557599940885,
      "grad_norm": 0.5349895358085632,
      "learning_rate": 0.0002938285685373309,
      "loss": 1.0725,
      "step": 920
    },
    {
      "epoch": 0.5497672356461982,
      "grad_norm": 0.780112624168396,
      "learning_rate": 0.0002935326345704171,
      "loss": 1.104,
      "step": 930
    },
    {
      "epoch": 0.5556787112983078,
      "grad_norm": 0.8973895311355591,
      "learning_rate": 0.00029322992658636525,
      "loss": 1.5861,
      "step": 940
    },
    {
      "epoch": 0.5615901869504175,
      "grad_norm": 2.284956455230713,
      "learning_rate": 0.0002929204588714668,
      "loss": 1.4961,
      "step": 950
    },
    {
      "epoch": 0.5675016626025271,
      "grad_norm": 1.720916748046875,
      "learning_rate": 0.00029260424603103877,
      "loss": 0.7762,
      "step": 960
    },
    {
      "epoch": 0.5734131382546368,
      "grad_norm": 3.2334513664245605,
      "learning_rate": 0.00029228130298873385,
      "loss": 1.4747,
      "step": 970
    },
    {
      "epoch": 0.5793246139067465,
      "grad_norm": 0.3923763036727905,
      "learning_rate": 0.00029195164498583637,
      "loss": 0.9833,
      "step": 980
    },
    {
      "epoch": 0.5852360895588561,
      "grad_norm": 0.9399965405464172,
      "learning_rate": 0.0002916152875805431,
      "loss": 0.9483,
      "step": 990
    },
    {
      "epoch": 0.5911475652109658,
      "grad_norm": 1.4126980304718018,
      "learning_rate": 0.0002912722466472288,
      "loss": 1.6528,
      "step": 1000
    },
    {
      "epoch": 0.5970590408630755,
      "grad_norm": 1.4300436973571777,
      "learning_rate": 0.000290922538375697,
      "loss": 1.3048,
      "step": 1010
    },
    {
      "epoch": 0.6029705165151851,
      "grad_norm": 1.3519343137741089,
      "learning_rate": 0.0002905661792704161,
      "loss": 1.5751,
      "step": 1020
    },
    {
      "epoch": 0.6088819921672948,
      "grad_norm": 0.410167396068573,
      "learning_rate": 0.0002902031861497402,
      "loss": 1.0088,
      "step": 1030
    },
    {
      "epoch": 0.6147934678194045,
      "grad_norm": 0.8773415088653564,
      "learning_rate": 0.0002898335761451157,
      "loss": 0.9187,
      "step": 1040
    },
    {
      "epoch": 0.6207049434715141,
      "grad_norm": 2.3186562061309814,
      "learning_rate": 0.00028945736670027243,
      "loss": 1.3892,
      "step": 1050
    },
    {
      "epoch": 0.6266164191236238,
      "grad_norm": 1.0862361192703247,
      "learning_rate": 0.0002890745755704006,
      "loss": 1.2894,
      "step": 1060
    },
    {
      "epoch": 0.6325278947757333,
      "grad_norm": 0.8714461326599121,
      "learning_rate": 0.00028868522082131274,
      "loss": 1.219,
      "step": 1070
    },
    {
      "epoch": 0.638439370427843,
      "grad_norm": 1.4897832870483398,
      "learning_rate": 0.00028828932082859124,
      "loss": 1.1224,
      "step": 1080
    },
    {
      "epoch": 0.6443508460799527,
      "grad_norm": 2.1309995651245117,
      "learning_rate": 0.0002878868942767209,
      "loss": 1.4618,
      "step": 1090
    },
    {
      "epoch": 0.6502623217320623,
      "grad_norm": 1.5315428972244263,
      "learning_rate": 0.00028747796015820735,
      "loss": 1.2241,
      "step": 1100
    },
    {
      "epoch": 0.656173797384172,
      "grad_norm": 1.0699809789657593,
      "learning_rate": 0.0002870625377726804,
      "loss": 0.9697,
      "step": 1110
    },
    {
      "epoch": 0.6620852730362817,
      "grad_norm": 1.799970030784607,
      "learning_rate": 0.0002866406467259835,
      "loss": 1.0213,
      "step": 1120
    },
    {
      "epoch": 0.6679967486883913,
      "grad_norm": 0.8505222797393799,
      "learning_rate": 0.0002862123069292483,
      "loss": 1.2248,
      "step": 1130
    },
    {
      "epoch": 0.673908224340501,
      "grad_norm": 1.3934144973754883,
      "learning_rate": 0.00028577753859795494,
      "loss": 1.0988,
      "step": 1140
    },
    {
      "epoch": 0.6798196999926106,
      "grad_norm": 0.9352688789367676,
      "learning_rate": 0.000285336362250978,
      "loss": 1.2504,
      "step": 1150
    },
    {
      "epoch": 0.6857311756447203,
      "grad_norm": 1.4182519912719727,
      "learning_rate": 0.00028488879870961815,
      "loss": 1.1847,
      "step": 1160
    },
    {
      "epoch": 0.69164265129683,
      "grad_norm": 1.372120976448059,
      "learning_rate": 0.00028443486909661935,
      "loss": 1.3844,
      "step": 1170
    },
    {
      "epoch": 0.6975541269489396,
      "grad_norm": 1.2593580484390259,
      "learning_rate": 0.00028397459483517216,
      "loss": 0.9362,
      "step": 1180
    },
    {
      "epoch": 0.7034656026010493,
      "grad_norm": 1.0964024066925049,
      "learning_rate": 0.00028350799764790256,
      "loss": 1.286,
      "step": 1190
    },
    {
      "epoch": 0.709377078253159,
      "grad_norm": 1.644283652305603,
      "learning_rate": 0.0002830350995558468,
      "loss": 1.1282,
      "step": 1200
    },
    {
      "epoch": 0.7152885539052686,
      "grad_norm": 0.6417403817176819,
      "learning_rate": 0.00028255592287741195,
      "loss": 1.1041,
      "step": 1210
    },
    {
      "epoch": 0.7212000295573783,
      "grad_norm": 1.200995922088623,
      "learning_rate": 0.00028207049022732286,
      "loss": 1.1965,
      "step": 1220
    },
    {
      "epoch": 0.727111505209488,
      "grad_norm": 0.6340023875236511,
      "learning_rate": 0.0002815788245155548,
      "loss": 1.0427,
      "step": 1230
    },
    {
      "epoch": 0.7330229808615976,
      "grad_norm": 0.935417890548706,
      "learning_rate": 0.00028108094894625183,
      "loss": 0.7884,
      "step": 1240
    },
    {
      "epoch": 0.7389344565137073,
      "grad_norm": 1.2260072231292725,
      "learning_rate": 0.0002805768870166323,
      "loss": 1.2559,
      "step": 1250
    },
    {
      "epoch": 0.7448459321658168,
      "grad_norm": 1.202872395515442,
      "learning_rate": 0.00028006666251587944,
      "loss": 1.0569,
      "step": 1260
    },
    {
      "epoch": 0.7507574078179265,
      "grad_norm": 2.5879993438720703,
      "learning_rate": 0.0002795502995240188,
      "loss": 0.9203,
      "step": 1270
    },
    {
      "epoch": 0.7566688834700362,
      "grad_norm": 0.8845624923706055,
      "learning_rate": 0.00027902782241078194,
      "loss": 0.7764,
      "step": 1280
    },
    {
      "epoch": 0.7625803591221458,
      "grad_norm": 0.8509868383407593,
      "learning_rate": 0.000278499255834456,
      "loss": 1.2114,
      "step": 1290
    },
    {
      "epoch": 0.7684918347742555,
      "grad_norm": 0.7744777798652649,
      "learning_rate": 0.00027796462474071994,
      "loss": 1.1977,
      "step": 1300
    },
    {
      "epoch": 0.7744033104263652,
      "grad_norm": 0.9116315841674805,
      "learning_rate": 0.00027742395436146773,
      "loss": 1.3982,
      "step": 1310
    },
    {
      "epoch": 0.7803147860784748,
      "grad_norm": 1.059754490852356,
      "learning_rate": 0.0002768772702136169,
      "loss": 0.8068,
      "step": 1320
    },
    {
      "epoch": 0.7862262617305845,
      "grad_norm": 1.341540813446045,
      "learning_rate": 0.0002763245980979048,
      "loss": 1.0405,
      "step": 1330
    },
    {
      "epoch": 0.7921377373826941,
      "grad_norm": 3.170034885406494,
      "learning_rate": 0.00027576596409767043,
      "loss": 0.8736,
      "step": 1340
    },
    {
      "epoch": 0.7980492130348038,
      "grad_norm": 1.3075296878814697,
      "learning_rate": 0.0002752013945776238,
      "loss": 1.0294,
      "step": 1350
    },
    {
      "epoch": 0.8039606886869135,
      "grad_norm": 0.7042812705039978,
      "learning_rate": 0.0002746309161826018,
      "loss": 0.8612,
      "step": 1360
    },
    {
      "epoch": 0.8098721643390231,
      "grad_norm": 1.0310754776000977,
      "learning_rate": 0.00027405455583630995,
      "loss": 0.8773,
      "step": 1370
    },
    {
      "epoch": 0.8157836399911328,
      "grad_norm": 0.8753094673156738,
      "learning_rate": 0.0002734723407400527,
      "loss": 1.022,
      "step": 1380
    },
    {
      "epoch": 0.8216951156432425,
      "grad_norm": 1.4632091522216797,
      "learning_rate": 0.0002728842983714489,
      "loss": 1.4363,
      "step": 1390
    },
    {
      "epoch": 0.8276065912953521,
      "grad_norm": 1.3844555616378784,
      "learning_rate": 0.0002722904564831354,
      "loss": 1.0108,
      "step": 1400
    },
    {
      "epoch": 0.8335180669474618,
      "grad_norm": 0.8012031316757202,
      "learning_rate": 0.0002716908431014572,
      "loss": 0.9257,
      "step": 1410
    },
    {
      "epoch": 0.8394295425995714,
      "grad_norm": 0.48592811822891235,
      "learning_rate": 0.0002710854865251446,
      "loss": 1.0916,
      "step": 1420
    },
    {
      "epoch": 0.8453410182516811,
      "grad_norm": 2.0743398666381836,
      "learning_rate": 0.00027047441532397794,
      "loss": 0.9367,
      "step": 1430
    },
    {
      "epoch": 0.8512524939037908,
      "grad_norm": 1.422998309135437,
      "learning_rate": 0.0002698576583374388,
      "loss": 1.0587,
      "step": 1440
    },
    {
      "epoch": 0.8571639695559004,
      "grad_norm": 2.077911853790283,
      "learning_rate": 0.0002692352446733494,
      "loss": 1.0621,
      "step": 1450
    },
    {
      "epoch": 0.86307544520801,
      "grad_norm": 2.380415916442871,
      "learning_rate": 0.0002686072037064984,
      "loss": 0.9464,
      "step": 1460
    },
    {
      "epoch": 0.8689869208601197,
      "grad_norm": 1.3697080612182617,
      "learning_rate": 0.00026797356507725513,
      "loss": 0.9229,
      "step": 1470
    },
    {
      "epoch": 0.8748983965122293,
      "grad_norm": 0.9470162987709045,
      "learning_rate": 0.00026733435869017005,
      "loss": 0.8697,
      "step": 1480
    },
    {
      "epoch": 0.880809872164339,
      "grad_norm": 0.5239623188972473,
      "learning_rate": 0.00026668961471256397,
      "loss": 0.9903,
      "step": 1490
    },
    {
      "epoch": 0.8867213478164487,
      "grad_norm": 1.287339210510254,
      "learning_rate": 0.00026603936357310395,
      "loss": 0.9427,
      "step": 1500
    },
    {
      "epoch": 0.8926328234685583,
      "grad_norm": 0.9761462211608887,
      "learning_rate": 0.0002653836359603674,
      "loss": 0.9243,
      "step": 1510
    },
    {
      "epoch": 0.898544299120668,
      "grad_norm": 0.7651516199111938,
      "learning_rate": 0.0002647224628213936,
      "loss": 0.7775,
      "step": 1520
    },
    {
      "epoch": 0.9044557747727776,
      "grad_norm": 0.3834855854511261,
      "learning_rate": 0.00026405587536022324,
      "loss": 0.8652,
      "step": 1530
    },
    {
      "epoch": 0.9103672504248873,
      "grad_norm": 1.4622029066085815,
      "learning_rate": 0.00026338390503642586,
      "loss": 0.6217,
      "step": 1540
    },
    {
      "epoch": 0.916278726076997,
      "grad_norm": 0.7228372693061829,
      "learning_rate": 0.00026270658356361483,
      "loss": 0.9316,
      "step": 1550
    },
    {
      "epoch": 0.9221902017291066,
      "grad_norm": 2.2301530838012695,
      "learning_rate": 0.00026202394290795095,
      "loss": 0.89,
      "step": 1560
    },
    {
      "epoch": 0.9281016773812163,
      "grad_norm": 0.3718915581703186,
      "learning_rate": 0.0002613360152866335,
      "loss": 0.8487,
      "step": 1570
    },
    {
      "epoch": 0.934013153033326,
      "grad_norm": 2.8389663696289062,
      "learning_rate": 0.00026064283316638,
      "loss": 1.1533,
      "step": 1580
    },
    {
      "epoch": 0.9399246286854356,
      "grad_norm": 1.1260708570480347,
      "learning_rate": 0.00025994442926189384,
      "loss": 1.1317,
      "step": 1590
    },
    {
      "epoch": 0.9458361043375453,
      "grad_norm": 2.0674495697021484,
      "learning_rate": 0.0002592408365343202,
      "loss": 1.2019,
      "step": 1600
    },
    {
      "epoch": 0.9517475799896549,
      "grad_norm": 0.509918212890625,
      "learning_rate": 0.0002585320881896907,
      "loss": 1.0216,
      "step": 1610
    },
    {
      "epoch": 0.9576590556417646,
      "grad_norm": 1.1706818342208862,
      "learning_rate": 0.00025781821767735606,
      "loss": 1.0687,
      "step": 1620
    },
    {
      "epoch": 0.9635705312938743,
      "grad_norm": 1.2221193313598633,
      "learning_rate": 0.00025709925868840745,
      "loss": 1.1468,
      "step": 1630
    },
    {
      "epoch": 0.9694820069459839,
      "grad_norm": 1.1034770011901855,
      "learning_rate": 0.0002563752451540867,
      "loss": 1.097,
      "step": 1640
    },
    {
      "epoch": 0.9753934825980936,
      "grad_norm": 0.807884693145752,
      "learning_rate": 0.00025564621124418436,
      "loss": 0.5802,
      "step": 1650
    },
    {
      "epoch": 0.9813049582502033,
      "grad_norm": 1.3589593172073364,
      "learning_rate": 0.0002549121913654278,
      "loss": 1.0961,
      "step": 1660
    },
    {
      "epoch": 0.9872164339023128,
      "grad_norm": 0.7613115310668945,
      "learning_rate": 0.00025417322015985666,
      "loss": 0.6417,
      "step": 1670
    },
    {
      "epoch": 0.9931279095544225,
      "grad_norm": 2.2618770599365234,
      "learning_rate": 0.0002534293325031885,
      "loss": 1.2346,
      "step": 1680
    },
    {
      "epoch": 0.9990393852065321,
      "grad_norm": 0.9102917313575745,
      "learning_rate": 0.00025268056350317254,
      "loss": 0.7727,
      "step": 1690
    }
  ],
  "logging_steps": 10,
  "max_steps": 5073,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.136172639243469e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
